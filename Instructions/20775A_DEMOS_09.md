# Module 9: Analyze Data with Hive and Phoenix


## Demo 1: Implement interactive queries for big data with Interactive Hive

### Scenario

In this demonstration, you will perform the following tasks to implement an interactive query with Hive:
-	Create an Interactive Hive cluster.
-	Review the LLAP settings in Ambari.
-	Connect Excel to an Interactive Hive cluster.

### Preparation

Ensure that the **MT17B-WS2016-NAT** and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**. 

Install the following: [Microsoft HiveODBC Driver (32 and 64 bit versions)](http://www.microsoft.com/en-ca/download/details.aspx?id=40886)

### Create an Interactive Hive cluster
1.	In Microsoft Internet Explorer®, navigate to **https://portal.azure.com**, and then sign in using the Microsoft account that is associated with your Azure Pass subscription.
2.	Click the **+ New** button, type **HDInsight** in the search box, and press Enter.
3.	Click **HDInsight** from the list—the HDInsight blade will open—and then click **Create**.
4.	In the cluster creation wizard, click **Custom (size, settings, apps)**.
5.	Click **Cluster type**, in the **Cluster Configuration** blade select the following options, and then click **Select**:
    -	Cluster type: **Interactive Hive (Preview)**
    -	Version: **Interactive Hive 2.1.0 (HDI 3.6)**
6.	On the **Basics** blade, enter the following settings, and then click **Next**:
    -	Cluster name: **hivellap\<your name\>01**
    -	Subscription: **\<your subscription name\>**
    -	Cluster login username: **admin**
    -	Cluster login password: **Pa55w.rdPa55w.rd**
    -	Secure Shell (SSH) username: **sshuser** 
    -	Use same password as cluster login: **Selected**
    -	Resource group (Create new): **hivellaptrain01rg**
    -	Location: **Pick your location**
7.	In **Select a Storage account**, click **Create new**, enter the following settings, and then click **Next**:
    -	Create a new Storage account: **hivellaptrain01stg**
    -	Default container: **hivellaptrain01ctr**
8.	In **Applications**, normally you can install popular data science tools such as H20 or Dataiku or any other apps; however, for Interactive Hive Cluster, no applications are available so click **Next** and move on to the _Cluster size_ section.
9.	In **Cluster size**, set node sizes appropriately, and then click **Next**:
    -	Number of Worker nodes: **2**
    -	Worker node size: **D13 v2 (2 nodes, 16 cores)**
    -	Head node size: **D13 V2 (2 nodes, 16 cores)**
    -	Zookeeper node sizes: **default**
10.	In the **Advanced settings** section, set the following, if needed, and then click **Next**:
    -	**Script Actions (optional)**
    -	**Virtual network settings (optional)**
    -	For demo purposes, leave the above settings as blank
11.	Review the summary section and then click **Create**.
12.	Cluster Creation will be pinned to Azure Porta Dashboard and provisioning will take about 20 minutes to finish.
13.	When the cluster is provisioned, you are good to start connecting to Hive Cluster, loading data and performing interactive querying on the cluster.
14.	For detailed steps with screenshots, refer to the following document: **E:\Demofiles\Mod09\Create_Delete_Interactive_Hive_Cluster_on_Azure.docx**.

### Review the LLAP settings in Ambari
1.	Open the **Provisioned Interactive Hive Cluster** blade, and click **Overview**.
2.	In the overview page, under **Quick links**, click **Cluster Dashboard**, and in the next screen, click **HDInsight cluster dashboard**.
3.	This will open Ambari in a browser and show prompts for Login and password. Enter your cluster username name **admin** and password **Pa55w.rdPa55w.rd** to log into Ambari. Ambari is a web-based Hadoop Cluster management console. 
4.	Click **Hive** on the left-hand side and you will see Summary and Configs tabs on the right-hand side.
5.	Click the **Configs** tab and you will be able to see Interactive Hive settings.

    Interactive Query is set to **Yes**, and the key LLAP configurations come with default values. You can always adjust these values as per your query concurrency and cache memory requirements.

### Connect Excel to an Interactive Hive cluster
1.	Identify the dataset you want to analyze and visualize. You will be using the
**hivesampletable** available in the Hive **default** schema.
2.	Set up ODBC DSN on your workstation:
    -	Open **ODBC Data Sources (32-bit)** and click the **User** DSN tab.
    -	Click **Add**, click Microsoft **Hive ODBC Diver**, and then click **Finish**.
    -	Enter the following details:
        -	Data Source name: **Microsoft Hive DSN**
        -	Description: Sample **Microsoft Hive DSN**
        -	Host(s): **hivellap\<your name\>01.azurehdinsight.net**
        -	Port: **443**
        -	Database: **default**
        -	Authentication:
            -	Mechanism: **Windows Azure HDInsight Service**
            -	Realm: **disabled**
            -	Host **FQDN: disabled**
            -	Service Name: **disabled**
            -	User Name: **admin**
            -	Password:  **Pa55w.rdPa55w.rd**
            -	Delegation UID: **Keep blank**
        -	Thrift Transport: **HTTP**
        -	Click HTTP Options
            -	HTTP Path: **/hive2**
            -	Click **OK**
        -	Click SSL Options
            -	Ensure the **Enable SSL** check box is selected
            -	Click **OK**
        -	Click Test 
            -	You should see a “Successfully connected to data source”  message
            -	Click **OK**
        -	Click the **OK** button to complete the ODBC DSN setup and click **OK** to close the ODBC Data Source Administrator
3.	Start with a new Excel workbook.
4.	Go to the **Data** tab and, in the **Get External Data** section, click **From other sources**, and then click **From Data Connection Wizard**, which will open a Data Connection wizard.
5.	The **Data Connection Wizard** displays a list of data sources. Select **ODBC DSN**, and click **Next**.
6.	In the next step, select an ODBC source from the list and click **Next**. In this case, it would be the ODBC data source **Microsoft Hive DSN** created in Step 2.
7.	In **Password**, type your cluster login password **Pa55w.rdPa55w.rd**, and click **OK**.
8.	In the **Select Database and Table** window, select the database **HIVE** from the drop-down list, select the table **hivesampletable**, and click **Next**.
9.	In the **Select Data Connection File and Finish** window, give a friendly name and description to the connection and click **Finish** to start importing data.
10.	In the **Import Data** window, to change or specify the query, click **Properties** to open a new window, and click **OK** to come out of the Connection property window.
11.	Click **OK** to come out of the import data window.
12.	In **Password**, type your cluster login password **Pa55w.rdPa55w.rd**, and click **OK**.
13.	Within few seconds, your data will be available in Excel to carry out further analysis.

## Demo 2: Perform exploratory data analysis using Hive

### Scenario

In this demonstration, you will perform the following tasks to carry out an exploratory analysis:
-	Open Zeppelin Notebook.
-	Create ORC formatted tables.
-	Perform exploratory analysis.

### Preparation

In this demonstration, you will use the Interactive Hive cluster you created in the previous lesson. If you have already deleted the cluster, you should use the steps outlined in the following document on your workstation for creating an Interactive Hive cluster in Azure:
**E:\Demofiles\Mod09\Create_Delete_Interactive_Hive_cluster_on_Azure.docx**.

**Source Dataset**: you will use a **hivesampletable** that has weblog related information. This table already exists in the default schema on Interactive Hive Cluster.
**Tools**: you will use Zeppelin Notebook to perform data ingestion, exploration, and visualization.

### Open Zeppelin Notebook

1.	Launch Zeppelin Notebook by using the following steps:
    -	Log in to the Azure Portal **http://portal.azure.com** with your Azure credentials, if you have not already logged into the portal.
    -	Open your Interactive Hive cluster **hivellap\<your name\>01**.
    -	On the overview page, under **Quick links**, click **Cluster dashboard**, and then click **Zeppelin Notebook**.
    -	If prompted, enter the cluster admin credentials to launch Zeppelin Notebook.

### Create an ORC formatted table

1.	Create a new notebook in **Zeppelin Notebook**:
    -	From the header pane, click **Notebook**, and click **+ Create new note**.
    -	Give a name to your notebook and click **Create Note**.
    -	In the first cell, type **%jdbc(hive)**, and be ready to paste the contents from demo files.
2.	Open the Demo Contents folder pertinent to this demo at the following location:** **E:\Demofiles\Mod09\Demo02_analysis_with_interactive_hive**.
3.	Open the **create_orc_table.hql** file from the above location using Notepad, review the code, and then paste the whole file contents into the first cell of your notebook following the **%jdbc(hive)** command.
4.	Press Shift+Enter.

### Perform exploratory analysis

1.	Use Notepad to open the **exploratory_data_analysis_with_hive.hql** file at the following demo contents location:
**E:\Demofiles\Mod09\Demo02_analysis_with_interactive_hive**.
2.	Review all the SQL queries/Steps in **exploratory_data_analysis_with_hive.hql** file, and start pasting into your notebook step-by-step. When you finish running a query in a cell (in Notebook), you have a rectangular box with visualization charts (bar, pie, trend along with table). By clicking one of them, your result set will be converted into a visualization. By clicking the setting, you can also apply advanced settings to the visualization. Paste each step into a separate cell in your notebook, and ensure that every cell starts with the **%jdbc(hive)** command:
    -	In **Step 1**, you perform basic data exploration (counts, viewing sample rows, and so on).
    -	In **Step 2**, you run a query to identify the countries where the Contoso e-commerce platform is operating.
    -	In **Step 3**, you run a query to identify the countries that have the highest web traffic to this e-commerce platform.
    Please note that any errors reported back, can be ignored.
    -	In **Step 4**, you run a query to identify the device platform that is most commonly used by website users.
    -	In **Step 5**, you run a query to identify the frequent visitors (appearing more than six times in this dataset) across different countries.
    -	For all the above queries, when the data is loaded, click a visualization (bar, pie or trend chart in the rectangular icon at the top-left corner of the cell) to convert the data into a visualization. Switching between visualizations can take a long time. Also, click settings to play with advanced settings.
3.	Typically, your notebooks are saved on the head node of the cluster. However, its lifetime is for the duration of the cluster only, so you can export the notebook onto your local file system/blob storage and can import in Notebook on any cluster for later usage. 
    -	On the notebook you created, click the **Export this note** icon at the top to export the whole notebook into the following location: **E:\Demofiles\Mod09\Demo02_analysis_with_interactive_hive**.
    -	The export file will be stored in JSON format and can later be used to import into Zeppelin Notebook.
    -	You have a sample JSON file **HiveSampleAnalysis.json** that is used for the demo at the following location: **E:\Demofiles\Mod09\Demo02_analysis_with_interactive_hive**.
    -	On the Zeppelin Notebook landing, click **Import note** and select the above file. Your notebook will be imported.
    -	In the Notebook drop-down list, click **HiveSampleAnalysis** and you can run the steps cell-by-cell by pressing either Shift+Enter or the play icon in the top corner of the cell.

## Demo 3: Perform interactive processing by using Apache Phoenix on HBase

### Scenario

In this demonstration, you will perform the following tasks to implement an interactive query with Phoenix:
-	Create an HBase cluster
-	Ingest data into Phoenix
-	Perform interactive queries

### Create an HBase cluster

1.	In Microsoft Internet Explorer, navigate to **https://portal.azure.com**, and then sign in using the Microsoft account that is associated with your Azure Pass subscription.
2.	Click the **+ New** button, type **HDInsight** in the search box, and press Enter.
3.	Click **HDInsight** from the list—the **HDInsight** blade will open—and then click **Create**.
4.	In the cluster creation wizard, click **Custom (size, settings, apps)**.
5.	On the **Basics** blade, enter the following settings, and then click **Next**:
    -	Cluster name: **hbasetrain01**
    -	Subscription: **your subscription**
    -	Cluster type: 
        -	Cluster type: **HBase**
        -	Operating System: **Linux**
        -	Version: **HBase 1.1.2 (HDI 3.6)**
        -	Cluster Tier: **Standard**
    -	Cluster login username: **admin**
    -	Cluster login password: **Pa55w.rdPa55w.rd**
    -	Secure Shell (SSH) username: **sshuser**
    -	Use same password as cluster login: **selected** 
    -	Resource group (Create new): **hbasetrain01rg**
    -	Location: **your location**
6.	On the **Storage** blade, enter the following settings, and then click **Next**:
    -	Create a new Storage account: **hbasetrain01stg**
    -	Default container: **hbasetrain01ctr**
7.	On the **Applications** blade, you can install popular data science tools such as H20 or Dataiku or any other apps; however, for Interactive Hive Cluster, no applications are available. Click Next.
8.	On the **Cluster size** blade, enter the following settings, and then click **Next**:
    -	Number of Region nodes: **2**
    -	Region node size:** D3 v2 (2 nodes, 8 cores)**
    -	Head node size: **D3 v2 (2 nodes, 8 cores)**
    -	Zookeeper node sizes: **default**
9.	On the **Advanced settings** blade, click **Next**:
10.	On the **Cluster summary** blade, review the summary, and then click **Create**.
11.	Cluster Creation will be pinned to the Azure Portal dashboard and provisioning will take about 20 minutes to finish.
12.	When the cluster is provisioned, you are good to start connecting to HBase Cluster, loading data and performing interactive querying on the cluster.
13.	For detailed steps with screenshots, refer to the following document: **E:\Demofiles\Mod09\Create_Delete_HBase_Cluster_on_Azure.docx**.

### Ingest data into Phoenix
1.	Upload **Demo03_interactive_queries_with_phoenix** folder from the **E:\Demofiles\Mod09** folder into the Blob storage container (**hbasetrain01ctr**) attached to the HBase cluster you created in the previous step.
    -	Click **Start**, type **Microsoft Azure Storage Explorer**, and then press Enter.
    -	If the **Connect to Azure Storage** dialog box appears, click **Add an Azure Account**, and then click **Sign in**.
    -	In the **Sign in to your account** dialog box, enter your Azure credentials, and then click **Sign in**.
    -	In Microsoft Azure Storage Explorer, under **Storage Accounts**, expand **hbasetrain01stg**, expand **Blob Containers**, and then click **hbasetrain01ctr**.
    -	In the top menu, click **Upload**, and then click **Upload Folder**.
    -	Click the ellipsis (**…**), navigate to **E:\Demofiles\Mod09**, click the **Demo03_interactive_queries_with_phoenix** folder, click **Select Folder**, and then click **Upload**.
2.	SSH to cluster head node
    -	Click **Start**, type **Putty**, and then press Enter.
    -	In the **Hostname (or IP address)** box, type **sshuser@\<clustername\>-ssh.azurehdinsight.net**, and then click **Open**. Replace \<clustername\> with your cluster name. For example: **sshuser@hbasetrain01-ssh.azurehdinsight.net**.
    -	If the **PuTTY Security Alert** dialog box appears, click **Yes**.
    -	At the command prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.
    -	Run the following command to install **jq** (json parser), type **y**, and press Enter when prompted:
        ```
        sudo apt install jq
        ```
    -	Run the following command to install the dos2unix package:
        ```
        sudo apt install dos2unix
        ```
3.	Run the following command to copy the demo folder contents from HDFS to the local file system on the head node of the cluster:
    ```
    mkdir Demo03_interactive_queries_with_phoenix;hdfs dfs -copyToLocal /Demo03_interactive_queries_with_phoenix/*.* $HOME/Demo03_interactive_queries_with_phoenix
    ```
4.	Run the following command to copy the **uk_cities_population.csv** from the demo folder to the HDFS location at **/example/data**:
    ```
    hdfs dfs -copyFromLocal $HOME/Demo03_interactive_queries_with_phoenix/uk_cities_population.csv /example/data/
    ```
5.	Run the following command to convert the files into unix format:
    ```
    dos2unix $HOME/Demo03_interactive_queries_with_phoenix/*.*
    ```
6.	Run the following command to review the **first_phoenix_query.sql** file by opening the file:
    ```
    cat $HOME/Demo03_interactive_queries_with_phoenix/first_phoenix_query.sql
    ```
7.	Normally HBase cluster has three zookeeper nodes, and you need one of these nodes to be running every time you connect to Phoenix. Run the following curl command to get one of the zookeeper names:
    ```
    curl -u admin:Pa55w.rdPa55w.rd -sS -G "https://<CLUSTERNAME>.azurehdinsight.net/api/v1/clusters/<CLUSTERNAME>/services/ZOOKEEPER/components/ZOOKEEPER_SERVER" | jq '.host_components[].HostRoles.host_name' | head -1 | sed "s/\"//g"
    ```
    In the above command, replace **\<CLUSTERNAME\>** with your clustername. You need **ZOOKEEPER_NAME** in the following steps and throughout this demo and lab.
8.	Run the following command to get the Horton Works Data Platform version (**HDP_VERSION**):
    ```
    /usr/bin/hdp-select status|grep phoenix|head -1|cut -d " " -f3
    ```
    You need **HDP_VERSION** in the following step and throughout this demo and lab.
9.	Run the following command to load into the **REGION** table and to view the records:
    ```
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure $HOME/Demo03_interactive_queries_with_phoenix/first_phoenix_query.sql
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8 and the **\<ZOOKEEPER_NAME\>** with the zookeeper name you obtained in Step 7.
10.	Run the following command to create **UK_CITIES_POPULATION** and **UK_CITIES_POPULATION_MR** tables in Phoenix:
    ```
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure $HOME//Demo03_interactive_queries_with_phoenix/create_uk_cities_population.sql
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8 and the **\<ZOOKEEPER_NAME\>** with the zookeeper name you obtained in Step 7.
11.	Run the following command to load into the **UK_CITIES_POPULATION** table in single-threaded mode:
    ```
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py -t UK_CITIES_POPULATION <ZOOKEEPER_NAME>:2181:/hbase-unsecure $HOME/Demo03_interactive_queries_with_phoenix/uk_cities_population.csv
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8 and the **\<ZOOKEEPER_NAME\>** with the zookeeper name you obtained in Step 7.
12.	Run the following command to find the client jar version:
    ```
    cd /usr/hdp/<HDP_VERSION>/phoenix
    ls -lrt phoenix*client.jar
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8.
    From the above command, make a note of the version you see for the Phoenix client jar.
13.	Run the following command to load into **UK_CITIES_POPULATION_MR** table in map reduce mode:
    ```
    HADOOP_CLASSPATH=/usr/hdp/<HDP_VERSION>/hbase/lib/hbase-protocol.jar:/usr/hdp/<HDP_VERSION>/hbase/conf hadoop jar /usr/hdp/<HDP_VERSION>/phoenix/phoenix-<PHOENIX_CLIENT_JAR_VERSION>-client.jar org.apache.phoenix.mapreduce.CsvBulkLoadTool --table UK_CITIES_POPULATION_MR --input /example/data/uk_cities_population.csv
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8, the **\<ZOOKEEPER_NAME\>** with the zookeeper name you obtained in Step 7, and **\<PHOENIX_CLIENT_JAR_VERSION\>** obtained in Step 12. 
14.	Run the following command to verify the data loads:
    ```
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure $HOME/Demo03_interactive_queries_with_phoenix/verify_bulk_ingests.sql
    ```
    In the above command, replace the **\<HDP_VERSION\>** with the **HDP_VERSION** you obtained in step 8 and the **\<ZOOKEEPER_NAME\>** with the zookeeper name you obtained in Step 7.

### Perform interactive queries

1.	Run the following command to review the queries in the **query_uk_cities_population_table.sql** file, and then copy the contents to a notepad:
    ```
    cat $HOME/Demo03_interactive_queries_with_phoenix/query_uk_cities_population_table.sql
    ```
2.	Connect to Phoenix using SQLLine Console and start running the queries listed in **query_uk_cities_population_table.sql** step-by-step to complete this demo:
    -	Run the following command to launch SQLLine Console:
        ```
        python /usr/hdp/<HDP_VERSION>/phoenix/bin/sqlline.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure
        ```
        Replace the **\<HDP_VERSION\>** and **\<ZOOKEEPER_NAME\>** from the previous lab task. Now you are good to run Phoenix queries in a step-by-step fashion.
    -	Run the following command to count the number of rows in the **UK_CITIES_POPULATION** table in Step1:
        ```
        SELECT COUNT(*) 
        FROM UK_CITIES_POPULATION;
        ```
    -	Run the following command to fetch the cities from the **UK_CITIES_POPULATION** table in Step2:
        ```
        SELECT CITYNAME 
        FROM UK_CITIES_POPULATION;
        ```
    -	Run the following command to count the number of rows in the **UK_CITIES_POPULATION** table in Step3:
        ```
        SELECT REGIONID, 
            CITYNAME, 
            POPULATION 
        FROM UK_CITIES_POPULATION 
        WHERE CITYNAME='London';
        ```
    -	Run the following command to count the number of rows in the **UK_CITIES_POPULATION** table in Step4:
        ```
        SELECT REGIONID, 
            COUNT(*) 
        FROM UK_CITIES_POPULATION 
        GROUP BY REGIONID;
        ```
    -	Run the following command to count the number of rows in the **UK_CITIES_POPULATION** table in Step5:
        ```
        SELECT REGIONID, 
            SUM(POPULATION) 
        FROM UK_CITIES_POPULATION 
        WHERE REGIONID=1000001
        GROUP BY REGIONID;
        ```
    -	Run the following command to count the number of rows in the **UK_CITIES_POPULATION** table in Step6:
        ```
        SELECT B.REGIONNAME AS "Country", COUNT(A.CITYNAME) AS "City Count", SUM(A.POPULATION) as "Total Population"
        FROM UK_CITIES_POPULATION A
        INNER JOIN
        REGION B
        ON A.REGIONID = B.REGIONID 
        GROUP BY B.REGIONNAME
        ORDER BY SUM(A.POPULATION) DESC;
        ```

>**Note:** This is the final demonstration in this module. Make sure you delete the clusters before you start the next module.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.