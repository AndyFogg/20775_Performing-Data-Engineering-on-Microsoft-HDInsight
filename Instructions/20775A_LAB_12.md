# Module 12: Developing Real-time Processing Solutions with Apache Storm

## Lab: Building an Apache Storm Application

### Scenario

You have access to a stream of data generated by the meters in a fleet of taxis in New York. The data includes information about each trip, including the time and location of each pickup, the time and location of each drop-off, and the price, tax, and tips paid. You want to create a streaming application in HDInsight that receives events from this data stream, saves the data to Azure Table Storage (for further offline analysis) calculates summary statistics and displays information in Power BI, and records information about disputed rides to Azure SQL Database, also for possible future analysis.

You want to calculate the following summary statistics based on the live data stream:

-   The number of trips currently running.

-   The average number of passengers.

-   The average distance for a trip.

-   The average fare for a trip.

You have been asked to build this application as an Apache Storm application. You will use an Azure Event Hub as a source of taxi event data.

### Objectives

At the end of this lab, you will be able to:

-   Install client software required for working with Apache Storm streaming applications.

-   Configure an Azure Event Hub and use it as a source of data.

-   Populate an Azure Event Hub with sample data.

-   Create an Apache Storm streaming topology that can be hosted in Azure HDInsight.

-   Write data from a Storm topology to an Azure Storage table.

-   Display Apache Storm topology data in a Power BI dashboard.

-   Save data from a Storm topology to Azure SQL Database.

### Lab Setup

-   **Estimated Time**: 90 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

> **Note:** This lab only runs the topology locally. This is to save time and resources. If students have sufficient time at the end of the lab (and they have sufficient resources available in their Azure subscription), they can try deploying the topology to an HDInsight cluster and running it, following the process described in Lesson 3.

## Exercise 1: Installing required software

### Scenario

Before you can build an Apache Storm streaming application that analyses a taxi data stream, you must install and configure the required software on your development machine.

The main tasks for this exercise are as follows:

1. Install IntelliJ IDEA Community edition

2. Configure IntelliJ IDEA

3. Install PuTTy

#### Task 1: Install IntelliJ IDEA Community edition

1.  Install the **IntelliJ Community Edition** on to the 20775A-LON-DEV virtual machine, from **https://www.jetbrains.com/idea/download/index.html#section=windows**.

2.  When the installation is complete, run **IntelliJ IDEA Community Edition**, without importing any settings, and install the **Scala** plugin.

#### Task 2: Configure IntelliJ IDEA

1.  In Customize IntelliJ IDEA, create a new **Scala** **IDEA** project.

2.  On the **New Project** page, download and install the Java JDK, using **C:\\Program Files\\Java\\jdk1.8.0_131**, as the home directory.

3.  On the **New Project** page, download and install the Scala version 2.12.2 SDK. If this errors, restart IntelliJ IDEA and repeat up to this point (you can skip the downloading of Java).

4.  Finish the new project configuration.

5.  If you get a **Windows Security Alert** dialog box for the **OpenJDK Platform binary**, click **Allow access**.

6.  If you get a **Windows Security Alert** dialog box for **IntelliJ IDEA**, click **Allow access**.

7.  Disable the **Android Support** plugin.

8.  From the available repositories, install the **Azure Toolkit for IntelliJ**.

9.  Close IntelliJ IDEA.

#### Task 3: Install PuTTY (if not already installed)

1.  Navigate to **http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html**.

2.  Download the Putty.exe file to a convenient location and create a shortcut on your desktop.

**Results**: At the end of this exercise, you will have a Windows client with IntelliJ IDEA Community Edition, the Java SE Development Kit, the Scala SDK, and the PuTTY tool installed for use later in the lab.

> **Note:** If you already have all these components installed in the 20775A-LON-DEV virtual machine, you can skip this exercise.

## Exercise 2: Configure event hubs

### Scenario

The Apache Storm streaming topology will obtain data about taxi trips from an Azure event hub. In this exercise, you will configure such an event hub.

The main tasks for this exercise are as follows:

1. Create an Event Hubs namespace

2. Create an Event Hub

3. Record the Shared Access Primary Key

#### Task 1: Create an Event Hubs namespace

-   In the Azure Portal, create a new Event Hubs namespace. Use the following information:

    -   **Name**: \<*your name\>\<date*\>ehn (must be unique)

    -   **Pricing tier**: Basic

    -   **Subscription**: your subscription

    -   **Resource group (Create new)**: StormRG

    -   **Location**: Choose your region

    -   **Throughput Units**: 1

#### Task 2: Create an Event Hub

-   In the Azure Portal, create an event hub within the Event Hubs namespace you created in the previous task. Use the following information:

    -   **Name**: TaxiEventSource

    -   **Partition Count**: 2

#### Task 3: Record the Shared Access Primary Key

-   In the Azure Portal, access the **RootManageSharedAccessKey** shared access policy for the Event Hub namespace. Copy the primary key for this policy to a text file and save the text file.

**Results**: At this end of this exercise, you will have an event hub and namespace configured in your Azure subscription.

## Exercise 3: Populate the event hub with data

### Scenario

The Event Hub Sender project can populate an Azure event hub with data. In this exercise, you will configure the application to populate the event hub you created in the last exercise. You will then run the Event Hub Sender application to store taxi data in the event hub.

The main tasks for this exercise are as follows:

1. Configure the Event Hub Sender

2. Run the Event Hub Sender

#### Task 1: Configure the Event Hub Sender

1.  Start IntelliJ and open the project in the **E:\\Labfiles\\Lab12\\Starter\\EventHubSender** folder.

2.  Add a new run configuration to the project with the following values, and then run the project:

    -   **Name**: MyEventHubSenderConfig

    -   **Main**: EventhubClientDriver

    -   **--sourcefilename program argument**: E:\\Labfiles\\Lab12\\yellow_tripdata_2015-01.csv

    -   **--eventhubs-namespace program argument**: *\<your name\>\<date\>ehn*

    -   **--eventhubs-name program argument**: TaxiEventSource

    -   **--policy-name program argument**: RootManageSharedAccessKey

3.  Copy the primary key value from the text file you created in the last exercise. Use this value for the --policy-key program attribute in your new IntelliJ run configuration.

#### Task 2: Run the Event Hub Sender

1.  Run the IntelliJ project for a few minutes. The project adds data from the **yellow_tripdata_2015-01.csv** file to the event hub in Azure.

2.  Close the project.

**Results**: At the end of this exercise, you will have an event hub that contains taxi data. You will use this event hub as a source of data for the Storm topology.

## Exercise 4: Create a Storm streaming topology

### Scenario

Now that you have a suitable data source, you can begin building the Apache Storm streaming topology that will process and distribute your data. In this exercise, you will add code that obtains data and parses it.

The main tasks for this exercise are as follows:

1. Add an Event Hubs Spout

2. Add a Parser Bolt

3. Configure a new shared access policy

4. Configure the Storm streaming topology

5. Test the Storm topology

#### Task 1: Add an Event Hubs Spout

1.  Use IntelliJ IDEA to open the project in the folder **E:\\Labfiles\\Lab12\\Starter\\StormProcessor - Exercise 4**.

2.  In the StormTopologyDriver file, add code that creates a new EventHubSpout object using the eventHubSpoutConfig configuration.

3.  Add code that creates a new TopologyBuilder object and adds the EventHubSpout you just created to the topology.

#### Task 2: Add a Parser Bolt

1.  In IntelliJ IDEA, open the ParserBolt code file.

2.  Add code to the file that defines the fields pass to the next bolt.

3.  Add code that takes the next record retrieved from Azure.

4.  Add code that creates a Values object containing the values of individual fields from the JSON data.

5.  Add code that sends the data on to the next bolt.

6.  In the StormTopologyDriver code file, add code that adds the parser bolt to the topology and attaches it to the event hubs spout.

#### Task 3: Configure a new shared access policy

-   In the Azure Portal, in the event hub namespace you created in Exercise 2, add a new shared access policy named **ListenRule**. Grant the new policy Listen claims only.

#### Task 4: Configure the Storm streaming topology

1.  Edit the StormProcessor application configuration with the following values, and then run the project:

    -   **--eventhubs-namespace**: \<*your name\>\<date*\>ehn

    -   **--eventhubs-name**: TaxiEventSource

    -   **--policy-name**: ListenRule

2.  Copy the primary key value for the ListenRule policy from the Azure Portal. Use this value for the --policy-key program attribute in your new IntelliJ run configuration.

#### Task 5: Test the Storm topology

1.  In IntelliJ, in the StormTopologyDriver code file, add code that builds the topology from the definition, starts the topology using the local cluster, and stops the local cluster after a time.

2.  Run the application using the **MyEventHubSenderConfig** configuration.

3.  When the cluster stops, or you decide to terminate it, close the project.

**Results**: At the end of this exercise, you will have an Apache Storm streaming application that obtains data from an Azure event hub and parses that data.

## Exercise 5: Write data from the Storm topology to an Azure Storage table

### Scenario

You want to store information about taxi trips in Azure Table Storage. This will enable you to perform additional analyses later. In this exercise, you will add a bolt to your Apache Storm topology that writes the streamed data to a table.

The main tasks for this exercise are as follows:

1. Create an Azure Storage account

2. Write data to the Azure Storage table

3. Add the save table bolt to the Storm topology

4. Configure and test the updated topology

5. View the data saved by the SaveToTableBolt

#### Task 1: Create an Azure Storage account

-   In the Azure Portal, create a new storage account. Use the following information:

    -   **Name**: \<*your name\>\<date*\>sta (must be unique)

    -   **Deployment model**: Resource manager

    -   **Account kind**: General purpose

    -   **Performance**: Standard

    -   **Replication**: Locally redundant storage

    -   **Storage encryption**: Disabled

    -   **Secure transfer required**: Disabled

    -   **Subscription**: Choose your subscription

    -   **Resource Group**: StormRG

    -   **Location**: Choose your region

#### Task 2: Write data to the Azure Storage table

1.  In IntelliJ IDEA, open the **E:\\Labfiles\\Lab12\\Starter\\StormProcessor - Exercise 5 project**.

2.  In the SaveToTableBolt code file, add code that:

    -   Retrieves input fields.

    -   Connects to the Azure Storage account.

    -   Creates a table in the storage account if one does not already exist.

    -   Creates a table entity containing the taxi event data.

    -   Passes the data on to the next bolt in the topology.

#### Task 3: Add the save table bolt to the Storm topology

-   In the StormTopologyDriver code file, add code that adds the SaveToTable bolt to the topology and attaches it to the parser bolt.

#### Task 4: Configure and test the updated topology

1.  Open the StormProcessor - Exercise 4 project and copy the program arguments from the **StormProcessor** application configuration to the clipboard.

2.  Edit the existing configuration to the StormProcessor - Exercise 5 project. Paste the clipboard text into the Program arguments text box.

3.  Add the following information to the new application configuration:

    -   **--storage-account**: \<*your name\>\<date*\>sta

    -   **--storage-key**: Copy the storage key from the Key1 in the list of access keys for the storage account in the Azure Portal.

4.  Run the application using your new configuration. When the cluster stops, or you decide to terminate it, close the project.

#### Task 5: View the data saved by the SaveToTableBolt

1.  Start Azure Storage Explorer, ensure that **Add an Azure Account** is selected, and then sign in using the credentials of the Microsoft account that is associated with your Azure Learning Pass subscription.

2.  Under your Azure Learning Pass subscription, find your storage account, and view anyone of the **TaxiData+\<date\>** tables.

3.  Close Microsoft Azure Storage Explorer.

**Results**: At the end of this exercise, you will have an Apache Storm topology that stores data in an Azure Storage table.

## Exercise 6: Display data in Power BI

### Scenario

You want to view summary statistics about taxi trips in a Power BI Dashboard. In this exercise, you will create such a dashboard, and then modify your Apache Storm topology to send data to it. The bolt that you create will run after the Parser bolt, in parallel with the SaveToTable bolt.

The main tasks for this exercise are as follows:

1. Create a Power BI streaming dataset

2. Create a Power BI dashboard

3. Add the Statistics bolt to the Storm topology

4. Configure and test the updated topology

#### Task 1: Create a Power BI streaming dataset

1.  Browse to the Power BI portal and sign in using your Power BI credentials.

2.  Create a new API streaming dataset named Taxi Data. Use the following values:

    -   **Passengers**: Number

    -   **Distance**: Number

    -   **Fares**: Number

    -   **Tips**: Number

    -   **AvgPassengers**: Number

    -   **AvgDistance**: Number

    -   **AvgFare**: Number

    -   **AvgTip**: Number

3.  Copy the Push URL for the new streaming dataset to a new text file in notepad. Save the file for later use.

#### Task 2: Create a Power BI dashboard

1.  In the Power BI portal, create a new dashboard called Taxi Data.

2.  Add card tiles for the following fields to the dashboard:

    -   **Passengers**: text label Number of Passengers

    -   **Distance**: text label Distance

    -   **Fares**: text label Fares

    -   **Tips**: text label Tips

    -   **AvgPassengers**: text label Average Passengers

    -   **AvgDistance**: text label Average Distances

    -   **AvgFare**: text label Average Fare

    -   **AvgTip**: text label Average Tip

#### Task 3: Add the Statistics bolt to the Storm topology

1.  In IntelliJ IDEA, open the **E:\\Labfiles\\Lab12\\Starter\\StormProcessor - Exercise 6 project**.

2.  In the StatisticsBolt code file, write code that:

    -   Iterates through the tuples for the current time window and accumulates the results.

    -   Sends the results to the Power BI dashboard.

    -   Formats the data as JSON.

    -   Posts the data to the endpoint for the Power BI dashboard.

    -   Logs the responses.

    -   Define the format of the JSON data sent to Power BI.

3.  In the StormTopologyDriver code file, write code that adds the Statistics bolt to the topology and attaches it to the Parser bolt. Note that you should only run a single instance of this bolt.

#### Task 4: Configure and test the updated topology

1.  Open the StormProcessor - Exercise 5 project and copy the program arguments from the MyStormProcessorConfig application configuration to the clipboard.

2.  Add a new application configuration to the StormProcessor - Exercise 6 project. Paste the clipboard text into the Program arguments text box.

3.  Add the following information to the new application configuration:

    -   **Name**: MyStormProcessorConfig

    -   **Main class**: StormTopologyDriver

    -   **--dashboard-endpoint program argument**: Copy the endpoint from the text file you created in the first task of this exercise.

4.  Run the application using your new configuration and examine statistics in Power BI. When the cluster stops, or you decide to terminate it, close IntelliJ IDEA.

**Results**: At this end of this exercise, you will have an Apache Storm topology that communicates data to Power BI for display.

## Exercise 7: Persist data from the Storm topology to Azure SQL Database

### Scenario

You want to record information about disputed taxi trips to a table in Azure SQL Database. This data consists of the following information:

-   A unique reference ID.

-   The pickup date and time.

-   The dropoff date and time.

-   The number of passengers.

-   The fare charged.

-   The payment type.

In this exercise, you will add a bolt to your Apache Storm topology that writes the streamed data to Azure SQL Database. You will configure the topology so that this new bolt runs directly after the **SaveToTable** bolt.

The main tasks for this exercise are as follows:

1. Create an Azure SQL Server and Database

2. Add the SaveToSQL bolt to the Storm topology

3. Configure and test the updated topology

4. View the data saved by the SaveToSQL bolt

5. Lab clean-up

#### Task 1: Create an Azure SQL Server and Database

1.  In the Azure Portal, create a new Azure SQL Database and Server with the following properties:

    -   **Database name**: \<*your name\>\<date*\>db (must be unique)

    -   **Subscription**: Your subscription

    -   **Resource group**: StormRG

    -   **Select source**: Blank database

    -   **Server**: Create a new server with the following values:

    -   **Server name**: \<*your name\>\<date*\>srv

    -   **Server admin login name**: student

    -   **Password**: Pa55w.rd

    -   **Confirm password**: Pa55w.rd

    -   **Location**: Accept the default value

    -   **Allow azure services to access server**: selected

    -   **Want to use SQL elastic pool**: Not now

    -   **Pricing tier**: Accept the default pricing tier

    -   **Collation**: Accept the default value

2.  Open the SQL Server firewall to permit access to the IP address of your computer.

3.  Use the SQL database tools to connect to the database and run the script **disputed_trips.sql** located in the **E:\\Labfiles\\Lab12\\Starter** folder on your virtual machine. This script creates the **disputed_trips** table.

#### Task 2: Add the SaveToSQL bolt to the Storm topology

1.  In IntelliJ IDEA, open the **E:\\Labfiles\\Lab12\\Starter\\StormProcessor - Exercise 7** project.

2.  In the SaveToSQLBolt code file, write code that:

    -   Creates a connection string for accessing your Azure SQL Database. The connection parameters will be passed in as the following command line parameters:

    -   **ClientArgumentKeys.SQLServer** (the URL of the server hosting the database)

    -   **ClientArgumentKeys.SQLDatabase** (the name of the database),

    -   **ClientArgumentKeys.SQLAccountName** (the login name of the account to use - student)

    -   **ClientArgumentKeys.SQLPassword** (the password for this account - Pa55w.rd)

    -   Uses the connection string to connect to the database.

    -   Inserts a new record into the **disputed_trips** table in the database using the information in the current record in the stream.

    -   Releases resources associated with running an Insert statement

    -   Closes the connection to the database.

3.  In the StormTopologyDriver code file, write code that adds the SaveToSQL bolt to the topology and attaches it to the SaveToTable bolt.

#### Task 3: Configure and test the updated topology

1.  Open the **StormProcessor - Exercise 6** project and copy the program arguments from the MyStormProcessorConfig application configuration to the clipboard.

2.  Add a new application configuration to the StormProcessor - Exercise 7 project. Paste the clipboard text into the Program arguments text box.

3.  Add the following information to the new application configuration:

    -   **Name**: MyStormProcessorConfig

    -   **Main class**: StormTopologyDriver

    -   **--sql-account-name program argument**: student.

    -   **--sql-password program argument**: Pa55w.rd

    -   **--sql-server program argument**: The name of your Azure SQL Database server (note that you should not include the suffix *.database.windows.net* - this will be appended by your code)

    -   **--sql-database**: The name of your database

4.  Run the application using your new configuration and examine statistics in Power BI. When the cluster stops, or you decide to terminate it, close IntelliJ IDEA.

#### Task 4: View the data saved by the SaveToSQL bolt

1.  Using the Azure Portal, return to the **Query editor** for your database.

2.  Run the following query:
    ````
    SELECT * FROM taxi.disputed_trips
    ````

3.  Verify that the data for a number of disputed trips is displayed.

4.  Close Internet Explorer.

#### Task 5: Lab clean-up

1.  In the Microsoft Azure Portal, delete your resource group.

2.  Verify that the event hub, SQL server, SQL database, and the storage account that were created, have all been removed.

**Results**: At the end of this exercise, you will have an Apache Storm topology that writes data to Azure SQL Database.

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
