# Module 8: Analyzing Data with Spark SQL

## Lab: Performing exploratory data analysis by using iterative and interactive queries

### Scenario

You work as a consultant for Contoso, a large IT consultancy that has consultants in different industrial sectors. You have been asked to perform some exploratory analysis on a dataset from one of Contoso's clients, a multinational real estate management company. The client is hoping to use data collected from 20 large buildings around the world to analyze usage of heating, ventilation, and air-conditioning (HVAC) systems.

In this lab, you will use Jupyter Notebooks to build a machine learning model, and Zeppelin for interactive data exploration, modeling, and visualization. Finally, you will use Livy to interact with your Spark cluster.

### Objectives

After completing this lab, you will be able to:

-   Build a machine learning application by using Jupyter Notebooks.

-   Use Zeppelin notebooks for interactive data analysis.

-   View and manage Spark sessions by using Livy.

### Lab Setup

This lab requires that you have access to the resources outlined in the *Prerequisites* topic in *Module Overview* above.

-   **Estimated Time**: 45 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

## Exercise 1: Build a machine learning application

### Scenario

You want to create an iterative machine learning application to predict whether a building will be hotter or colder than its target temperature, based on the system identity and system age.

> **Note:** A text file that contains the code used in this lab can be found at **E:\\Labfiles\\Lab08\\HVAC MLlib.txt** on the virtual machine that accompanies this course.

The main tasks for this exercise are as follows:

0. Prepare the environment

1. Review the data

2. Create an iterative machine learning application by using Jupyter Notebooks

#### Task 0: Prepare the environment

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.

2.  Connect to the Azure Portal, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

3.  Create a new HDInsight cluster with the following details:

    -   **Cluster name**: \<*your name\>\<date*\>hdi

    -   **Cluster type**: Spark

    -   **Version**: 2.1.0 (HDI 3.6)

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rdPa55w.rd
    
    -   **Secure Shell (SSH) username**: sshuser

    -   **Use same password as cluster login**: selected

    -   **Resource group (Create new)**: SparkSQLrg

    -   **Location**: Select your region

    -   **Storage account (Create new)**: \<*your name\>\<date*\>sa

    -   **Default container**: replace the suggested name with \<*the name of your cluster*\>-ctr (for example, \<*your name\>\<date*\>hdi-ctr).

    -   **Cluster size**:
    
        -   Number of Worker nodes: 4

        -   **Worker node size**: A3 General Purpose

        -   **Head node size**: A3 General Purpose

4.  The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

#### Task 1: Review the data

-   Review the raw data used in this exercise. You can download the file from the Azure Portal. The **HVAC.csv** data file is in the **\\HdiSamples\\HdiSamples\\SensorSampleData\\hvac** folder of the Azure Storage account linked to your HDInsight cluster.
    > **Note**: A copy of the file is available at **E:\\Labfiles\\Lab08\\Soultion\\HVAC.csv**.

    The data shows the target temperature and the actual temperature of a building that has an HVAC system. The **System** column represents the system ID and the **SystemAge** column represents the number of years that the HVAC system has been in place at the building.

    You will use this data to predict whether a building will be hotter or colder than the target temperature, given a system ID and system age.

#### Task 2: Create an iterative machine learning application by using Jupyter Notebooks

1.  Create a new Jupyter notebook by using the **PySpark** kernel. Name the notebook **HVAC-ML**.

2.  In the first cell of the notebook, type the following code to import the libraries required for this application.
    ````
    from pyspark.ml import Pipeline
    from pyspark.ml.classification import LogisticRegression
    from pyspark.ml.feature import HashingTF, Tokenizer
    from pyspark.sql import Row

    import os
    import sys
    from pyspark.sql.types import *

    from pyspark.mllib.classification import LogisticRegressionWithSGD
    from pyspark.mllib.regression import LabeledPoint
    from numpy import array
    ````

3.  In a new cell, type the following code to load and parse the data.
    ````
    # 0 Date
    # 1 Time
    # 2 TargetTemp
    # 3 ActualTemp
    # 4 System
    # 5 SystemAge
    # 6 BuildingID

    LabeledDocument = Row("BuildingID", "SystemInfo", "label")

    # Define a function that parses the raw CSV file and returns an object of type LabeledDocument

    def parseDocument(line):
        values = [str(x) for x in line.split(',')]
        if (values[3] > values[2]):
            hot = 1.0
        else:
            hot = 0.0        

        textValue = str(values[4]) + " " + str(values[5])

        return LabeledDocument((values[6]), textValue, hot)

    # Load the raw HVAC.csv file, parse it using the function
    data = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    documents = data.filter(lambda s: "Date" not in s).map(parseDocument)
    training = documents.toDF()
    ````

4.  In a new cell, type the following code to configure the machine learning pipeline, and to fit the training data to the pipeline.
    ````
    tokenizer = Tokenizer(inputCol="SystemInfo", outputCol="words")
    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
    lr = LogisticRegression(maxIter=10, regParam=0.01)
    pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

    model = pipeline.fit(training)
    ````

5.  In a new cell, type the following code to verify the training
    process.
    ````
    training.show()
    ````

6.  Run all of the cells in the model to view your progress. The output
    should appear as follows.
    ````
    +----------+----------+-----+

    |BuildingID|SystemInfo|label|

    +----------+----------+-----+

    | 4| 13 20| 0.0|

    | 17| 3 20| 0.0|

    ...
    ````

    **SystemInfo** represents the prediction data (SystemId and age). **Label** represents whether the building is hotter (**1.0**) or colder (**0.0**) than the target temperature.

7.  In a new cell, type the following code to create a test dataset.
    ````
    # SystemInfo here is a combination of system ID followed by system age
    Document = Row("id", "SystemInfo")
    test = sc.parallelize([(1L, "20 25"),
                (2L, "4 15"),
                (3L, "16 9"),
                (4L, "9 22"),
                (5L, "17 10"),
                (6L, "7 22")]) \
        .map(lambda x: Document(*x)).toDF()
    ````

8.  In a new cell, add the following code to display the predictions for the test dataset.
    ````
    # Make predictions and display the results
    prediction = model.transform(test)
    selected = prediction.select("SystemInfo", "prediction", "probability")
    for row in selected.collect():
        print row
    ````

9.  Run all of the cells. The output of the final cell should appear as follows.
    ````
    Row(SystemInfo=u'20 25', prediction=1.0,
    probability=DenseVector([0.4999, 0.5001]))

    Row(SystemInfo=u'4 15', prediction=0.0,
    probability=DenseVector([0.5016, 0.4984]))

    ...
    ````

    The first value of the **DenseVector** array is the predicted probability that the building will be cold. The second value is the probability that the building will be hot. The higher value selects the value of **prediction** (0.0 = colder; 1.0 = hotter).

10. Experiment with the model by changing or adding data to the test dataset. When you have finished, leave the notebook running.

11. **Note:** You should normally close and halt a notebook when you have finished running it, but for the purposes of this lab, leave this notebook running.

**Results**: At the end of this exercise, you will have built a machine learning application with an Apache Spark cluster in HDInsight by using Jupyter Notebooks.

## Exercise 2: Use Zeppelin for interactive data analysis

### Scenario

You decide to use Zeppelin to analyze and visualize the HVAC dataset.

> **Note:** A text file that contains the code used in this lab can be found at **E:\\Labfiles\\Lab08\\HVAC Zeppelin.txt** on the virtual machine that accompanies this course.

The main tasks for this exercise are as follows:

1. Launch Zeppelin and load data

2. Load data into a temporary table

3. Use interactive queries for data analysis

#### Task 1: Launch Zeppelin and load data

-   In the Azure Portal, open a new Zeppelin note called **HVAC-zep** that is connected to your HDInsight on Azure instance.

#### Task 2: Load data into a temporary table

1.  In the new notebook, register a temporary table called **hvac** that contains the HVAC data (contained in the cluster storage account at **\\HdiSamples\\SensorSampleData\\hvac**\\**HVAC.csv**). Use the following code as a basis.
    ````
    %livy.spark
    //The above magic instructs Zeppelin to use the Livy Scala interpreter

    // Create an RDD using the default Spark context, sc
    val hvacText = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    // Define a schema
    case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)

    // Map the values in the .csv file to the schema
    val hvac = hvacText.map(s => s.split(",")).filter(s => s(0) != "Date").map(
        s => Hvac(s(0), 
                s(1),
                s(2).toInt,
                s(3).toInt,
                s(6)
        )
    ).toDF()

    // Register as a temporary table called "hvac"
    hvac.registerTempTable("hvac")
    ````

2.  If the process is successful, you will see the following result.
    ````
    hvacText: org.apache.spark.rdd.RDD[String] =
    wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv
    MapPartitionsRDD[1] at textFile at <console>:27

    defined class Hvac

    hvac: org.apache.spark.sql.DataFrame = [date: string, time: string,
    targettemp: int, actualtemp: int, buildingID: string]
    ````

#### Task 3: Use interactive queries for data analysis

1. In the next paragraph of the notebook, run an SQL query to select **buildingId**, the difference between **targettemp** and **actualtemp** aliased as **temp_diff**, and the **date** from the **hvac** table for dates greater than **6/1/13**.
    >**Hint:** The paragraph should start with the **%sql** keyword.

2.  Display the output as a bar graph.

3.  Display the output as a line graph.

4.  Change the settings to use **date** as the key of the results, use **buildingId** as the group, and use the **MAX** value of **temp_diff** as the value. Notice how the line graph changes to reflect the new settings. Experiment with the other visualizations available.

5.  In a new paragraph, create a parameterized SQL query that selects **buildingId**, **date**, **targettemp**, and the difference between **targettemp** and **actualtemp** aliased as **temp_diff** from the **hvac** table. Create a parameter called *Temp* for the **targettemp** column to select values greater than 65, 68, or 70. Use 65 as the default value. You can use the following as the basis for your code.
    ````
    %sql
    select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff from hvac where targettemp > "${Temp = 65,65|68|70}"
    ````

6.  Execute the query, and then use the *Temp* variable to filter the results.

> **Note:** You should normally halt a notebook by restarting the Livy interpreter when you have finished running it, but for the purposes of this lab, leave this notebook running.
> 
> Instructions for halting a notebook (**do not run in this lab**)
> 
> 1.  To close the notebook, at the upper-right of the page, click **anonymous**, and then click **Interpreter**.
> 2.  In the entry for the Livy interpreter, click **restart**.

**Results**: At the end of this exercise, you will have used Zeppelin to interactively analyze a data file. You will also have learned how to import an external package into Zeppelin.

## Exercise 3: View and manage Spark sessions by using Livy

### Scenario

While working on data analysis tasks, you decide to monitor the status of all Livy sessions that are connected to your HDInsight on Azure instance.

The main tasks for this exercise are as follows:

1. View Livy session information by using the REST API

2. Terminate Livy sessions by using the REST API

3. Remove all Azure resources

#### Task 1: View Livy session information by using the REST API

1.  Using PowerShell, use the HTTP GET verb to query the list of all sessions using the **/sessions** endpoint of your HDInsight cluster's Livy instance.

2.  Use the HTTP GET verb to query the details of one of the sessions in the list by using the **/sessions/{sessionid}** endpoint of your HDInsight cluster's Livy instance.

3.  Use the HTTP GET verb to query information about statements that were used in the session that you used in the previous step, using the **/sessions/{sessionid}/statements** endpoint of your HDInsight cluster's Livy instance.

#### Task 2: Terminate Livy sessions by using the REST API

1.  Using PowerShell, use the HTTP DELETE verb to close a session by using the **/sessions/{sessionid}** endpoint of your HDInsight cluster's Livy instance. Repeat this step until no sessions remain.

2.  Close PowerShell and any open browser windows.

#### Task 3: Remove all Azure resources

1.  Delete the HDInsight cluster and storage account, by deleting the associated resource group.

2.  Verify that all Azure resources have been removed.

**Results**: At the end of this exercise, you will have viewed and run Livy sessions through the Livy REST API.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
