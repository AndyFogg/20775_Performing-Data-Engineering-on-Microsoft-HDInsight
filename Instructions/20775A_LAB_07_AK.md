# Module 7: Design Batch ETL Solutions for Big Data with Spark

## Lab: Working with Spark ETL

### Exercise 1: Design a Spark ETL application

#### Task 0: Prepare the environment

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**. 

2.  In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**.  

3.  In the Azure Portal, in the left pane, click **+ Create a resource**. 

4.  Click **Analytics**, and then click **HDInsight**. 

5.  On the **HDInsight** blade, click **Custom (size, settings, apps)**. 

6.  On the **Basics** blade, type the following details, and then click **Cluster type**: 
    -   **Cluster name**: \<*your name\>\<date*\>hdi
    -   **Subscription**: \<your subscription\>

7.  On the **Cluster configuration** blade, enter the following details, and then click **Select**: 
    -   **Cluster type**: Spark
    -   **Version**: 2.3.0 (HDI 3.6)

8.  On the **Basics** blade, enter the following details, and then click **Next**: 
    -   **Cluster login username**: sparkadmin
    -   **Cluster login password**: Pa55w.rdPa55w.rd
    -   **Secure Shell (SSH) username**: sshadmin
    -   **Use same password as cluster login**: selected
    -   **Resource group (Create new)**: Sparkrg
    -   **Location**: Select your region

9.  On the **Security + networking** blade, click **Next**. 

10. On the **Storage** blade, under **Select a Storage account**, click **Create new**. 

11. In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**; point out that this name must be globally unique. 

12. In the **Default container** box, replace the suggested name with the name of your cluster (for example, **\<*your name\>\<date*\>-ctr**). 

13. Leave all other settings at their defaults, and then click **Next**. 

14. On the **Applications (optional)** blade, click **Next**. 

15. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **1**. 

16. Click **Worker node size**. 

17. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**. 

18. Click **Head node size**. 

19. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**. 

20. On the **Cluster size** blade, click **Next**. 

21. On the **Script actions** blade, click **Next**. 

22. On the **Cluster summary** blade, point out the estimated cost per hour of this cluster. Remember that, to avoid using up your Azure Pass allowance, it is important that you remove clusters when you are not using them. 

23. On the **Cluster summary** blade, click **Create**. 

24. The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned.

25. In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>hdi**, and then ensure the status shows as **Running**.


#### Task 1: Design the ETL

1.  On the **\<*your name\>\<date*\>hdi** blade, in the **Cluster dashboards** section, click **Jupyter notebook**.

2.  If a **Windows Security** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**.

3.  On the **Jupyter** page, click the **New** drop-down list, and then click **PySpark**.

4.  If the new tab opens with the **404 : Not Found** message, complete the following steps:

    1.  Close the tab.

    2.  In the **Creating Notebook Failed** dialog box, click **OK**.

    3.  On the **Jupyter** home page, click **Untitled.ipynb**.

5.  In the new notebook, click **Untitled**, type **lab-etl**, and then click **OK**.

6.  In the first cell of the notebook, type the following code:
    > **Note**: A text file containing this code can be found at **E:\\Labfiles\\Lab07\\exercise_1_cell_1.txt**.
    ````
    from pyspark.sql import Row
    text_file = sc.textFile("wasbs:////HdiSamples/HdiSamples/MahoutMovieData/user-ratings.txt")
    split_data = text_file.map(lambda line: line.split("\t"))
    ratings = split_data.map(lambda p: Row(movieId=int(p[1]), userRating=int(p[2]) * 2, timestamp=int(p[3])))
    schemaRatings = sqlContext.createDataFrame(ratings)
    schemaRatings.registerTempTable("allUserRatings")
    ````

7.  Press SHIFT+ENTER to create a new cell, in the new cell, type the following code:
    > **Note**: A text file containing this code can be found at **E:\\Labfiles\\Lab07\\exercise_1_cell_2.txt**.
    ````
    output = sqlContext.sql("select * from allUserRatings limit 5")
    for eachrow in output.collect():
        print(eachrow)
    ````

8.  On the **Cell** menu, click **Run All**. When the cells have finished running, review the output of cell 2.

#### Task 2: Create the ETL application

1.  Open Notepad, type the following code into a new text file:
    > **Note**: A text file containing this code can be found at **E:\\Labfiles\\Lab07\\lab-7-etl.txt**.
    ````
    from pyspark import SparkContext, SparkConf, SQLContext

    conf = SparkConf().setAppName("lab-7-etl")
    sc = SparkContext(conf=conf)
    sqlContext = SQLContext(sc)

    from pyspark.sql import Row

    text_file = sc.textFile("wasbs:////HdiSamples/HdiSamples/MahoutMovieData/user-ratings.txt")
    split_data = text_file.map(lambda line: line.split("\t"))
    ratings = split_data.map(lambda p: Row(movieId=int(p[1]), userRating=int(p[2]) * 2, timestamp=int(p[3])))
    schemaRatings = sqlContext.createDataFrame(ratings)
    schemaRatings.registerTempTable("allUserRatings")
    ````

2.  On the **File** menu, click **Save As**.

3.  In the **Save As** dialog box, navigate to **E:\\Labfiles\\Lab07**, in the **File name** box, type **lab-7-etl.py**, and then click **Save**.

4.  Close Notepad.

5.  In the Azure Portal, click **All resources**, and then click **\<*your name\>\<date*\>sa**.

6.  On the **\<*your name\>\<date*\>sa** blade, under **Blob service**, click **Blobs**, and then click **\<*your name\>\<date*\>hdi-ctr**.

7.  On the **\<*your name\>\<date*\>hdi-ctr** blade, click the **apps** folder, and then click **Upload**.

8.  On the **Upload blob** blade, click in the **Files** box.

9.   In the **Choose File to Upload** dialog box, navigate to **E:\\Labfiles\\Lab07**, click **lab-7-etl.py**, and then click **Open**.

10. On the **Upload blob** blade, click **Advanced**.

11. In the **Block size** box, click **64 KB**, and then click **Upload**.

#### Task 3: Execute the application using spark-submit

1.  Click **Start**, type **cmd**, and then press Enter.

2.  At the command prompt, type **putty**, and then press Enter.

3.  In the **PuTTY Configuration** dialog box, in the **Host Name (or IP address)** box, type **\<*your name\>\<date*\>hdi-ssh.azurehdinsight.net**, and then click **Open**.

4.  In the **PuTTY Security Alert** dialog box, click **Yes**.

5.  In the PuTTY terminal window, at the **login as** prompt, type **sshadmin**, and then press Enter.

6.  At the **sshadmin@\<*your name\>\<date*\>hdi-ssh.azurehdinsight.net's password** prompt, type **Pa55w.rdPa55w.rd**, and then press Enter.

7.  At the PuTTY prompt, type the following code, and then press Enter:
    ````
    hdfs dfs -copyToLocal /apps/lab-7-etl.py ~/lab-7-etl.py
    ````

8.  At the PuTTY prompt, type the following code, and then press Enter to execute the script using **spark-submit**:
    ````
    spark-submit ~/lab-7-etl.py
    ````

9.  When the script has completed, review the results.

10. At the PuTTY prompt, type **exit**, and then press Enter to close your ssh session.

11. In Internet Explorer, on the **lab-etl** tab (where your Jupyter notebook is open), on the **File** menu, click **Close and Halt**.

12. Close all open applications.

#### Task 4: Review Spark history

1.  On the **\<*your name\>\<date*\>hdi** blade, in the **Cluster dashboards** section, click **Spark History server**.

2.  If a **Windows Security** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**.

3.  Your application is the row in the application list where **App Name** is **lab-7-etl**. You can determine the application run time from the value in the **Duration** column.

4.  Click the link in the **App ID** column that relates to your application.

5.  On the **Spark Jobs** page, note that the application contained only one job. You can determine the job run time from the value in the **Duration** column.

6.  Spend some time examining the details available for your job from the Spark History server. When you have finished, close the browser tab.

#### Task 5: Remove all Azure resources

1.  In the Microsoft Azure Portal, click **Resource groups**.

2.  On the **Resource groups** blade, right-click **Sparkrg**, and then click **Delete resource group**.

3.  On the **Are you sure you want to delete** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **Sparkrg**, and then click **Delete**.

4.  Wait for your resource group to be deleted, and then click **All resources**. Verify that the cluster, and the storage account that was created with your cluster, have all been removed.

**Results**: At the end of this exercise, you should have designed a Spark ETL process.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
