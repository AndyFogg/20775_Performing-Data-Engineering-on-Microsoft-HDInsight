# Module 9: Analyze Data with Hive and Phoenix

## Lab: Analyze data with Hive and Phoenix

### Scenario

Contoso is a global e-commerce giant that has a presence in 20 countries and operates from many office buildings. Contoso's finance department wants to reduce the amount of money it spends on energy consumption in the company's office buildings. The finance department has asked the data science team to look at the data generated by the sensors attached to their smart energy meters and find ways to minimize the expense. As one of the data scientists, you plan to use interactive query capabilities to analyze this large amount of data and respond to the business questions.

### Objectives

After completing this lab, you will know how to:

-   Provision and configure Interactive Hive Cluster.

-   Connect Excel to Interactive Hive Cluster using a Hive ODBC Driver.

-   Perform exploratory data analysis using Hive.

-   Perform interactive processing using Apache Phoenix.

### Lab Setup

-   **Estimated Time**: 120 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

## Exercise 1: Implement interactive queries for big data with Interactive Hive

### Scenario

In this exercise, you will perform interactive queries using Interactive Hive. You will use the following sensor data that is generated by the Contoso heating systems:

-   **HVAC.csv** - heating, ventilation and air-conditioning data generated by HVAC products across all Contoso buildings.

-   **building.csv** - information that relates to the Contoso buildings.

When you provision the Interactive Hive cluster, these datasets will be available on Hive Cluster. Using Azure Storage Explorer, you download these files to your workstation to gain understanding of the dataset, columns, and so on. They are normally situated below the HDFS locations, as follows:

-   **HVAC.csv** - /HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv.

-   **building.csv** - /HdiSamples/HdiSamples/SensorSampleData/building/building.csv.

The main tasks for this exercise are as follows:

1. Prepare the lab environment

2. Import HVAC data into Hive

3. Connect Excel to Interactive Hive Cluster


#### Task 1: Prepare the lab environment

1.  Ensure that the **MT17B-WS2016-NAT** and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.

2.  Connect to the **Azure Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

3.  Create a new HDInsight cluster with the following details:

    -   **Cluster name**: \<*your name*\>hivellap

    -   **Subscription**: your subscription

    -   **Cluster type**: Interactive Query

        -   **Operating system**: Linux

        -   **Version**: Interactive Query 2.1.0 (HDI 3.6)

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rd123

    -   **Secure Shell (SSH) username**: sshuser

    -   **Use same password as cluster login**: selected

    -   **Resource group (Create new)**: hivellaptrainrg

    -   **Location**: your location

    -   **Select a Storage account** (**Create new)**: hivellaptrainstg

    -   **Default container**: \<*your name*\>hivellap-ctr

    -   **Cluster size**:
    
        -   **Number of Worker nodes**: 2

        -   **Worker node size (View all)**: D13 v2 (2 nodes, 16 cores)

        -   **Head node size (View all)**: D13 (2 nodes, 16 cores)

        -   **Zookeeper node sizes**: default

4.  The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

5.  Connect to the above provisioned cluster using an SSH terminal such as PuTTY.

6.  Launch Beeline.

7.  Install the **Microsoft HiveODBC32** and **HiveODBC64** driver from **http://www.microsoft.com/en-us/download/details.aspx?id=40886**.

#### Task 2: Import HVAC data into Hive

-   Open **E:\\Labfiles\\Lab09\\Starter\\Exercise01\\upload_hvac_into_hive_table.hql** file, and review all the steps to be created and, when ready, write HiveQL in the Beeline shell:

    1.  Switch to the default schema for ingesting data, and drop the HVAC external table in the default schema, if it already exists.

    2.  Impose a schema on top of the HVAC data stored in the HDFS location using the CREATE external table **(hvacdata)** statement and import data. Verify the data load by counting the rows and previewing the rows.

#### Task 3: Connect Excel to Interactive Hive Cluster

1.  Create ODBC DSN for the Interactive Hive cluster that was set up in the previous task.

2.  Connect Excel to Interactive Hive Cluster and Pull Data.

3.  Compare the output Excel file with the Excel file available in the solution.

**Results**: After this exercise, you will have a good understanding of how to create an HBase cluster, connect to HBase Cluster via SSH, and use Phoenix to ingest data and perform interactive queries.

## Exercise 2: Perform exploratory data analysis by using Hive

### Scenario

In this exercise, you will perform interactive queries using Interactive Hive. This will include the following high level tasks:

-   Ingest raw data into Hive.

-   Create ORC tables.

-   Enrich data.

-   Analyze and visualize data.

You will use the following sensor data that is generated by the Contoso heating systems.

-   **HVAC.csv** - heating, ventilation and air-conditioning data that is generated by HVAC products across all Contoso buildings.

-   **buildings.csv** - information that relates to the Contoso buildings.

When you provision the Interactive Hive cluster, these datasets will be available on Hive Cluster. Using Azure Storage Explorer, you download these files to your workstation to gain understanding of the dataset, columns, and so on. They are normally situated below the HDFS locations, as follows:

-   **HVAC.csv** - /HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv.

-   **buildings.csv** - /HdiSamples/HdiSamples/SensorSampleData/building/buildings.csv.

The main tasks for this exercise are as follows:

1. Ingest raw data into Hive

2. Create ORC tables

3. Enrich data

4. Analyze and visualize data

#### Task 1: Ingest raw data into Hive

1.  Launch Zeppelin Notebook and create a new notebook with the name "**HVAC Analysis Report**". You will be using this notebook to perform data ingestion, ORC format table creation, refinement of tables and for analysis and visualization.

2.  Open the **upload_hvac_buildings_into_hive.hql** file at the following lab contents location:
    ````
    E:\Labfiles\Lab09\Starter\Exercise02
    ````

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook. When you're ready, start writing the following steps:

    1.  In **Step1,** you switch to the default schema to create tables and ingest.

    2.  In **Step2**, drop the **hvacdata** table, if it already exists in the default schema.

    3.  In **Step3**, impose a structure on top of the **HVAC.csv** file using the **CREATE EXTERNAL TABLE** statement.

    4.  In **Step4**, drop the buildings table if it already exists in the **default** schema.

    5.  In **Step5**, impose a structure on top of the **buildings.csv** file using the **CREATE EXTERNAL TABLE** statement.

#### Task 2: Create ORC tables

-   Open the **create_orc_formatted_tables.hql** file at the following lab contents location:
    ````
    E:\Labfiles\Lab09\Starter\Exercise02
    ````

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook. When you're ready, start writing the following steps:

    1.  In **Step1,** you switch to the default schema to create tables and ingest.

    2.  In **Step2**, drop the **hvacdataorc** table, if it already exists in the default schema.

    3.  In **Step3**, create an internal table named **hvacdataorc** using the **CREATE TABLE** statement.

    4.  In **Step4**, populate the **hvacdataorc table** from the **hvacdata** using the **INSERT OVERWRITE TABLE** statement.

    5.  In **Step5**, drop the **buildingsorc** table if it already exists in the **default** schema.

    6.  In **Step6**, create an internal table named **buildingsorc** using the **CREATE TABLE** statement.

    7.  In **Step7**, populate the **buildingsorc table** from **building** using the **INSERT OVERWRITE TABLE** statement.

#### Task 3: Enrich data

-   Open the **refine_hvac_buildings.hql** file at the following lab contents location:
    ````
    E:\Labfiles\Lab09\Starter\Exercise02
    ````

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook. When you're ready, start writing the following steps:

    1.  In **Step1**, switch to the default schema to create tables and drop the **hvac_temperatures** table, if it already exists.

    2.  In **Step2**, you create the **hvac_temperatures** table by retaining all the columns from the **hvacdataorc** table and adding three new columns (Temp_Variance, tempboundary and worsetemp) as per the following business rules:

        -   **Temp_Variance**: difference between the TargetTemp column and the ActualTemp

        -   **tempboundary**: if the difference between the TargetTemp column and the ActualTemp is:

            -   greater than 4 then set the value to 'HOT'

            -   less than -4 then set the value to 'COLD'

            -   Otherwise Normal

        -   **worsetemp**: if the difference between the TargetTemp column and the ActualTemp is:

            -   greater than 4 then set the value to '1'

            -   less than -4 then set the value to '1'

            -   Otherwise '0'

    3.  In **Step3**, drop the **hvac_buildings** table if it already exists in the **default** schema and create a **hvac_buildings table** by joining the *buildings** table with the **hvac_temperatures table** created in **Step2**.

#### Task 4: Analyze and visualize data

-   Open the **exploratory_data_analysis_with_hive.hql** file at the following lab contents location:
    ````
    E:\Labfiles\Lab09\Starter\Exercise02
    ````

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook. When you're ready, start writing the following steps:

    1.  In **Step1,** you perform basic data profiling on the **hvac_buildings** table, and then write queries to:

        -   Count the number of rows

        -   Preview few rows

        -   Fetch the number of countries using the HVAC products

        -   Count the number of countries using HVAC products

    2.  In **Step2**, write a query to determine the number of buildings and the number of HVAC products in each country. Use the following columns (country, BuildingId and HVACProduct) to write the query.

    3.  In **Step3**, write a query to determine the temperature boundaries for various countries over a period (time series type view). Use the following columns (date, country and tempboundary) to write the query.

    4.  In **Step4**, write a query to determine the worse temperatures for various countries over a period (time series type view). Use the following columns (date, country and worsetemp) to write the query.

    5.  In **Step5**, write a query to identify the HVAC products that need to be upgraded. Use the following columns (HVACProduct, worsetemp) to write the query.

**Results**: You have now carried out interactive queries using Interactive Hive and compared your achieved results with the expected outcomes.

## Exercise 3: Perform interactive processing by using Apache Phoenix

### Scenario

In this exercise, you will perform interactive queries using Phoenix. This activity includes the following high level tasks:

-   Prepare the lab environment.

-   Ingest data into Phoenix.

-   Perform interactive queries.

You will use the following sensor data that is generated by the Contoso heating systems:

-   **HVAC.csv** - heating, ventilation and air-conditioning data generated by HVAC products across all Contoso buildings.

-   **building.csv** - information that relates to the Contoso buildings.

When you provision the HBase cluster, these datasets will be available on HBase Cluster. Using Azure Storage Explorer, you download these files to your workstation to gain understanding of the dataset, columns, and so on. They are normally situated below the HDFS locations, as follows:

-   **HVAC.csv** - /HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv.

-   **building.csv** - /HdiSamples/HdiSamples/SensorSampleData/building/building.csv.

The main tasks for this exercise are as follows:

1. Prepare the lab environment

2. Ingest data into Phoenix

3. Perform interactive queries

#### Task 1: Prepare the lab environment

1.  Connect to the **Azure Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

2.  Create a new HDInsight cluster with the following details:

    -   **Cluster name**: \<*your name*\>hbasetrain

    -   **Subscription**: your subscription

    -   **Cluster type**: HBase

        -   **Operating system**: Linux

        -   **Version**: HBase 1.1.2 (HDI 3.6)

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rd123

    -   **Secure Shell (SSH) username**: sshuser

    -   **Use same password as cluster login**: selected

    -   **Resource group (Create new)**: hbasetrainrg

    -   **Location**: your location

    -   **Select a Storage account (Create new)**: hbasetrainstg

    -   **Default container**: \<*your name*\>hbasetrain-ctr

    -   **Cluster size**:
    
        -   **Number of Region nodes**: 2

        -   **Region node size (View all)**: A3 (2 nodes, 8 cores)

        -   **Head node size (View all)**: A3 (2 nodes, 8 cores)

        -   **Zookeeper node sizes**: default

3.  The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

4.  Connect to the provisioned cluster using an SSH terminal such as PuTTY.

5.  Log in to Ambari and find the zookeeper host name.

6.  Upload the **Lab09** folder from **E:\\Labfiles** on your workstation into the Blob storage container attached to the HBase Cluster provisioned in the previous task.

7.  Copy the Lab09 folder contents from HDFS to the local file system on the head node of the cluster.

8.  Copy **HVAC.csv and building.csv files** from the HDFS location to the local filesystem at the following locations:
    ````
    $HOME/Lab09/Starter/Exercise03
    $HOME/Lab09/Solution/Exercise03
    ````
9.  Convert the files into unix format.

#### Task 2: Ingest data into Phoenix

In this tab task, you ingest **HVAC.csv** and **buildings.csv** files that are available on the cluster into Phoenix using CSV bulk loader utility. You will be using a **psql.py** script to bulk ingest in single-thread mode and a **SQLLine** console for running Phoenix queries.

1.  Open the SSH terminal and go to the lab contents folder. Create HVAC and buildings tables. Review the steps you need to write for **create_hvac_buildings_tables.sql** at **$HOME/Lab09/Starter/Exercise03**.

    -   Drop the HVAC table, if it already exists

    -   Create a HVAC table

    -   Drop the HVAC table, if it already exists

    -   Create a HVAC table

2.  Bulk ingest HVAC and buildings tables.

3.  Verify bulk ingests of HVAC and building tables. Review the steps you need to write for **verify_bulk_ingests.sql** at **$HOME/Lab09/Starter/Exercise03/**.

4.  Verify the results.

#### Task 3: Perform interactive queries

-   Following the previous lab task, review the steps you need to write for the **query_hvac_buildings.hql** file at **$HOME/Lab09/Starter/Exercise03.** Review all the steps to be created before writing the Phoenix queries in the SQLLine console. When you're ready, start writing the following steps:

    1.  In **Step1,** you perform basic data profiling on the table, and write queries to:

        1.  Fetch the number of countries using the HVAC products

        2. Count the number of countries using HVAC products

    2.  In **Step2**, write a query to determine temperatures and other columns for building number 17 on '06/09/13'.

    3.  In **Step3**, write a query to determine the maximum and minimum target, and the Actual temperatures.

    4.  In **Step4**, write a query to identify the countries where the Actual temperature reached a maximum of 80 degrees on 6/11/13. Use HVAC and buildings tables to write this query.

    5.  In **Step5**, write a query to identify the countries where Actual temperature reached a minimum of 55 degrees on 6/11/13. Use HVAC and buildings tables to write this query.

    6.  Verify the results.

#### Task 4: Remove all Azure resources

1.  Delete both HDInsight clusters and storage accounts, by deleting the associated resource groups.

2.  Verify that all Azure resources have been removed.
   
**Results**: After this exercise, you will have a good understanding of how to create an HBase cluster, connect to HBase Cluster via SSH, and use Phoenix to ingest data and perform interactive queries.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
