# Module 6: Implementing Batch Solutions

## Lab: Implement batch solutions

### Scenario

The Contoso IT department has a large on-premises IT infrastructure, and are planning to move much of this infrastructure into the cloud. The Contoso IT department is keen to understand the big data management capabilities in Azure. Contoso have approached your consulting organization to demonstrate, through a Proof of Concept (PoC), how to implement batch solutions using Azure, Apache, Hive, Pig, and SQL Database.

### Objectives

After completing this lab, you will be able to:

-   Deploy an HDInsight cluster on Linux, and configure data storage for the cluster.

-   Use data transfers with an HDInsight cluster, and deploy a cluster from a template.

-   Query HDInsight data using Pig and Hive.

### Lab Setup

-   **Estimated Time**: 90 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

## Exercise 1: Deploy HDInsight cluster and data storage

### Scenario

In the first part of your PoC for the Contoso IT department, you will create an HDInsight cluster on Linux, using an Azure SQL Database for Hive and Oozie.

The main tasks for this exercise are as follows:

1. Create an Azure SQL Database

2. Create an HDInsight cluster

#### Task 1: Create an Azure SQL Database

1.  Connect to the Azure Portal, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

2.  Create a new SQL Database with the following details:

    -   **Database name**: sqldb1

    -   **Subscription**: your subscription

    -   **Resource group (Create new)**: mod06rg

    -   **Select source**: Blank database

    -   **Server name**: \<*your* *name*\>\<*date*\>mod06sqlsvr1

    -   **Server admin login**: dbadmin

    -   **Password**: Pa55w.rd123

    -   **Confirm password**: Pa55w.rd123

    -   **Location**: your region

    -   **Pricing tier**: Basic

3.  Wait for the database to be created before proceeding.

#### Task 2: Create an HDInsight cluster

1.  Create a new HDInsight cluster with the following details:

    -   **Cluster name**: \<*your name*\>\<*date*\>mod06hive

    -   **Cluster type**: Hadoop

    -   **Operating system**: Linux

    -   **Version**: Hadoop 2.7.3 (HDI 3.6)

    -   **Cluster tier**: STANDARD

    -   **Cluster login username**: hadmin

    -   **Cluster login password**: Pa55w.rd123

    -   **Secure Shell (SSH) username**: sadmin

    -   **Resource group (Create new)**: mod06rg

    -   **Location**: Select your region

    -   **Storage account (Create new)**: \<*your name*\>\<*date*\>mod06sa

    -   **Default container**: replace the suggested name with \<*the name of your cluster*\>-ctr (for example, \<*your name*\>\<*date*\>mod06hive-ctr).

    -   **Select a SQL database for Hive**: select your SQL database

    -   **Select a SQL database for Oozie**: select your SQL database

    -   **SQL Database username**: dbadmin

    -   **SQL Database password**: Pa55w.rd123

    -   **Cluster size**: Number of Worker nodes: 1

    -   **Worker node size**: A3 General Purpose

    -   **Head node size**: A3 General Purpose

2.  Before creating the cluster, save the settings as a template named **my_hive_deployment**, with the description **Module 6 hive deployment**, in the **E:\\Labfiles\\Lab06** folder.

3.  The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

**Results**: At the end of this exercise, you will have created an Azure SQL Database, and created an HDInsight, Hadoop cluster that uses this data storage.

## Exercise 2: Use data transfers with HDInsight clusters

### Scenario

In the second part of your PoC for the Contoso IT department, you will download data from the cluster using the Azure Portal, and make use of the Cloud Shell (and associated storage) for use as an "offline" storage location that can be used to hold data separately from a cluster. You will demonstrate the potential value of this storage by storing some data, deprovisioning and then reprovisioning an HDInsight cluster, and then uploading the data from the Cloud Shell to the new HDInsight cluster. You will also demonstrate the value of templates, by using a JSON template during the reprovisioning of the HDInsight cluster.

The main tasks for this exercise are as follows:

1. Download data from Blob storage using the Azure Portal

2. Download data using the Cloud Shell

3. Deprovision an HDInsight cluster

4. Reprovision an HDInsight cluster using a template

5. Upload data from the Cloud Shell to HDInsight cluster

#### Task 1: Download data from Blob storage using the Azure Portal

-  In the Azure Portal, download **yellowthings.txt**, from **example/data/** in your container, and save it as **E:\\Labfiles\\Lab06\\yellowthings.txt**.

#### Task 2: Download data using the Cloud Shell

1.  Use **Microsoft Edge**, and from the Azure Portal, copy the **key1 Key** for your storage account to Notepad.

2.  Click **\>** to open a Bash session, and when prompted, **Create storage**.

3.  Wait for the Cloud Shell initialization to complete.

> **Note**: All of the following commands can be copied from **E:\\Labfiles\\Lab06\\BashCmds.txt**.

4.  At the Bash prompt, type the following command, and press Enter:
    ````
    azure storage account list
    ````

    A list of all storage accounts is now shown; note, that you now have an additional cloud shell account.

5.  At the Bash prompt, type the following command, and press Enter (replacing **\<storage account\>** with the storage account you created earlier):
    ````
    export AZURE_STORAGE_ACCOUNT=<storage account>
    ````

    This command saves your storage account in a variable, so that you do not have to enter it every time; note that in Edge, you can right-click, and click **Paste**, to copy the text strings you have saved in Notepad.

6.  At the Bash prompt, type the following command, and press Enter (replacing **\<primary key\>** with the details you saved to Notepad):
    ````
    export AZURE_STORAGE_ACCESS_KEY=<primary key>
    ````

    This command saves your primary key in a variable, so that you do not have to enter it every time.

7.  To list all the blobs in your Azure storage container, type the following command, and press Enter (replacing **\<container\>** with the storage container you created earlier):
    ````
    azure storage blob list <container>
    ````

    This command lists all the blobs in the default container.

8.  At the Bash prompt, type the following command, and press Enter:
    ````
    ls
    ````

    The file list should be empty, except for a mount point for the Cloud Shell's persistent file share (clouddrive).

9.  At the Bash prompt, type the following command, and press Enter:
    ````
    cd clouddrive
    ````

    The file list should also be empty.

10. At the Bash prompt, type the following command, and press Enter (replacing **\<container\>** with the storage container you created earlier):
    ````
    azure storage blob download <container> example/data/fruits.txt
    fruits.txt
    ````

11. At the Bash prompt, type the following command, and press Enter:
    ````
    ls
    ````

    The file list should now show that fruits.txt has been downloaded to your bash user's cloud shell folder.

12. At the Bash prompt, type the following command, and press Enter:
    ````
    nano
    ````

13. In the nano editor, type some text, and then press CTRL + X.

14. Press Y.

15. At the **File Name to Write** prompt, type worknotes.txt, and press Enter.

16. At the Bash prompt, type the following command, and press Enter:
    ````
    ls
    ````

    The file list should show that worknotes.txt has been created in your bash user's cloud shell folder.

17. Close the bash shell.

#### Task 3: Deprovision an HDInsight cluster

-   Delete your cluster, but not its storage or resource group.

#### Task 4: Reprovision an HDInsight cluster using a template

1.  Add the **my_hive_deployment** template to Azure templates, and edit as follows.

> **Note**: All of the following template code can be copied from **E:\\Labfiles\\Lab06\\templateCode.txt**.

2.  At the end of line 6, press Enter, and then type the following code:
    ````
    "defaultValue": "<your name><date>mod06hiveX",
    ````

3.  On line 14, replace **admin** with **hadmin**.

4.  On line 55, replace **sshuser** with **sadmin**.

5.  At the end of line 67, press Enter, and then type the following code:
    ````
    "defaultValue": "dbadmin",
    ````

6.  At the end of line 77, press Enter, and then type the following code:
    ````
    "defaultValue": "dbadmin",
    ````

7.  Deploy the template, using the following custom deployment information:

    -   **Resource group (Use existing):** mod06rg

    -   **Cluster Name**: replace "X" with 2, e.g. \<*your name\>\<date*\>mod06hive2

    -   **Cluster Login Password**: Pa55w.rd123

    -   **Ssh Password**: Pa55w.rd123

    -   **Hive Password**: Pa55w.rd123

    -   **Oozie Password**: Pa55w.rd123

8.  Wait until the cluster is created before continuing to the next task; cluster creation will take between 10 and 15 minutes.

#### Task 5: Upload data from the Cloud Shell to HDInsight cluster

> **Note**: All of the following commands can be copied from **E:\\Labfiles\\Lab06\\BashCmds.txt**.

1.  In Microsoft Edge, in the Bash console, at the Bash prompt, type the following command, and press Enter:
    ````
    azure storage account list
    ````

2.  Note, that you still have the same storage accounts as before.

3.  At the Bash prompt, type the following command, and press Enter (replacing **\<storage account\>** with the details you saved to Notepad):
    ````
    export AZURE_STORAGE_ACCOUNT=<storage account>
    ````

4.  At the Bash prompt, type the following command, and press Enter (replacing **\<primary key\>** with the details you saved to Notepad):
    ````
    export AZURE_STORAGE_ACCESS_KEY=<primary key>
    ````

5.  At the Bash prompt, type the following command, and press Enter:
    ````
    cd clouddrive
    ````

6.  At the Bash prompt, type the following command, and press Enter (replacing **\<container\>** with the details you saved to Notepad):
    ````
    azure storage blob upload worknotes.txt <container>
    example/data/worknotes.txt
    ````

7.  In the Azure Portal, browse to your container, and the **example/data/** folder, and verify that **worknotes.txt** has been uploaded.

**Results**: At the end of this exercise, you will have downloaded data from Blob storage using the Azure Portal, and by using the Cloud Shell; you will also have deprovisioned an HDInsight cluster, reprovisioned the HDInsight cluster using a template, and then uploaded data from the Cloud Shell to the HDInsight cluster.

## Exercise 3: Query HDInsight cluster data

### Scenario

In the third part of your PoC for the Contoso IT department, you will query HDInsight data using different tools, including Pig, and HiveQL. Using Pig, you will use two methods to query the same sample log data, and determine the total number of different severity levels reported in status messages in the sample log. Using HiveQL, you will use a slightly different query against this data, to determine the number of [ERROR] level messages in one column of the sample data.

The main tasks for this exercise are as follows:

1. Query data using a Pig job in PowerShell

2. Query data using an interactive Pig job

3. Query data using HiveQL from the Ambari dashboard

4. Remove all Azure resources

#### Task 1: Query data using a Pig job in PowerShell

1.  Rename **E:\\Labfiles\\Lab06\\hivepig.ps1.txt** to **hivepig.ps1**.

2.  Open **hivepig.ps1** file in the PowerShell ISE, and then run the script.

3.  When prompted for Azure credentials, use the Microsoft account and password that is associated with your Azure Learning Pass subscription.

4.  When prompted for a cluster name, use the name of the cluster you created in Exercise 2.

5.  When prompted for cluster credentials, use the following details:

    -   **User name**: hadmin

    -   **Password**: Pa55w.rd123

6.  If you get errors, run the Pig script again.

7.  When the job has completed, it will display the following results:
    ````
    (TRACE,816)
    (DEBUG,434)
    (INFO,96) 
    (WARN,11) 
    (ERROR,6) 
    (FATAL,2)
    ````
    These results show the total number of different severity level messages reported in in the sample log.

#### Task 2: Query data using an interactive Pig job

1.  Use PuTTY to open an ssh session to your cluster.

2.  When prompted, enter **sadmin** and **Pa55w.rd123** as the credentials.

3.  At the ssh console prompt, type the following command, and press Enter:
    ````
    pig
    ````

4.  At the ssh console prompt, type the following commands, and press Enter; this code can be copied from **E:\\Labfiles\\Lab06\\pigJob.txt**):
    ````
    LOGS = LOAD 'wasbs:///example/data/sample.log';
    LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1) as LOGLEVEL;
    FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;
    GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;
    FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;
    RESULT = order FREQUENCIES by COUNT desc;
    DUMP RESULT;
    ````

5.  The job will take several minutes to complete; the results should be the same as the previous task:
    ````
    (TRACE,816)
    (DEBUG,434)
    (INFO,96) 
    (WARN,11) 
    (ERROR,6) 
    (FATAL,2)
    ````
    As before, these results show the total number of different severity level messages reported in in the sample log.

#### Task 3: Query data using HiveQL from the Ambari dashboard
1.  Use Microsoft Edge to open the Ambari dashboard for your cluster.

2.  If prompted to enter credentials to log into Ambari, use the following:

    -   **User name**: hadmin

    -   **Password**: Pa55w.rd123

3.  In the Ambari dashboard, open the **Hive View**.

4.  In the **Query Editor**, execute the following code; this code can be copied from **E:\\Labfiles\\Lab06\\hiveHql.txt**).
    ````
    set hive.execution.engine=tez;
    DROP TABLE log4jLogs;
    CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
    STORED AS TEXTFILE LOCATION 'wasbs:///example/data/';
    SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;
    ````

5.  The job will take several minutes to complete; there are only 3 rows in the logs where the column t4 contains the value **[ERROR]**, so the result should be:
    ````
    [ERROR] 3
    ````

#### Task 4: Remove all Azure resources

1.  Delete the HDInsight cluster and storage account, by deleting the associated resource group.

2.  Delete the cloud shell storage account, by deleting the associated resource group.

3.  Verify that all Azure resources have been removed.

**Results**: At the end of this exercise, you will have queried HDInsight data using a Pig job in PowerShell, and by using Pig in an interactive session. You will also have queried HDInsight data using HiveQL from the Ambari dashboard.

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
