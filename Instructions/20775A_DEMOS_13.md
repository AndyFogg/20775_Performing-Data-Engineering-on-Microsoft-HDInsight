# Module 13: Create Spark Streaming applications

## Demo 1: Spark Streaming in action 

### Scenario

In this demonstration, you will see an example of a simple Spark Streaming application.
The application connects to a TCP port and reads text from it using the **socketTextStream** data source. The data consists of pairs of stock market tickers and prices. The application displays the data as it is received.

### Preparation

This demo uses a local Spark cluster, but requires that you install the Scala 2.11.8 runtime (required by the DStream libraries), and configure the local Hadoop environment, as follows:
1.	Ensure that the **MT17B-WS2016-NAT** and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	On the **20775A-LON-DEV** virtual machine, create a folder named **E:\Demofiles\Hadoop\bin**.
3.	Create another folder named **E:\Demofiles\Checkpoints**.
4.	Using Internet Explorer, download **winutils.exe** file from **https://github.com/steveloughran/winutils/blob/master/hadoop-2.6.0/bin/winutils.exe**. Save the file in the **E:\Demofiles\Hadoop\bin** folder.
5.	Using IntelliJ IDEA, open the **StockMarketStreamProcessor** project in the **E:\Demofiles\Mod13** folder.
6.	On the **File** menu, click **Project Structure**.
7.	In the **Project Structure** dialog box, under **Platform Settings**, click **Global Libraries**.
8.	Click **scala-sdk.2.12.2**, and then in the toolbar, click the **Delete** button (**–**).
9.	If the **Remove Library** message box appears, click **OK**.
10.	In the toolbar, click the **New Global Library** button (**+**), and then click **Scala SDK**.
11.	In the **Select JAR’s for the new Scala SDK** dialog box, click **Download**.
12.	In the **Download** dialog box, in the **Version** list, select **Scala 2.11.8**, and then click **OK**.
13.	When the download is complete, in the **Select JAR’s for the new Scala SDK** dialog box, click the row for **Ivy Scala 2.11.8**, and then click **OK**.
14.	In the **Project Structure** dialog box, click **OK**.
15.	On the **File** menu, click **Save All**.

### Examine the code for the StockMarketStreamProcessor application
1.	In IntelliJ IDEA, if the Project pane is not visible, on the View menu, point to Tool Windows, and then click Project.
2.	In the Project pane, move to the StockMarketStreamProcessor\src\StockMarketStreamProcessor folder, and then double-click Streamprocessor.
3.	In the editor window, highlight the following code in the main function:
    ```
    // Create a streaming context using the createStreamingContext function
    val streamingContext: StreamingContext = StreamingContext.getOrCreate(checkpointDirectory,
        () => createStreamingContext(inputOptions))

    // Start streaming
    streamingContext.start()

    // Stream until terminated or a few minutes have passed
    streamingContext.awaitTermination(120000)
    ```
    This code constitutes the typical life cycle of a Spark Streaming application; create a streaming context, start streaming, and then wait for streaming to finish.
4.	Highlight the following code in the **createStreamingContext** function:
    ```
    // Create a Spark streaming context
    val config: SparkConf = new SparkConf
    config.setAppName("StockMarket")
    config.set("spark.streaming.driver.writeAheadLog.allowBatching", "true")
    config.set("spark.streaming.driver.writeAheadLog.batchingTimeout", "60000")
    config.set("spark.streaming.receiver.writeAheadLog.enable", "true")
    config.set("spark.streaming.receiver.writeAheadLog.closeFileAfterWrite", "true")
    config.set("spark.streaming.stopGracefullyOnShutdown", "true")
    config.setMaster("local[*]") // change to "spark://host:port to run in a Spark cluster
    val sparkContext: SparkContext = new SparkContext(config)
    ```
    This code configures the Spark environment and indicates on which Spark cluster to run the application (local, in this case).
5.	Highlight the following code:
    ```
    // Configure the Spark streaming context
    val checkpointDir: String = options(Symbol(ClientArgumentKeys.CheckpointDirectory)).asInstanceOf[String]
    val batchDuration = 5 // 5 second batches
    val streamingContext: StreamingContext = new StreamingContext(sparkContext, Seconds(batchDuration))
    streamingContext.checkpoint(checkpointDir)
    ```
    This code creates the Spark streaming context, specifying a batch interval of five seconds, and forcing a checkpoint.
6.	Highlight the following code:
    ```
    // Create a DStream that connects to the stock market sender socket using the basic socketTextStream source
    val sender: String = options(Symbol(ClientArgumentKeys.Sender)).asInstanceOf[String]
    val port: Int = options(Symbol(ClientArgumentKeys.Port)).asInstanceOf[String].toInt
    val stream = streamingContext.socketTextStream(sender, port)
    ```
    This code connects the Spark streaming context to a TCP/IP socket. Another application will post a stream of stock market tickers and prices to this socket.
7.	Highlight the following code:
    ```
    // Define the processing for the stream
    doProcessing(streamingContext, stream)

    // Return the streaming context
    return(streamingContext)
    ```
    This code calls another function named _doProcessing_ to define the Spark streaming operations to perform, before returning the _StreamingContext_ object to the caller.
8.	Highlight the following code in the **doProcessing** function:
    ```
    def doProcessing(streamingContext: StreamingContext, stream: DStream[String]): Unit = {

    // Perform a foreachRDD transformation to process the data in the stream
    // Iterate through each item in each partition in each RDD in the stream
    val parsedDdata = stream.foreachRDD {rdd =>
        try
        {
        rdd foreachPartition { part =>
            part foreach {stockMarketEvent =>

            // Split the input data into the ticker and price
            val tickerInfo = stockMarketEvent.split(",")

            // Display the parsed data
            println(s"Ticker: ${tickerInfo(0)}, Price: ${tickerInfo(1)}")
            }
        }
        } catch {
        case ex: Exception => val message: String = ex.getMessage
            println(s"General error processing data: $message\nDiscarding data")
        }
    }
    }
    ```
    This code uses the foreachRDD output function of the stream to step through each RDD in the stream. The code iterates through each partition, and each item in each partition. The data in each item is a (ticker, price) pair received as a string. The code splits the pair into its individual elements and displays them.

### Run the StockMarketStreamProcessor application
1.	On the **File** menu, click **Open**. 
2.	Move to the **E:\Demofiles\Mod13\StockMarketSocketSender** folder, and then click **OK**.
3.	In the **Open Project** dialog box, click **New Window**.
4.	Switch to the **StockMarketSocketSender** project.
5.	On the **Run** menu, click **Run 'StockMarketSocketSender'**.
This application generates stock market prices and sends them to a socket. It will not start to generate prices until the streaming client application runs.
6.	If the **Windows Security Alert** dialog box appears, click **Allow access**.
7.	Switch to the **StockMarketStreamProcessor** project.
8.	On the **Run** menu, click **Run 'StockMarketStreamingProcessor'**.
After a few seconds, the console window for the **StockMarketStreamProcessor** application should display a couple of informational messages indicating that it is connecting to the socket. At this point, the **StockMarketSender** application should start generating data. The tickers and prices will be received and displayed by the StockMarketStreamProcessor application. 
9.	Allow both applications to run for a couple of minutes, and then stop them both.
10.	Close both IntelliJ IDEA windows.

## Demo 2: Working with Structured Streaming

### Scenario

In this demonstration, you will see an example of a Structured Streaming application. This application performs a streaming analysis of stock market data, generating a running count of the number of times each stock market item has a price change.

### Preparation

The instructions in this demonstration assume that you have configured IntelliJ IDEA with the Scala SDK version 2.11.8, and that you have installed the local Hadoop runtime on your virtual machine. If you have not performed these tasks, follow the Preparation instructions listed at the start of the previous demonstration.

### Examine the code for the StructuredStreamReceiver application
1.	In IntelliJ IDEA, open the **StructuredStreamReceiver** project in the **E:\Demofiles\Mod13** folder.
2.	If the **Project** pane is not visible, on the **View** menu, point to **Tool Windows**, and then click **Project**.
3.	In the **Project** pane, move to the **StructuredStreamReceiver\src\StructuredStreamReceiver** folder, and then double-click **Receiver**.
4.	In the editor window, highlight the following code in the **main** function:
    ```
    // Create a Spark session
    val config: SparkConf = new SparkConf
    config.setAppName("StockMarket")
    config.set("spark.streaming.driver.writeAheadLog.allowBatching", "true")
    config.set("spark.streaming.driver.writeAheadLog.batchingTimeout", "60000")
    config.set("spark.streaming.receiver.writeAheadLog.enable", "true")
    config.set("spark.streaming.receiver.writeAheadLog.closeFileAfterWrite", "true")
    config.set("spark.streaming.stopGracefullyOnShutdown", "true")
    config.setMaster("local[*]") // change to "spark://host:port to run in a Spark cluster

    val spark: SparkSession = SparkSession.builder.config(config).getOrCreate()
    ```
    This code creates and configures a _SparkSession_. It is very similar to the code used by a _DStreams_ application.
5.	Highlight the following code:
    ```
    // Create a DataFrame for the input stream containing the stock market events
    val stockMarketEvent = spark.readStream.format("socket").option("host", sender).option("port", port).load()
    ```
    This code connects to the socket specified by the sender and host parameters, and then reads each batch of data as it arrives into a _DataFrame_.
6.	Highlight the following code:
    ```
    // Extract the ticker from each row in the DataFrame
    import spark.implicits._
    val tickers = stockMarketEvent.as[String].map(_.split(",")(0)).toDF("Ticker")
    ```
    This code parses the data in each row in the _DataFrame_ (each row consists of a string in the format "\<ticker\>, \<price\>"). The ticker values are extracted and written to a new _DataFrame_ containing a single column named "Ticker".
7.	Highlight the following code:
    ```
    // Count how many times each ticker occurs in the DataFrame
    val tickerCounts = tickers.groupBy("Ticker").count
    ```
    This code groups the data in the _DataFrame_ by Ticker, and then counts the number of times each distinct Ticker value occurs. The result is a new _DataFrame_ containing Ticker/Count pairs.
8.	Highlight the following code:
    ```
    // Display the tickers and counts
    val output = tickerCounts.writeStream.outputMode("complete").format("console").start()
    ```
    This code uses the _writeStream_ output function to display the results. The start function causes stream processing to begin.
9.	Highlight the following code:
    ```
    output.awaitTermination()
    ```
    This statement blocks the application from finishing until the output stream is terminated. Stream processing will continue all the time the output stream is open.

### Run the StructuredStreamReceiver application
1.	On the **File** menu, click **Open**. 
2.	Move to the **E:\Demofiles\Mod13\StockMarketSocketSender** folder, and then click **OK**.
3.	In the **Open Project** dialog box, click **New Window**.
4.	Switch to the **StockMarketSocketSender** project.
5.	 On the **Run** menu, click **Run 'StockMarketSocketSender'**. This is the same application used by the previous demonstration; it generates stock market prices and sends them to a socket.
6.	Switch to the **StructuredStreamReceiver** project.
7.	On the **Run** menu, click **Run 'StructuredStreamReceiver'**. After a few seconds, the console window for the StructuredStreamReceiver application should display a couple of informational messages indicating that it is connecting to the socket. At this point, the StockMarketSender application should start generating data. The data will be received, and the counts of the number of times each ticker occurs will displayed by the StructuredStreamReceiver application. Note that the counts are cumulative; this is because a DataFrame is essentially an ever-updating table that aggregates and stores data as it is processed.
8.	Allow both applications to run for a couple of minutes and then stop them both.
9.	Close both IntelliJ IDEA windows.


---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.