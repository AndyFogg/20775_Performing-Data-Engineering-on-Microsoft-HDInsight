# Module 13: Create Spark Streaming applications

## Lab: Building a Spark Streaming application

### Scenario

You have access to a stream of data generated by the meters in a fleet of taxis in New York. The data includes information about each trip, including the time and location of each pickup, the time and location of each drop-off, and the price, tax, and tips paid. You want to create a streaming application in HDInsight that receives events from this data stream, calculates summary statistics, and displays information in Power BI.

You want to calculate the following summary statistics, based on the live data stream:

-   The number of trips currently running.

-   The average number of passengers.

-   The average distance for a trip.

-   The average fare for a trip.

You have been asked to build this application as a Spark Structured Streaming application. You will use an event hub as a source of taxi event data. The event hub sends the taxi event data as a JSON string. The document **data_dictionary_trip_records_yellow.pdf** in the **E:\\Labfiles\\Lab13** folder describes the fields in the data.

### Objectives

At the end of this lab, you will be able to:

-   Install and configure all the prerequisite software necessary to create Spark Streaming applications that run on HDInsight clusters.

-   Build and configure Azure components that a Spark Streaming application can use as data sources and destinations.

-   Use IntelliJ IDEA to create, deploy, and test a Spark Streaming application.

### Lab Setup

-   **Estimated Time**: 30 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

### Prerequisites:

-   An Azure subscription---to get an Azure free trial, see: https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight

## Exercise 1: Installing required software

### Scenario

Before you can build a Spark Structured Streaming application that analyses a taxi data stream, you must install and configure the required software on your development machine.

The main tasks for this exercise are as follows:

1. Install IntelliJ IDEA community edition

2. Install the Java SE Development Kit

3. Install the Scala SDK and configure IntelliJ IDEA

4. Install PuTTY

#### Task 1: Install IntelliJ IDEA community edition

-   If IntelliJ IDEA community edition is not installed on your computer, download and install it. Use the following information:

    -   Download Source: **https://www.jetbrains.com/idea/download/index.html#section=windows**

    -   Installation options: 64-bit launcher

    -   Do not import settings

    -   Download and install Scala

    -   For all other choices, use the default values

#### Task 2: Install the Java SE Development Kit

-   In IntelliJ, create a new Scala IDEA project. Use the new project wizard to download and install the Java SE Development Kit.

#### Task 3: Install the Scala SDK and configure IntelliJ IDEA

1.  In IntelliJ, use the new project wizard to download and install the Scala SDK version 2.12.2.

2.  When prompted, allow access to the OpenJDK Platform binary and IntelliJ IDEA.

3.  In the **Settings** dialog box, in the **Plugins** pane, clear **Android Support** and install the Azure Toolkit for IntelliJ.

#### Task 4: Install PuTTY

-   If the PuTTY tool is not installed, download and install it from the following location: **http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html**.

**Results**: At the end of this exercise, you will have a Windows client with IntelliJ IDEA Community Edition, the Java SE Development Kit, the Scala SDK, and the PuTTY tool installed for use later in the lab.

> **Note**: If you already have all of these components installed in the 20775A-LON-DEV virtual machine, you can skip this exercise.

## Exercise 2: Creating the data sources and destinations for the Spark application

### Scenario

You have decided to build the taxi event data analysis application as a Spark Structured Streaming application. You will use an event hub as a source of data, and display summary data generated by the streaming application using Power BI. In this exercise, you will create and configure the event hub and a Power BI dashboard.

The main tasks for this exercise are as follows:

1. Create an Event Hubs namespace

2. Create an event hub

3. Record the Shared Access primary key

4. Configure a Shared Access policy for listeners

5. Create a Power BI streaming dataset

6. Create a Power BI dashboard

#### Task 1: Create an Event Hubs namespace

-   In the Azure Portal, create a new Event Hubs namespace. Use the following information:

    -   **Name**: \<*your name\>\<date*\>ehn (must be unique)

    -   **Pricing tier**: Basic

    -   **Subscription**: your subscription

    -   **Resource group (Create new)**: StormRG

    -   **Location**: Choose your region

    -   **Throughput Units**: 1

#### Task 2: Create an event hub

-   In the Azure Portal, create an event hub within the Event Hubs namespace you created in the previous task. Use the following information:

    -   **Name**: TaxiEventSource

    -   **Partition Count**: 2

#### Task 3: Record the Shared Access primary key

-   In the Azure Portal, access the **RootManageSharedAccessKey** shared access policy for the Event Hub namespace. Copy the primary key for this policy to a text file and save the text file.

#### Task 4: Configure a Shared Access policy for listeners

1.  In the Azure Portal, in the Event Hub namespace, add a new shared access policy named **ListenRule**. Grant the new policy Listen claims only.

2.  Copy the primary key for the **ListenRule** policy to Notepad.

#### Task 5: Create a Power BI streaming dataset

1.  Browse to the Power BI portal and sign in using your Power BI credentials.

2.  Create a new API streaming dataset named **Taxi Data**. Use the following values:

    -   **Passengers**: Number

    -   **Distance**: Number

    -   **Fares**: Number

    -   **Tips**: Number

    -   **AvgPassengers**: Number

    -   **AvgDistance**: Number

    -   **AvgFare**: Number

    -   **AvgTip**: Number

3.  Copy the Push URL for the new streaming dataset to a new text file in notepad. Save the file for later use.

#### Task 6: Create a Power BI dashboard

1.  In the Power BI portal, create a new dashboard called Taxi Data.

2.  Add card tiles for the following fields to the dashboard:

    -   **Passengers**: text label Number of Passengers

    -   **Distance**: text label Distance

    -   **Fares**: text label Fares

    -   **Tips**: text label Tips

    -   **AvgPassengers**: text label Average Passengers

    -   **AvgDistance**: text label Average Distances

    -   **AvgFare**: text label Average Fare

    -   **AvgTip**: text label Average Tip

3.  When you have finished, leave the page open in Internet Explorer.

**Results**: At the end of this exercise, you will have created an event hub that will act as a data source for the Spark Streaming application, and a Power BI dashboard that will display the results of the stream processing.

> **Note:** This lab uses the same event hub and Power BI dashboard used by the lab for Module 12. If you have already created these components, you can skip this exercise.

## Exercise 3: Building a Spark Streaming application

### Scenario

Now that you have installed the required software and configured the infrastructure, you can proceed in this exercise to creating, and running the Spark Streaming application. You will run the Spark application using a local cluster running on your virtual machine. This is the best way to build and test Spark applications. If you have time, you can create a Spark cluster using HDInsight, and then deploy the application to Azure.

The main tasks for this exercise are as follows:

1. Set up the infrastructure required to run a local Spark cluster

2. Configure streaming in a Spark application

3. Set up the streaming context

4. Define the stream processing

5. Specify the command-line arguments for the streaming application

6. Test the streaming application

#### Task 1: Set up the infrastructure required to run a local Spark cluster

1.  In the virtual machine, create a folder named **E:\\Labfiles\\Hadoop\\bin**.

2.  Create two further folders named **E:\\Labfiles\\Checkpoints** and **E:\\Labfiles\\Progress**.

3.  Using Internet Explorer, download **winutils.exe** file from **https://github.com/steveloughran/winutils/blob/master/hadoop-2.6.0/bin/winutils.exe**.

4.  Save the file in the **E:\\Labfiles\\Hadoop\\bin** folder.

#### Task 2: Configure streaming in a Spark application

1.  Start IntelliJ IDEA and open the project in the folder **E:\\Labfiles\\Lab13\\Starter\\SparkProcessor - Exercise 3**.

2.  Use the following procedure to install the Scala SDK version 2.11.8:

    -   On the **File** menu, click **Project Structure**.

    -   In the **Project Structure** dialog box, under **Platform Settings**, click **Global Libraries**.

    -   In the toolbar, click the **New Global Library** (**+**) button, and then click **Scala SDK**.

    -   In the **Select JAR's for the new Scala SDK** dialog box, click **Download**.

    -   In the **Download** dialog box, in the version drop-down list box, select **Scala** **2.11.8**, and then click **OK**.

    -   When the download is complete, in the **Select JARs for the new Scala SDK** dialog box, click the row for **Ivy Scala 2.11.8**, and then click **OK**.

    -   In the **Choose Modules** dialog box, click **OK**.

    -   In the **Project Structure** dialog box, click **OK**.

    > **Note:** This step is necessary because the Event Hubs driver and the DStreams libraries have a dependency on this version of the Scala SDK.

3.  Using the **Project** window, open the code window for the **SparkDriver** object.

4.  In the SparkDriver file, after the comment **// TODO: Create the streaming context and define the processing to be performed**, add the following code:
    ````
    val checkpointDirectory =
    inputOptions(Symbol(ClientArgumentKeys.CheckpointDirectory)).asInstanceOf[String]

    val streamingContext: StreamingContext =
    StreamingContext.getOrCreate(checkpointDirectory,

    () => createStreamingContext(inputOptions))
    ````

    This code sets the checkpoint directory used by the Spark application, and then calls the **createStreamingContext** function (which you will add shortly), to set up the processing to be performed.

5.  After the comment **// TODO: Start streaming**, add the following code:
    ````
    streamingContext.start()
    ````

    This code starts the streaming operations. The processing defined by the **createStreamingContext** function will begin running at this point.

6.  After the comment **// TODO: Stream until terminated or a few minutes have passed**, add the following code:
    ````
    streamingContext.awaitTerminationOrTimeout(120000)
    ````

    This code allows streaming to continue until the user stops the program, or a couple of minutes have passed (as defined by the duration-120000 ms).

#### Task 3: Set up the streaming context

1.  In the **createStreamingContext** function, after the comment **// TODO: Create a Spark streaming context**, add the following code:
    ````
    val checkpointDirectory =
    inputOptions(Symbol(ClientArgumentKeys.CheckpointDirectory)).asInstanceOf[String]

    val config: SparkConf =
    EventHubsUtils.initializeSparkStreamingConfigurations

    config.setAppName(options(Symbol(ClientArgumentKeys.AppName)).asInstanceOf[String])

    config.set("spark.streaming.driver.writeAheadLog.allowBatching",
    "true")

    config.set("spark.streaming.driver.writeAheadLog.batchingTimeout",
    "60000")

    config.set("spark.streaming.receiver.writeAheadLog.enable", "true")

    config.set("spark.streaming.receiver.writeAheadLog.closeFileAfterWrite",
    "true")

    config.set("spark.streaming.stopGracefullyOnShutdown", "true")

    config.setMaster("local[*]") // change to "spark://host:port to
    run in a Spark cluster

    val sparkContext: SparkContext = new SparkContext(config)
    ````

    This code creates a SparkContext that listens for incoming data using an Event Hub (you specify which event hub later). This code also configures the Spark environment.

    > **Note:** The **setMaster** method is used to specify a local cluster. If you want to run the application in a cluster on HDInsight, you replace the argument *local[*]* with the URL *spark://<host>:<port>* where *<host>* and *<port>* are the IP address and port number of the edge node in the cluster.

2.  After the comment **// TODO: Configure the Spark streaming context**, add the following code:
    ````
    val checkpointDir: String =
    options(Symbol(ClientArgumentKeys.CheckpointDirectory)).asInstanceOf[String]

    val batchDuration =
    options(Symbol(ClientArgumentKeys.BatchDuration)).asInstanceOf[Int]

    val streamingContext: StreamingContext = new
    StreamingContext(sparkContext, Seconds(batchDuration))

    streamingContext.checkpoint(checkpointDir)
    ````

    This code configures the batch duration for stream processing and then forces a checkpoint. Checkpointing ensures that the application does not repeat processing for events that have already been handled.

3.  After the comment **// TODO: Configure the Event Hub parameters for the streaming data source**, add the following code:
    ````
    val namespace: String =
    options(Symbol(ClientArgumentKeys.EventhubsNamespace)).asInstanceOf[String]

    val name: String =
    options(Symbol(ClientArgumentKeys.EventhubsName)).asInstanceOf[String]

    val progressDir: String =
    options(Symbol(ClientArgumentKeys.EventHubsProgressDirectory)).asInstanceOf[String]

    val eventHubsParams = Map[String, String] (

    "eventhubs.namespace" -> namespace,

    "eventhubs.name" -> name,

    "eventhubs.policyname" ->
    options(Symbol(ClientArgumentKeys.PolicyName)).asInstanceOf[String],

    "eventhubs.policykey" ->
    options(Symbol(ClientArgumentKeys.PolicyKey)).asInstanceOf[String],

    "eventhubs.consumergroup" -> "$Default",

    "eventhubs.partition.count" ->
    options(Symbol(ClientArgumentKeys.EventHubsPartitions)).asInstanceOf[Int].toString()

    )
    ````

    This code sets up the parameters for connecting the Spark streaming context to the event hub. The details are retrieved from command-line arguments (you will specify values for these arguments later).

4.  After the comment **// TODO: Create a stream that connects to the Event Hub**, add the following code:
    ````
    val stream = EventHubsUtils.createDirectStreams(streamingContext,
    namespace, progressDir, Map(name -> eventHubsParams))
    ````

    This statement creates a stream within the SparkContext. The stream connects to the event hub.

5.  After the comment **// TODO: Process the streamed data (parse, persist, and visualize)**, add the following code:
    ````
    doProcessing(streamingContext, stream)
    ````

    This statement runs the **doProcessing** function. You will write this function in the next task. The purpose of the **doProcessing** function is to specify the processing that the streaming application will perform over each batch of streamed data.

6.  After the comment **// TODO: Return the streaming context**, add the following code:
    ````
    return(streamingContext)
    ````

    This code passes the newly created and configured streaming context back to the caller so that it can start streaming.

#### Task 4: Define the stream processing

1.  In the **doProcessing** function, after the comment **// TODO: Use the map transformation to iterate through the RDDs and partitions in the current stream batch**, add the following code after the comment on the following line:
    ````
    stream.map(eventData => new String(eventData.getBytes)) foreachRDD {
    rdd =>
    ````

    This code specifies that the stream should perform a *map* transformation over each batch of data. The input data consists of JSON strings, and each RDD contains a collection of these strings (one for each message received from the event hub).

2.  After the comment **// TODO: Store the elements in a Set of Map objects**, add the following block of code:
    ````
    var parsedData: Set[Map[String, String]] = Set.empty

    println(s"*********** rdd is $rdd")

    rdd foreachPartition { part =>

    println(s"*********** part is $part")

    part foreach { item =>

    val mapper: ObjectMapper = new ObjectMapper // Use the ObjectMapper
    class to parse the record

    val taxiEventData: JsonNode = mapper.readTree(item)

    if (taxiEventData.has("eventData")) { // Only continue with this
    record if it is an instance of "eventData". Ignore otherwise

    val taxiRecord: JsonNode = taxiEventData.get("eventData")

    var taxiRecordInfo = Map("Pickup" ->
    taxiRecord.get("pickup").asText,

    "Dropoff" -> taxiRecord.get("dropoff").asText, "Passengers" ->
    taxiRecord.get("passengers").asText,

    "Distance" -> taxiRecord.get("distance").asText, "PickupLatitude"
    -> taxiRecord.get("pickupLatitude").asText,

    "PickupLongitude" -> taxiRecord.get("pickupLongitude").asText,
    "DropoffLatitude" -> taxiRecord.get("dropoffLatitude").asText,

    "DropoffLongitude" -> taxiRecord.get("dropoffLongitude").asText,
    "PaymentType" -> taxiRecord.get("paymentType").asText,

    "Fare" -> taxiRecord.get("fare").asText, "Extra" ->
    taxiRecord.get("extra").asText, "TipAmount" ->
    taxiRecord.get("tipAmount").asText)

    println(s"Parsed data is: ${taxiRecordInfo.toString}")

    parsedData += taxiRecordInfo

    }

    }

    }
    ````

    This code iterates through each RDD in the batch. An RDD contains zero or more partitions (if no data has arrived recently, the batch in the RDD will be empty), and each partition contains zero or more records. The code parses the JSON string and extracts the various fields it contains into a Scala **Map** object (don't confuse this with the *map* function---they are different things that just happen to have the same name). It then adds this **Map** object to a **Set**. When the   processing for the batch is complete, this **Set** will contain a **Map** for each record.

#### Task 5: Specify the command-line arguments for the streaming application

1.  Edit the existing application run configuration for the project and replace the placeholders for the following program arguments:

    -   **--application-name**: taxi-trips

    -   **--batch-duration**: 5

    -   **--eventhubs-namespace**: *\<your name\>\<date\>*ehn

    -   **--eventhubs-name**: TaxiEventSource

    -   **--policy-name**: ListenRule

    -   **--policy-key**: Copy the primary key value for the **ListenRule** policy from the file currently open in Notepad

    -   **--partition-count**: 1

    -   **--progress-directory**: E:\\Labfiles\\Progress

    -   **--checkpoint-directory**: E:\\Labfiles\\Checkpoints

2.  Make sure that the environment variable **HADOOP_HOME** listed in the run configuration is set to **E:\\Labfiles\\Hadoop**.

#### Task 6: Test the streaming application

1.  In IntelliJ IDEA, open the project in the **E:\\Labfiles\\Lab13\\Starter\\EventHubSender** folder in a new window.

    > **Note:** Make sure you don't use the existing IntelliJ window because you want to run both projects at the same time.

2.  Edit the existing application run configuration for the project and replace the placeholders for the following program arguments:

    -   **--sourcefilename**: E:\\Labfiles\\Lab13\\yellow_tripdata_2015-01.csv

    -   **--eventhubs-namespace**: \<*your name\>\<date*\>ehn

    -   **--eventhubs-name**: TaxiEventSource

    -   **--policy-name**: RootManageSharedAccessKey

    -   **--policy-key**: Copy the primary key value for the **RootManageSharedAccess** policy from the file currently open in Notepad

3.  Start the **EventHubSender** program running. Verify that it is sending messages to the event hub.

4.  While the application is running, switch back to the **SparkProcessor** project and start it running. The application should display a lengthy list of startup and debug messages, followed by messages of the form *Parsed data is: Map (...)* (in black). These messages contain the data retrieved and parsed by the application. Blocks of messages should appear at approximately five-second intervals (this is the batch duration you specified in the configuration of the application).

5.  After a few minutes, stop both applications.

6.  Examine the contents of the **E:\\Labfiles\\Checkpoints** folder. This folder contains files created by the Spark application for checkpointing progress. If you start the SparkProcessor application again, it should resume processing from where it left off rather than reprocessing all of the data in the event hub. You can force restarting from scratch by deleting the files in this folder.

 > **Note:** The checkpoint files also include information about the structure of the data being streamed. If you change this structure in your code between runs of your application (while testing and debugging, for example), your application will not be able to resume correctly using the checkpoint information. In this case, you must delete the contents of the checkpoint directory before continuing.

**Results**: At the end of this exercise, you will have created and tested a Spark Streaming application.

## Exercise 4: Summarizing and visualizing data in a Spark Streaming application

### Scenario

Your final task is to summarize the data as it is streamed to calculate the various statistics required, and display these statistics on a Power BI dashboard. In this exercise, you will extend the streaming processing to perform these tasks.

The main tasks for this exercise are as follows:

1. Calculate statistics over streamed data

2. Test the application

3. Monitor the Spark cluster

4. Lab clean-up

#### Task 1: Calculate statistics over streamed data

1.  In the IntelliJ IDEA window displaying the SparkProcessing application, open the in the folder **E:\\Labfiles\\Lab13\\Starter\\SparkProcessor - Exercise 4**. Reuse the existing IntelliJ IDEA window.

2.  Using the **Project** window, open the code window for the **SparkDriver** object.

3.  In the SparkDriver file, after the comment **// TODO: Calculate the stats for Power BI (but only if there is some data in the current batch)**, add the following code:
    ````
    if (!parsedData.isEmpty) {

    val totalPassengers: Int = parsedData.toSeq map { record =>
    record("Passengers").toInt } sum

    val totalDistance: Float = parsedData.toSeq map { record =>
    record("Distance").toFloat } sum

    val totalFares: Float = parsedData.toSeq map { record =>
    record("Fare").toFloat } sum

    val totalTips: Float = parsedData.toSeq map { record =>
    record("TipAmount").toFloat } sum

    val numTrips: Int = parsedData.size
    ````

4.  In the IntelliJ IDEA window displaying the SparkProcessing application, on the **File** menu, click **Open**, move to the **E:\\Labfiles\\Lab13\\Starter\\SparkProcessor - Exercise 4** folder, and then click **OK**.

5.  In the **Open Project** dialog box, click **This Window**.

6.  If the **Project** window is not visible, on the **View** menu, point to **Tool Windows**, and then click **Project**.

7.  In the **Project** window, move to the **SparkProcessor - Exercise 4\\src\\sparkprocessor.main\\scala** folder, and then click **SparkDriver**.

8.  Locate the comment **// TODO Calculate the stats for Power BI (but only if there is some data in the current batch)**.

9.  On the next line, add the following code:
    ````
    if (!parsedData.isEmpty) {

    val totalPassengers: Int = parsedData.toSeq map { record =>
    record("Passengers").toInt } sum

    val totalDistance: Float = parsedData.toSeq map { record =>
    record("Distance").toFloat } sum

    val totalFares: Float = parsedData.toSeq map { record =>
    record("Fare").toFloat } sum

    val totalTips: Float = parsedData.toSeq map { record =>
    record("TipAmount").toFloat } sum

    val numTrips: Int = parsedData.size
    ````

    This code checks that the Map containing parsed data is not empty (because no data was actually received during the batch interval), and then computes the sum of the Passengers, Distance, Fare, and TipAmount items in the map (these will be the aggregate values for the batch). The code also calculates how many items were processed in the batch.

10. After the comment **// TODO Send the results to the PowerBI dashboard**, add the following code:
    ````
    emitStatsToDashboard(StatisticsData(totalPassengers, totalDistance,
    totalFares, totalTips,

    totalPassengers / numTrips, totalDistance / numTrips, totalFares /
    numTrips, totalTips / numTrips, numTrips))

    }
    ````

    This statement calls the **emitStats** function to send the statistics to Power BI. The **emitStats** function is implemented after the doProcessing function. The code is the same as that utilized in previous labs in this course---it constructs an HTTP post request containing the data to display on the Power BI dashboard and sends it to the endpoint for the dashboard.

#### Task 2: Test the application

1.  Edit the existing application run configuration for the project and replace the placeholders for the following program arguments:

    -   **--application-name**: taxi-trips

    -   **--batch-duration**: 5

    -   **--eventhubs-namespace**: \<*your name\>\<date*\>ehn

    -   **--eventhubs-name**: TaxiEventSource

    -   **--policy-name**: ListenRule

    -   **--policy-key**: Copy the primary key value for the **ListenRule** policy from the file currently open in Notepad

    -   **--partition-count**: 1

    -   **--progress-directory**: E:\\Labfiles\\Progress

    -   **--checkpoint-directory**: E:\\Labfiles\\Checkpoints

    -   **--dashboard-endpoint**: Copy the URL from the **PowerBIPushURL.txt** that you created earlier

2.  Switch to the IntelliJ IDEA window for the EventHubSender program and start it running. Verify that it is sending messages to the event hub.

3.  While the application is running, switch back to the SparkProcessor project and start it running, too.

4.  Switch to Internet Explorer and view the page displaying your Power BI dashboard. Verify that the statistics are being updated every few seconds.

#### Task 3: Monitor the Spark cluster

1.  Open another Internet Explorer page and navigate to **http://localhost:4040/streaming/**.

    This site displays the telemetry generated by the Spark cluster.

2.  On the **Jobs** page, verify that jobs are running successfully and very few (if any) are failing.

3.  Examine the data on the Stages page. Each job consists of one or more processing stages, and this page displays the status and outcome of each stage.

4.  After a few minutes, stop both Spark applications.

#### Task 4: Lab clean-up

1.  In the Microsoft Azure Portal, delete your resource group.

2.  Verify that the event hub, SQL server, SQL database, and the storage account that were created, have all been removed.

3.  Close Internet Explorer, and any other open windows.

**Results**: At the end of this exercise, you will have modified the application to calculate aggregated statistics over streamed data, and display the results of these calculations using Power BI. You will also see how to monitor a Spark cluster.

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
