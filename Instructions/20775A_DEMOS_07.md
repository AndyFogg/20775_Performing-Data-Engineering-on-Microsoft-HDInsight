# Module 7: Design Batch ETL Solutions for Big Data with Spark

- [Module 7: Design Batch ETL Solutions for Big Data with Spark](#module-7-design-batch-etl-solutions-for-big-data-with-spark)
    - [Demo 1: Spark ETL](#demo-1-spark-etl)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Run a simple Spark ETL process from a Jupyter Notebook](#run-a-simple-spark-etl-process-from-a-jupyter-notebook)
    - [Demo 2: Tracking and debugging jobs running on Spark in HDInsight](#demo-2-tracking-and-debugging-jobs-running-on-spark-in-hdinsight)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Start the workload](#start-the-workload)
        - [Start Yarn UI](#start-yarn-ui)
        - [Start Spark UI](#start-spark-ui)
        - [Start Spark History server](#start-spark-history-server)

## Demo 1: Spark ETL

### Scenario

In this demonstration, you will see:
-	How to run a simple Spark ETL process from Jupyter Notebook. 

The code used in this example could be saved as a .py file then triggered as a Spark application.

### Preparation

This demonstration requires you to have access to the resources outlined in the Prerequisites section of the Module Overview.

### Run a simple Spark ETL process from a Jupyter Notebook

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**. 
3.	In the Azure Portal, click **All Resources**, click your HDInsight cluster name, and then click **Cluster dashboard**. 
4.	In the **Cluster dashboard** blade, click **Jupyter Notebook**. 
5.	If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
6.	On the Jupyter home page, click **New**, then click **PySpark**. A new browser tab will open with a new, empty notebook. Click **Untitled**, then type **wordcount** to rename the notebook.
7.	In the first cell of the notebook, copy and paste the following code to load data from a text file into an RDD, then transform the RDD into a word count:
    ```python
    from pyspark.sql import Row
    text_file = sc.textFile("wasbs:///HdiSamples/HdiSamples/TwitterTrendsSampleData/tweets.txt")
    counts = text_file.flatMap(lambda line: line.split(" ")) \
                .map(lambda word: (word, 1)) \
                .reduceByKey(lambda a, b: a + b)
    counts.take(5) #for visualization of the interim step only
    counts.count() #for visualization of the interim step only
    ```
8.	Press SHIFT+ENTER to create a new cell, then copy and paste the following code into the new cell to map the word count into an RDD of rows, convert the new RDD to a DataFrame, and then save the DataFrame as a table:
    ```python
    counts_row = counts.map(lambda p: Row(word=p[0], count=int(p[1])))
    schemaCounts = sqlContext.createDataFrame(counts_row)
    schemaCounts.registerTempTable("counts")
    ```
9.	Press SHIFT+ENTER to create a new cell, then copy and paste the following code into the new cell to view the top five most common words in the table:
    ```python
    output = sqlContext.sql("select * from counts order by count desc limit 5")
    for eachrow in output.collect():
    print(eachrow) 
    ```
10.	Click **Cell**, then click **Run All**. When the notebook has finished processing, review the results, click **File**, and then click **Close and Halt** to close the notebook.

## Demo 2: Tracking and debugging jobs running on Spark in HDInsight

### Scenario

In this demonstration, you will learn how to track and debug Spark jobs using: 
-	YARN UI
-	Spark UI
-	Spark History Server

### Preparation

This demonstration requires an HDInsight on Azure Spark cluster, created in the previous demonstration.

The workload used in this module is a machine learning example given as one of the sample notebooks provided with Jupyter on all HDInsight on Azure clusters.

### Start the workload

1.	In Internet Explorer, navigate to:
    ```
    https://CLUSTERNAME.azurehdinsight.net/jupyter
    ```
    (Replace **CLUSTERNAME** with the name of your HDInsight cluster.) 
2.	If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.	Click **PySpark**, click **05 - Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb**, click **Cell**, and then click **Run All**.

### Start Yarn UI

1.	In Internet Explorer, open a new browser tab, and navigate to:
    ```
    https://CLUSTERNAME.azurehdinsight.net/yarnui
    ```
    (Replace **CLUSTERNAME** with the name of your cluster.)
2.	If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.	Click the link in the ID column for the application with the name **remotesparkmagics** and a state of **ACCEPTED** or **RUNNING**.
4.	On the **Application** page, click the link in the **Attempt ID** column to view the details of the nodes that are executing the job.
5.	On the **Application Attempt** page, click the link in the first row of the Logs column to view the logs for a container.
6.	On the **Logs for container** page, click the link named **stderr : Total file length is _n_ bytes** to view the stderr output for the container, and then click **Back**.
7.	On the **Logs for container** page, click **Back**.
8.	On the **Application Attempt** page, click **Back** to return to the **All Applications** page.

### Start Spark UI

1.	On the **All Applications** page of Yarn UI, in the application with the name **remotesparkmagics**, click the **ApplicationMaster** link, on the row labelled **Tracking URL** to start Spark UI.
2.	On the **Spark Jobs** page, you see a list of all the completed jobs in the application. The number of jobs on the page will vary, depending on how long the job has been running when you reach this point. Click the link in the first row of the **Description** column to view the stages of the individual job.
3.	On the **Details for Job _n_** page, you see a list of stages in the job. Click the link in the first row of the **Description** column to view details of the stage (depending on the job, there might be only one stage).
4.	On the **Details for Stage _n_** page, you see the stage details. Click the **Event Timeline** link to view the timeline visualization.
5.	Click the **SQL** link to switch to the Spark SQL view for the application. Click the link in the row of the **Description** column of row **ID = 1** to view details of the query. This should be the SQL query with the longest duration.
6.	On the **Details for Query 1** page, click **Details** to view the textual detail of the query plan.
7.	Click **Executors**, then click the **stderr** link in the **Logs** column of the first row to view the error output for the first executor. 
8.	Click **Back**, then click **Thread Dump** on the first row to view a thread dump for the executor.

### Start Spark History server

1.	In Internet Explorer, open a new browser tab, and navigate to:
    ```
    https://CLUSTERNAME.azurehdinsight.net/sparkhistory
    ```
    (Replace **CLUSTERNAME** with the name of your cluster.)
2.	If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.	Notice that a history entry for your application does not appear, because the Jupyter notebook is still open.
4.	Return to the browser tab where the Jupyter notebook 05 - Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb is open. Click **File**, and then click **Close and Halt** to close the notebook.
5.	In the browser tab where **Spark History** is open, click **Refresh**. An entry should appear for the application (the application will have the same name as the one that appears in Spark UI).
6.	Click the link in the **App ID** column for the demo application. 
7.	Click **Jobs**, click **Stages**, and then click **SQL** to demonstrate that very similar information is available through Spark UI and Spark History Server.
8.	Close all browser windows.


---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.