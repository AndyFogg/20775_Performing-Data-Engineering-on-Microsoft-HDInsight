# Module 7: Design Batch ETL Solutions for Big Data with Spark

- [Module 7: Design Batch ETL Solutions for Big Data with Spark](#module-7-design-batch-etl-solutions-for-big-data-with-spark)
    - [Demo 1: Spark ETL](#demo-1-spark-etl)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Run a simple Spark ETL process from a Jupyter Notebook](#run-a-simple-spark-etl-process-from-a-jupyter-notebook)
    - [Demo 2: Tracking and debugging jobs running on Spark in HDInsight](#demo-2-tracking-and-debugging-jobs-running-on-spark-in-hdinsight)
        - [Scenario](#scenario-1)
        - [Preparation](#preparation-1)
        - [Start the workload](#start-the-workload)
        - [Start Yarn UI](#start-yarn-ui)
        - [Start Spark UI](#start-spark-ui)
        - [Start Spark History server](#start-spark-history-server)

## Demo 1: Spark ETL

### Scenario

In this demonstration, you will see:
-  How to run a simple Spark ETL process from Jupyter Notebook. 

The code used in this example could be saved as a .py file then triggered as a Spark application.

### Preparation

This demonstration requires an HDInsight on Azure Spark cluster, which is created in the following steps.

1.  On the **20775A-LON-DEV** virtual machine, on the Start menu, start to type **Internet Explorer**, and then click **Internet Explorer**.
2.  In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Microsoft Learning Azure Pass subscription.
3.  In the Azure Portal, click **All resources**, and then verify that there are no existing HDInsight clusters in your subscription. 
4.  In the Azure Portal, in the left pane, click **+ Create a resource**.
5.  Click **Analytics**, and then click **HDInsight**.
6.  On the **HDInsight** blade, click **Custom (size, settings, apps)**.
7.  On the **Basics** blade, type the following details, and then click **Cluster type**:
    -  **Cluster name**: \<*your name\>\<date*\>hdi
    -  **Subscription**: your subscription
8.  On the **Cluster configuration** blade, enter the following details, and then click **Select**:
    -  **Cluster type**: Spark
    -  **Operating system**: Linux
    -  **Version**: Spark 1.6.3 (HDI 3.5)
9.  On the **Basics** blade, enter the following details, and then click **Next**:
    -  **Cluster login username**: sparkadmin
    -  **Cluster login password**: Pa55w.rdPa55w.rd
    -  **Secure Shell (SSH) username**: sshadmin
    -  **Use same password as cluster login**: selected
    -  **Resource group (Create new)**: Sparkrg
    -  **Location**: Select your region
10. On the **Security + networking** blade, click **Next**.
11. On the **Storage** blade, under **Select a Storage account**, click **Create new**.
12. In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**; point out that this name must be globally unique.
13. In the **Default container** box, replace the suggested name with the name of your cluster (for example, **\<*your name\>\<date*\>hdi**).
14. Leave all other settings at their defaults, and then click **Next**.
15. On the **Applications (optional)** blade, click **Next**.
16. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **1**.
17. Click **Worker node size**.
18. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
19. Click **Head node size**.
20. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
21. On the **Cluster size** blade, click **Next**.
22. On the **Script Actions (optional)** blade, click **Next**.
23. On the **Cluster summary** blade, point out the estimated cost per hour of this cluster. Remind students that, to avoid using up their Azure Pass allowance, it is important that they remove clusters when they are not using them.
24. On the **Cluster summary** blade, click **Create**.
25. Wait for the cluster to be provisioned before proceeding with the demonstration. This may take 15â€“20 minutes
26. Click **All resources**, click **\<*your name\>\<date*\>hdi**, and then show the status as **Running**.

### Run a simple Spark ETL process from a Jupyter Notebook

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.  In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**. 
3.  In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>hdi**, and then click **Cluster dashboards**. 
4.  On the **Cluster dashboards** blade, click **Jupyter Notebook**. 
5.  If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
6.  On the Jupyter home page, click **New**, and then click **PySpark**.
7.  If the **404 : Not Found** page appears, complete the following steps:
    1.  Click the Jupyter page heading.
    2.  On the Jupyter home page, click **Untitled.ipynb**.
8.  In the new browser tab, click **Untitled**, type **wordcount**, and then click **OK** to rename the notebook.
9.  In the first cell of the notebook, copy and paste the following code to load data from a text file into an RDD, then transform the RDD into a word count:
    ````python
    from pyspark.sql import Row
    text_file = sc.textFile("wasbs:///HdiSamples/HdiSamples/TwitterTrendsSampleData/tweets.txt")
    counts = text_file.flatMap(lambda line: line.split(" ")) \
                .map(lambda word: (word, 1)) \
                .reduceByKey(lambda a, b: a + b)
    counts.take(5) #for visualization of the interim step only
    counts.count() #for visualization of the interim step only
    ````
10. Press SHIFT+ENTER to create a new cell, then copy and paste the following code into the new cell to map the word count into an RDD of rows, convert the new RDD to a DataFrame, and then save the DataFrame as a table:
    ````python
    counts_row = counts.map(lambda p: Row(word=p[0], count=int(p[1])))
    schemaCounts = sqlContext.createDataFrame(counts_row)
    schemaCounts.registerTempTable("counts")
    ````
11. Press SHIFT+ENTER to create a new cell, then copy and paste the following code into the new cell to view the top five most common words in the table:
    ````python
    output = sqlContext.sql("select * from counts order by count desc limit 5")
    for eachrow in output.collect():
        print(eachrow)
    ````
12. On the **Cell** menu, click **Run All**.
13. When the notebook has finished processing, review the results.
14. On the menu **File** menu, click **Close and Halt** to close the notebook.

## Demo 2: Tracking and debugging jobs running on Spark in HDInsight

### Scenario

In this demonstration, you will learn how to track and debug Spark jobs using: 
-  YARN UI
-  Spark UI
-  Spark History Server

### Preparation

This demonstration requires an HDInsight on Azure Spark cluster, created in the previous demonstration.

The workload used in this module is a machine learning example given as one of the sample notebooks provided with Jupyter on all HDInsight on Azure clusters.

### Start the workload

1.  In Internet Explorer, navigate to:
    ````
    https://<CLUSTERNAME>.azurehdinsight.net/jupyter
    ````
    (Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster.) 
2.  If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.  Click **PySpark**, click **05 - Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb**.
4.  On the **Cell** menu, click **Run All**.

### Start Yarn UI

1.  In Internet Explorer, open a new browser tab, and navigate to:
    ````
    https://<CLUSTERNAME>.azurehdinsight.net/yarnui
    ````
    (Replace **\<CLUSTERNAME\>** with the name of your cluster.)
2.  If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.  In the **Name** column, in the **remotesparkmagics** row, click the link in the **ID** column with a **State** of **ACCEPTED** or **RUNNING**.
4.  On the **Application** page, in the **Attempt ID** column, click the link to view the details of the nodes that are executing the job.
5.  On the **Application Attempt** page, in the **Logs** column, click the link in the first row to view the logs for a container.
6.  On the **Logs for container** page, click the link named **stderr : Total file length is *n* bytes** to view the stderr output for the container, and then click **Back**.
7.  On the **Logs for container** page, click **Back**.
8.  On the **Application Attempt** page, click **Back**.
9.  On the **Application** page, click **Back** to return to the **All Applications** page.

### Start Spark UI

1.  On the **All Applications** page of Yarn UI, in the application with the name **remotesparkmagics**, click the **ApplicationMaster** link, on the row labelled **Tracking UI** to start Spark UI.
2.  On the **Spark Jobs** page, you will see a list of all the completed jobs in the application. The number of jobs on the page will vary, depending on how long the job has been running when you reach this point. Click the link in the first row of the **Description** column to view the stages of the individual job.
3.  On the **Details for Job *n*** page, you will see a list of stages in the job. Click the link in the first row of the **Description** column to view details of the stage (depending on the job, there might be only one stage).
4.  On the **Details for Stage *n*** page, you will see the stage details. Click the **Event Timeline** link to view the timeline visualization.
5.  Click the **SQL** tab to switch to the Spark SQL view for the application.
6.  Click the link in the row of the **Description** column of row **ID = 1** to view details of the query. This should be the SQL query with the longest duration.
7.  On the **Details for Query 1** page, click **Details** to view the textual detail of the query plan.
8.  Click the **Executors** tab, then click the **stderr** link in the **Logs** column of the first row to view the error output for the first executor. 
9.  Click **Back**, then click **Thread Dump** on the first row to view a thread dump for the executor.

### Start Spark History server

1.  In Internet Explorer, open a new browser tab, and navigate to:
    ````
    https://<CLUSTERNAME>.azurehdinsight.net/sparkhistory
    ````
    (Replace **\<CLUSTERNAME\>** with the name of your cluster.)
2.  If an **Authentication Required** dialog box appears, use the username **sparkadmin** and the password **Pa55w.rdPa55w.rd**. 
3.  Notice that a history entry for your application does not appear, because the Jupyter notebook is still open.
4.  Return to the browser tab where the Jupyter notebook **05 - Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb** is open.
5.  On the **File** menu, click **Close and Halt** to close the notebook.
6.  In the browser tab where **Spark History** is open, click **Refresh**. An entry should appear for the application (the application will have the same name as the one that appears in Spark UI).
7.  Click the link in the **App ID** column for the demo application. 
8.  Click the **Jobs** tab, click the **Stages** tab, and then click the **Executors** tab to demonstrate that very similar information is available through Spark UI and Spark History Server.
9.  Close all browser windows.

>**Note**: This is the final demonstration in this module. Make sure you delete the cluster, and resource group before you start the next module.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
