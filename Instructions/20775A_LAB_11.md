# Module 11: Implementing Streaming Solutions with Kafka and HBase

## Lab: Implementing streaming solutions with Kafka and HBase

### Scenario

You have access to a stream of data generated by the meters in a fleet of taxis in New York. The data includes information about each trip, including the time and location of each pickup, the time and location of each drop-off, and the price, tax, and tips paid. You want to create a streaming application in HDInsight that receives events from this data stream, calculates summary statistics, and displays information in Power BI.

You want to calculate the following summary statistics based on the live data stream:

-   The number of trips currently running.

-   The average number of passengers.

-   The average distance for a trip.

-   The average fare for a trip.

You have been asked to build this application as a Kafka streaming application.

### Objectives

After completing this lab, you will be able to:

-   Create a virtual network and gateway.

-   Create a Storm cluster.

-   Create an HBase cluster.

-   Create a Kafka producer.

-   Create a Power BI dashboard.

-   Create a streaming processor client topology.

### Lab Setup

-   **Estimated Time**: 90 minutes

-   **Virtual machine**: 20775A-LON-DEV

-   **Username**: Admin

-   **Password**: Pa55w.rd

## Exercise 1: Prepare the lab environment

### Scenario

Before you can build a Kafka streaming application that analyses a taxi data stream, you must install and configure the required software on your development machine.

The main tasks for this exercise are as follows:

1. Install IntelliJ IDEA

2. Configure IntelliJ IDEA

3. Install PuTTY (if not already installed)

4. Run the Setup script

#### Task 1: Install IntelliJ IDEA

1.  Install the **IntelliJ Community Edition** on to the 20775A-LON-DEV virtual machine, from **https://www.jetbrains.com/idea/download/index.html#section=windows**.

2.  When the installation is complete, run **IntelliJ IDEA Community Edition**, without importing any settings, and install the **Scala** plugin.

#### Task 2: Configure IntelliJ IDEA

1.  In Customize IntelliJ IDEA, create a new **Scala** **IDEA** project.

2.  On the **New Project** page, download and install the Java JDK, using **C:\\Program Files\\Java\\jdk1.8.0_131**, as the home directory.

3.  On the **New Project** page, download and install the Scala version 2.12.2 SDK. If this errors, restart IntelliJ IDEA and repeat up to this point (you can skip the downloading of Java).

4.  Finish the new project configuration.

5.  If you get a **Windows Security Alert** dialog box for the **OpenJDK Platform binary**, click **Allow access**.

6.  If you get a **Windows Security Alert** dialog box for **IntelliJ IDEA**, click **Allow access**.

7.  Disable the **Android Support** plugin.

8.  From the available repositories, install the **Azure Toolkit for IntelliJ**.

9.  Close IntelliJ IDEA.

#### Task 3: Install PuTTY (if not already installed)

1.  Navigate to **http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html**.

2.  Download the Putty.exe file to a convenient location and create a shortcut on your desktop.

#### Task 4: Run the Setup script

-   Run the **Setup.cmd** script in the **E:\\Labfiles\\Lab11** folder as Administrator.

**Results**: At the end of this exercise, you will have a Windows client with IntelliJ IDEA Community Edition, the Java SE Development Kit, the Scala SDK, and the PuTTY tool installed for use later in the lab.

> **Note:** If you already have all of these components installed in the 20775A-LON-DEV virtual machine, you can skip this exercise.

## Exercise 2: Create a virtual network and gateway

### Scenario

You have decided to build the taxi event data analysis application as a Kafka streaming application that stores data in an HBase. During development, you will use an Excel spreadsheet as a source of data. In this exercise, you will create and configure the virtual network, public IP address, and virtual network gateway that you will need in order to access the Kafka nodes with your application.

The main tasks for this exercise are as follows:

1. Create and configure a virtual network

2. Create a public IP address

3. Create VPN gateway

4. Create and install certificates

5. Configure VPN gateway

6. Install VPN client

#### Task 1: Create and configure a virtual network

-   In the Azure Portal, create a virtual network using the following values:

    -   **Name**: *vnet\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date (make sure you include the *vnet* prefix).

    -   **Subnet address range**: Reduce size of the address space for the default subnet to **/25**. This is important, as it allows space for a VPN gateway subnet to be created later.

    -   **Subscription**: Azure Pass

    -   **Resource group (Create new)**: *rg\<your initials\>\<ddmmyyyy\>*

    -   **Location**: Select your region.

    -   Leave all other settings at their defaults.

#### Task 2: Create a public IP address

-   In the Azure Portal, create a public IP address blade, using the following values:

    -   **Name**: *pip\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date (make sure you include the *vnet* prefix).

    -   **DNS name label**: *pip\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date (this name must be regionally unique so, if necessary, append a letter or number).

    -   **Subscription**: Azure Pass.

    -   **Resource group**: Use existing, and select the resource group you created for the virtual network.

    -   **Location**: Select your region.

    -   Leave all other settings at their defaults.

#### Task 3: Create VPN gateway

1.  In the Azure Portal, create a virtual network gateway, using the following values:

    -   **Name**: *gwy\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date (make sure you include the *vnet* prefix).

    -   **SKU**: VpnGw1.

    -   **Virtual network**: select your virtual network.

    -   **Public IP address**: select your public IP address.

    -   **Location**: Select your region.

    -   Leave all other settings at their defaults.

2.  Note that the gateway deployment may take up to 45 minutes to complete; you can continue with the next task while the gateway is being deployed, but do not start Exercise 3 until the gateway is ready.

#### Task 4: Create and install certificates

1.  Use the **Windows PowerShell ISE**, to open and run **E:\\Labfiles\\Lab11\\createCertificates.ps1**.

2.  Use **Certificate Manager**, to move **P2SRootCert** from the **Personal\\Certificates** store to the **Trusted Root Certification Authorities\\Certificates** store.

3.  Export **P2SRootCert** to **Base-64 encoded X.509 (.CER)**, and save as **E:\\Labfiles\\Lab11\\P2SRootCert.cer**.

4.  Open **P2SRootCert.cer** in Notepad, and delete the following lines:
    ````
    -----BEGIN CERTIFICATE-----

    -----END CERTIFICATE-----
    ````

5.  Save the file as **P2SRootCert-edited.txt**.

6.  In the **Windows PowerShell ISE**, open and run **E:\\Labfiles\\Lab11\\cleanCertificate.ps1**, using **P2SRootCert-edited.txt** as the source file.

7.  Open **P2SRootCert-edited.txt**, and verify that the certificate data is now one single text string.

#### Task 5: Configure VPN gateway

1.  In the Azure Portal, check that the Virtual network gateway has been deployed; if the deployment has not completed, wait before proceeding.

2.  Open your Virtual network gateway, and edit the **Point-to-site configuration** with the following:

    -   **Address pool**: 172.16.201.0/24.

    -   **Root certificates NAME**: P2SRootCert.

    -   **Root certificates PUBLIC CERTIFICATE DATA**: copy the certificate data from P2SRootCert-edited.txt, and save the configuration.

#### Task 6: Install VPN client

1.  When the update to the Virtual network gateway is complete, download and install the VPN client, ensuring that **x64** is selected.

2.  If you get a **Windows Defender Smartscreen** dialog box, click **More info**, and then click **Run anyway**.

3.  Start the VPN connection, and verify that the VPN status is now showing as **Connected**.

**Results**: By the end of this exercise, you should have a functioning virtual network and gateway, with the VPN client installed.

## Exercise 3: Create a Storm cluster for Kafka

### Scenario

You are building the taxi event data analysis application as a Kafka streaming application that stores data in an HBase, using an Excel spreadsheet as a source of development data. In this exercise, you will create and configure the HDInsight Storm cluster, and configure Kafka so that you can connect to it from your application.

The main tasks for this exercise are as follows:

1. Create Storm cluster

2. Configure Kafka brokers for IP advertising

3. Get IP addresses of worker nodes

#### Task 1: Create Storm cluster

-   Create a new HDInsight cluster with the following details:

    -   **Cluster name**: *storm\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date.

    -   **Cluster type**: Storm, Linux, Storm 1.1.0 (HDI 3.6), Standard.

    -   **Cluster login username**: admin.

    -   **Cluster login password**: Pa55w.rdPa55w.rd.

    -   **Secure Shell (SSH) username**: sshuser.

    -   **Resource group (Use existing)**: Specify the resource group you created for the virtual network.

    -   **Location**: Select your region

    -   **Storage account (Create new)**: *account\<your initials\>\<ddmmyyyy\>.*

    -   **Virtual network**: select the virtual network you created earlier.

    -   **Cluster size**: Number of Supervisor nodes: 2.

    Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

#### Task 2: Configure Kafka brokers for IP advertising

1.  Open the Ambari dashboard for your Storm cluster.

2.  If you are prompted for credentials, use the following:

    -   **User name**: admin

    -   **Password**: Pa55w.rdPa55w.rd

3.  In the Ambari console, open the **Kafka** **Configs** tab, and filter for **kafka-env**.

4.  Add the following text to the bottom of the **kafka-env-template**; this can be copied from **E:\\Labfiles\\Lab11\\KafkaIPAdvertising.txt**:
    ````
    # Configure Kafka to advertise IP addresses instead of FQDN

    IP_ADDRESS=$(hostname -i)

    echo advertised.listeners=$IP_ADDRESS

    sed -i.bak -e '/advertised/{/advertised@/!d;}'
    /usr/hdp/current/kafka-broker/conf/server.properties

    echo "advertised.listeners=PLAINTEXT://$IP_ADDRESS:9092" >>
    /usr/hdp/current/kafka-broker/conf/server.properties
    ````

5.  Save the configuration as **Advertise Kafka IP**.

6.  Filter the configuration for **listeners**, and change the listeners string to the following:
    ````
    PLAINTEXT://0.0.0.0:9092
    ````

7.  Save the configuration as **Listen on all interfaces**.

8.  Turn on Maintenance Mode.

9.  Restart Kafka.

10. When all the background operations have completed turn off Maintenance Mode.

#### Task 3: Get IP addresses of worker nodes

1.  Switch to the PowerShell ISE.

2.  Open **E:\\Labfiles\\Lab11\\ShowNodeIPs.ps1**, and replace **\<*resource group*\>** with the name the resource group you created for your virtual network.

3.  Run the script and make a note of the two worker node IP addresses.

**Results**: By the end of this exercise, you should have created a Storm cluster, configured Kafka brokers for IP advertising, and obtained the IP addresses of the worker nodes.

## Exercise 4: Create an HBase cluster

### Scenario

You are building the taxi event data analysis application as a Kafka streaming application that stores data in an HBase, using an Excel spreadsheet as a source of development data. In this exercise, you will create and configure the HBase cluster.

The main tasks for this exercise are as follows:

1. Create HBase cluster

#### Task 1: Create HBase cluster

1.  Create a new HDInsight cluster with the following details:

    -   **Cluster name**: *hbase\<your initials\>\<ddmmyyyy\>* where *\<ddmmyyyy\>* is the current date

    -   **Cluster type**: Hbase, Linux, Hbase 1.1.2 (HDI 3.6), Standard

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rdPa55w.rd

    -   **Secure Shell (SSH) username**: sshuser

    -   **Resource group (Use existing)**: specify the resource group you created for the virtual network

    -   **Location**: Select your region

    -   **Storage account (Create new)**: s*a2\<your initials\>\<ddmmyyyy\>*

    -   **Virtual network**: select the virtual network you created earlier

    -   **Number of Region nodes**: 2

    -   **Region node size**: A3 General Purpose

    -   **Head node size**: A3 General Purpose

    Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

**Results**: By the end of this exercise, you will have created an HBase cluster.

## Exercise 5: Create a Kafka producer

### Scenario

You are building the taxi event data analysis application as a Kafka streaming application that stores data in an HBase, using an Excel spreadsheet as a source of development data. In this exercise, you will create a Kafka producer using Scala code, and then test this producer using Kafka console commands.

The main tasks for this exercise are as follows:

1. Create producer

2. Test the producer

#### Task 1: Create producer

1.  Start **IntelliJ IDEA**, and open **E:\\Labfiles\\Lab11\\Kafka\\Producer**.

2.  Edit the **Run** configurations, **Program arguments**, so that bootstrap server IP addresses are the worker node IP addresses that you noted earlier, and then run the code.

3.  After a few minutes, you should see messages being sent to topic "taxi-events".

#### Task 2: Test the producer

1.  Use the Azure Portal to obtain the **Secure Shell (SSH)** **host name** for your Storm cluster.

2.  Use PuTTY to open an SSH session using this host name, and the following credentials:

    -   User name: **sshuser**

    -   Password: **Pa55w.rdPa55w.rd**.

3.  In the Ambari console, on the **Kafka**, **Configs** page, make a note of the name of the first Kafka Broker host (it will be a long name, ending with **internal.cloudapp.net**).

4.  Type the following command to start a bash shell as the superuser (this can be copied from **E:\\Labfiles\\Lab11\\ KafkaCmds.txt**:
    ````
    sudo bash
    ````

5.  Type the following command to change to the kafka-broker/bin folder:
    ````
    cd /usr/hdp/current/kafka-broker/bin
    ````

6.  Type the following command to run kafka-console-consumer.sh, replacing **\<kafka-broker\>** with the name of the Kafka Broker host you copied in step 7:
    ````
    ./kafka-console-consumer.sh --bootstrap-server <kafka-broker>:9092
    --topic taxi-events
    ````

    You should now see the "taxi-events" topic messages being picked up by the Kafka consumer.

**Results**: By the end of this exercise, you will have created and tested a Kafka producer.

## Exercise 6: Create a Power BI dashboard

### Scenario

You are building the taxi event data analysis application as a Kafka streaming application that stores data in an HBase, using an Excel spreadsheet as a source of development data. In this exercise, you will be using Power BI to visualize the output from your Kafka streaming application.

The main tasks for this exercise are as follows:

1. Obtain the HBase node IP address

2. Create a Power BI streaming dataset

3. Create a Power BI dashboard

#### Task 1: Obtain the HBase node IP address

1.  In the Azure Portal, view the Dashboard of your HBase cluster in All Resources.

2.  Use credentials of **admin** and **Pa55w.rdPa55w.rd** to connect.

3.  In the Ambari console, go to the **HBase** summary page.

4.  In the **Active HBase Master**, note the IP address from the **Summary** section.

#### Task 2: Create a Power BI streaming dataset

1.  In Internet Explorer, browse to **http://www.powerbi.com** and sign in with your Power BI credentials.

2.  Create a new streaming dataset using the API.

3.  Enter the following details and then create the dataset:

    -   **Dataset name**: Taxi Trips

    -   **Values from stream**:

        -   **NumTrips**: Number

        -   **AvgPassengers**: Number

        -   **AvgDistance**: Number

        -   **AvgFare**: Number

4.  Copy the Push URL to Notepad.

#### Task 3: Create a Power BI dashboard

1.  In Power BI page, create a dashboard called **Taxi Trips**.

2.  Use the Taxi Trips custom streaming data option, add the following card tiles:

    -   **NumTrips**, text label **Number of Trips**

    -   **AvgPassengers**, text label **Average Passengers**

    -   **AvgDistance**, text label **Average Distance**

    -   **AvgFare**, text label **Average Fare ($)**

**Results**: By the end of this exercise, you will have obtained the HBase node IP address, created a Power BI streaming dataset and created a Power BI dashboard.

## Exercise 7: Create a streaming processor client topology

### Scenario

You are building the taxi event data analysis application as a Kafka streaming application that stores data in an HBase, using an Excel spreadsheet as a source of development data. In this exercise, you will create a Kafka streaming processor in Scala that writes data to HBase; you will then use the HBase shell to query this stored data.

The main tasks for this exercise are as follows:

1. Create Kafka streaming processor

2. View the Power BI dashboard

3. Run HBase query

4. Lab clean-up

#### Task 1: Create Kafka streaming processor

1.  In IntelliJ IDEA, open a file or project from the **E:\\Labfiles\\Lab11\\Kafka\\Streamer** folder, then select a new window.

2.  In the new instance, select **streamerdriver**, then click **Run**.

3.  Edit the configuration to set the following program arguments:

    -   **Bootstrap server IP addresses**: the worker node IP addresses that you noted earlier in Exercise 3.

    -   **Power BI dashboard endpoint URL**: the URL you copied in task 3 of the last exercise.

    -   **HBase node IP address**: the HBase active master IP address you noted in task 1 of the last exercise.

4.  Run the task and check that messages are being sent to the Power BI dashboard, and to HBase.

#### Task 2: View the Power BI dashboard

-   Switch to the Power BI dashboard; after a few moments, you should see the **Number of Trips**, **Average Passengers**, **Average Distance**, and **Average Fare ($)** tiles start to display live real-time data.

#### Task 3: Run HBase query

1.  Switch to the Azure Portal, and from the SSH Blade, copy the host name to the clipboard.

2.  Start two instances of PuTTY.

3.  In the first PuTTY instance, open the SSH host name.

4.  Log on as an **sshuser** with the password **Pa55w.rdPa55w.rd**.

5.  Start a bash shell as a superuser.

6.  Start the Hbase shell.

7.  Display the HBase tables.

8.  Display the content of the TaxiData table.

9.  Get a set of row data from the TaxiData table.

10.  Close the HBase shell and the SSH console.

#### Task 4: Lab clean-up

1.  Close both instances of IntelliJ IDEA.

2.  Close the remaining SSH console.

3.  Disconnect your VPN connection.

4.  In the Microsoft Azure Portal, delete your resource group, and verify that the clusters, gateway, vnet, and the storage accounts that were created with your clusters, have all been removed.

**Results**: By the end of this lab you will have created a Kafka streaming processor, viewed the outputs on the Power BI dashboard, run an HBase query and cleaned up the lab.

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
