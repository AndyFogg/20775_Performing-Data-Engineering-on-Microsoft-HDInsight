# Module 8: Analyzing Data with Spark SQL

## Lab: Performing exploratory data analysis by using iterative and interactive queries

### Exercise 1: Build a machine learning application

#### Task 0: Prepare the environment

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**. 

2.  In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**.  

3.  In the Azure Portal, in the left pane, click **+ Create a resource**. 

4.  Click **Analytics**, and then click **HDInsight**. 

5.  On the **HDInsight** blade, click **Custom (size, settings, apps)**. 

6.  On the **Basics** blade, type the following details, and then click **Cluster type**: 
    -   **Cluster name**: \<*your name\>\<date*\>hdi
    -   **Subscription**: \<your subscription\>

7.  On the **Cluster configuration** blade, enter the following details, and then click **Select**: 
    -   **Cluster type**: Spark
    -   **Version**: 2.1.0 (HDI 3.6)

8.  On the **Basics** blade, enter the following details, and then click **Next**: 
    -   **Cluster login username**: admin
    -   **Cluster login password**: Pa55w.rdPa55w.rd
    -   **Secure Shell (SSH) username**: sshuser
    -   **Use same password as cluster login**: selected
    -   **Resource group (Create new)**: SparkSQLrg
    -   **Location**: Select your region

9.  On the **Security + networking** blade, click **Next**. 

10. On the **Storage** blade, under **Select a Storage account**, click **Create new**. 

11. In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**; point out that this name must be globally unique. 

12. In the **Default container** box, replace the suggested name with the name of your cluster (for example, \<*your name\>\<date*\>-ctr). 

13. Leave all other settings at their defaults, and then click **Next**. 

14. On the **Applications (optional)** blade, click **Next**. 

15. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **4**. 

16. Click **Worker node size**. 

17. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**. 

18. Click **Head node size**. 

19. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**. 

20. On the **Cluster size** blade, click **Next**. 

21. On the **Script actions** blade, click **Next**. 

22. On the **Cluster summary** blade, point out the estimated cost per hour of this cluster. Remember that, to avoid using up your Azure Pass allowance, it is important that you remove clusters when you are not using them. 

23. On the **Cluster summary** blade, click **Create**. 

24. The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned.

25. In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>hdi**, and then ensure the **Status** shows as **Running**.

#### Task 1: Review the data

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.

2.  On the **Start** menu, type **Internet Explorer**, and then press Enter.

3.  In Microsoft Internet Explorer®, navigate to **https://portal.azure.com**, and then sign in using the Microsoft account that is associated with your Azure Pass subscription.

4.  In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>sa**.

5.  On the **\<*your name\>\<date*\>sa** blade, under **Blob service**, click **Blobs**, and then click **\<*your name\>\<date*\>hdi-ctr**.

6.  On the **\<*your name\>\<date*\>hdi-ctr** blade, navigate to the **HdiSamples\\HdiSamples\\SensorSampleData\\hvac** folder, and then click **HVAC.csv**.

7.  On the **Blob** blade, click **Download**.

8.  In the message box, click the **Save** drop-down arrow, and then click **Save as**.

9.  In the **Save As** dialog box, navigate to **E:\\Labfiles\\Lab08**, and then click **Save**.

10. In File Explorer, navigate to **E:\\Labfiles\\Lab08**, and then double-click **HVAC.csv**.

11. In Excel 2016, view the raw data.

    > **Note**: Alternatively, a copy of the file is available at **E:\\Labfiles\\Lab08\\Solution\\HVAC.csv**.

    The data shows the target temperature and the actual temperature of a building that has an HVAC system. The **System** column represents the system ID and the **SystemAge** column represents the number of years that the HVAC system has been in place at the building.

    You will use this data to predict whether a building will be hotter or colder than the target temperature, given a system ID and system age.
12. Close Excel 2016.

#### Task 2: Create an iterative machine learning application by using Jupyter Notebooks

1.  In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>hdi**, and then click **Cluster dashboards**.

2.  On the **Cluster dashboards** blade, click **Jupyter notebook**.

3.  If a **Windows Security** dialog box appears, in the **User name** box, type **admin**, in the **Password** box, type **Pa55w.rdPa55w.rd**, and then click **OK**.

4.  On the **Jupyter** home page, click the **New** drop-down list, and then click **PySpark**.

    A new browser tab will open with a new, empty notebook.

5.  If the new tab opens with the **404 : Not Found** message, complete the following steps:

    1.  Close the tab.

    2.  In the **Creating Notebook Failed** dialog box, click **OK**.

    3.  On the **Jupyter** home page, click **Untitled.ipynb**.

6.  Click **Untitled**, type **HVAC-ML**, and then click **OK**.

7.  In the first cell of the notebook, type the following code.
    ````
    from pyspark.ml import Pipeline
    from pyspark.ml.classification import LogisticRegression
    from pyspark.ml.feature import HashingTF, Tokenizer
    from pyspark.sql import Row

    import os
    import sys
    from pyspark.sql.types import *

    from pyspark.mllib.classification import LogisticRegressionWithSGD
    from pyspark.mllib.regression import LabeledPoint
    from numpy import array
    ````

8.  Press Shift+Enter to create a new cell, and then type the following code into the new cell.
    ````
    # 0 Date
    # 1 Time
    # 2 TargetTemp
    # 3 ActualTemp
    # 4 System
    # 5 SystemAge
    # 6 BuildingID

    LabeledDocument = Row("BuildingID", "SystemInfo", "label")

    # Define a function that parses the raw CSV file and returns an object of type LabeledDocument

    def parseDocument(line):
        values = [str(x) for x in line.split(',')]
        if (values[3] > values[2]):
            hot = 1.0
        else:
            hot = 0.0        

        textValue = str(values[4]) + " " + str(values[5])

        return LabeledDocument((values[6]), textValue, hot)

    # Load the raw HVAC.csv file, parse it using the function
    data = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    documents = data.filter(lambda s: "Date" not in s).map(parseDocument)
    training = documents.toDF()
    ````

9.  Press Shift+Enter to create a new cell, and then type the following code into the new cell.
    ````
    tokenizer = Tokenizer(inputCol="SystemInfo", outputCol="words")
    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
    lr = LogisticRegression(maxIter=10, regParam=0.01)
    pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

    model = pipeline.fit(training)
    ````

10. Press Shift+Enter to create a new cell, and then type the following code into the new cell.
    ````
    training.show()
    ````

11. On the **Cell** menu, click **Run All**.

    When the notebook has finished processing, review the results.

12. Press Shift+Enter to create a new cell, and then type the following code into the new cell.
    ````
    # SystemInfo here is a combination of system ID followed by system age
    Document = Row("id", "SystemInfo")
    test = sc.parallelize([(1L, "20 25"),
                (2L, "4 15"),
                (3L, "16 9"),
                (4L, "9 22"),
                (5L, "17 10"),
                (6L, "7 22")]) \
        .map(lambda x: Document(*x)).toDF()
    ````

13. Press Shift+Enter to create a new cell, and then type the following code into the new cell.
    ````
    # Make predictions and display the results
    prediction = model.transform(test)
    selected = prediction.select("SystemInfo", "prediction", "probability")
    for row in selected.collect():
        print row
    ````

14. On the **Cell** menu, click **Run All**.

    When the notebook has finished processing, review the results.

    > **Note**: You should normally close a notebook, on the **File** menu, click **Close and Halt**. In this lab, leave the notebook running.

**Results**: At the end of this exercise, you will have built a machine learning application with an Apache Spark cluster in HDInsight by using Jupyter Notebooks.

### Exercise 2: Use Zeppelin for interactive data analysis

#### Task 1: Launch Zeppelin and load data

1.  In the Azure Portal, on the **Cluster dashboards** blade, click **Zeppelin notebook**.

2.  If a **Windows Security** dialog box appears, in the **User name** box, type **admin**, in the **Password** box, type **Pa55w.rdPa55w.rd**, and then click **OK**.

3.  On the **Zeppelin** home page, click **Notebook**, and then click **Create new note**.

4.  In the **Create new note** dialog box, in the **Note name** box, type **HVAC-zep**, and then click **Create Note**.

#### Task 2: Load data into a temporary table

1.  In the new notebook, type the following code into the first empty paragraph.
    ````
    %livy.spark
    //The above magic instructs Zeppelin to use the Livy Scala interpreter

    // Create an RDD using the default Spark context, sc
    val hvacText = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    // Define a schema
    case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)

    // Map the values in the .csv file to the schema
    val hvac = hvacText.map(s => s.split(",")).filter(s => s(0) != "Date").map(
        s => Hvac(s(0), 
                s(1),
                s(2).toInt,
                s(3).toInt,
                s(6)
        )
    ).toDF()

    // Register as a temporary table called "hvac"
    hvac.registerTempTable("hvac")
    ````

2.  On the right side of the window, click the **Run this paragraph** button (or press Shift+Enter).

#### Task 3: Use interactive queries for data analysis

1.  In the next paragraph of the notebook, type the following SQL query, and then press Shift+Enter to execute it.
    ````
    %sql
    select buildingID, (targettemp - actualtemp) as temp_diff, date from hvac where date > "6/1/13"
    ````

2.  Click the **Bar Chart** button to display the output as a bar graph.

3.  Click the **Line Chart** button to display the output as a line graph.

4.  Click **settings**, and then in the **Keys** box, click the **X** to remove **buildingID**.

5.  Under **All fields**, drag **date** into the **Keys** box.

6.  Under **All fields**, drag **buildingID** into the **Groups** box.

7.  In the **Values** box, click **SUM**, and then click **max**.

8.  In the next paragraph of the notebook, type the following SQL query, and then press Shift+Enter to execute it.
    ````
    %sql
    select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff from hvac where targettemp > "${Temp = 65,65|68|70}"
    ````

    > **Note:** You should normally halt a notebook when you have finished using it, but for the purposes of this lab, leave this notebook running.
    > 
    > Instructions for halting a notebook (**do not run in this lab**):
    > 1.  At the upper-right of the page, click **anonymous**, and then click **Interpreter**.
    > 2.  In the entry for the Livy interpreter, click **restart**.

**Results**: At the end of this exercise, you will have used Zeppelin to interactively analyze a data file. You will also have learned how to import an external package into Zeppelin.

### Exercise 3: View and manage Spark sessions by using Livy

#### Task 1: View Livy session information by using the REST API

1.  Click **Start**, type **Windows PowerShell**, and then click **Windows PowerShell**.

2.  At the PowerShell prompt, type the following command, and then press Enter:
    ````
    $cred=Get-Credential
    ````

3.  In the **Windows PowerShell credential request** dialog box, in the **User name** box, type **admin**. In the **Password** box, type **Pa55w.rdPa55w.rd**, and then click **OK**.

4.  At the PowerShell prompt, type the following command, and then press Enter:
    ````
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster.

    Notice that the value of the **statements** property in the result set begins **{@{id=*n*...**. Note the value of ***n*** for the next step.

5.  At the PowerShell prompt, type the following command, and then press Enter to view session details:
    ````
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions/<SESSIONID>
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<SESSIONID\>** with the session ID of ***n*** that you noted in the previous step.

6.  At the PowerShell prompt, type the following command, and then press Enter to view session statement details:
    ````
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions/<SESSIONID>/statements
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<SESSIONID\>** with the session ID of ***n*** that you noted in the previous step.

7.  At the PowerShell prompt, type the following command, and then press Enter to get a clearer view of the statement history:
    ````
    (Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions/<SESSIONID>/statements).statements
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<SESSIONID\>** with the session ID of ***n*** that you noted in the previous step.

#### Task 2: Terminate Livy sessions by using the REST API

1.  At the PowerShell prompt, type the following command, and then press Enter:
    ````
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster.

    Notice that the value of the **statements** property in the result set begins **{@{id=*n*...**. Note the value of ***n*** for the next step. Also note the value of the **total** property.

2.  At the PowerShell prompt, type the following command, and then press Enter to close a session:
    ````
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/sessions/<SESSIONID> -Method DELETE
    ````

    Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<SESSIONID\>** with the session ID of ***n*** that you noted in the previous step.

3.  Repeat steps 1 and 2 of this task until there are no active Livy sessions (when the value of the **total** property returned by the command in step 1 is **0**).

4.  Close PowerShell and any open browser windows.

#### Task 3: Remove all Azure resources

1.  In the Microsoft Azure Portal, click **Resource groups**.

2.  On the **Resource groups** blade, right-click **SparkSQLrg**, and then click **Delete resource group**.

3.  On the **Are you sure you want to delete** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **SparkSQLrg**, and then click **Delete**.

4.  Wait for your resource group to be deleted, and then click **All resources**. Verify that the cluster, and the storage account that was created with your cluster, have all been removed.

**Results**: At the end of this exercise, you will have viewed and run Livy sessions through the Livy REST API.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
