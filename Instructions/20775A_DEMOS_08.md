# Module 8: Analyzing Data with Spark SQL

- [Module 8: Analyzing Data with Spark SQL](#module-8-analyzing-data-with-spark-sql)
    - [Demo 1: Joining dataframes](#demo-1-joining-dataframes)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Data setup](#data-setup)
        - [Creating an inner join](#creating-an-inner-join)
        - [Creating a left outer join](#creating-a-left-outer-join)
        - [Creating a right outer join](#creating-a-right-outer-join)
        - [Creating a full outer join](#creating-a-full-outer-join)
        - [Creating a left semi join](#creating-a-left-semi-join)
        - [Using explicit join columns](#using-explicit-join-columns)
    - [Demo 2: Using Livy to submit a Spark application](#demo-2-using-livy-to-submit-a-spark-application)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Verify that Livy is running on your HDInsight cluster](#verify-that-livy-is-running-on-your-hdinsight-cluster)
        - [Submit a batch job](#submit-a-batch-job)
        - [Query batch jobs](#query-batch-jobs)
        - [Delete a batch job](#delete-a-batch-job)

## Demo 1: Joining dataframes

### Scenario

In this demonstration, you will see the effects of various types of dataframe joins.

### Preparation

This demonstration requires the resources outlined in the “Prerequisites” topic in “Module Overview” above.

A text file containing the example code can be found at **E:\Demofiles\Mod08\dataframe_joins.txt** on the virtual machine that accompanies this course.

### Data setup

1. Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**.
3.	In the Azure Portal, click **All resources**, click your HDInsight cluster name, and then click **Cluster dashboard**.
4.	On the Cluster dashboard blade, click **Jupyter Notebook**.

    >**Note:** If an Authentication Required dialog box appears, use the username **admin** and the password that you defined for your HDInsight cluster admin account. A new browser tab will open on the Jupyter home page.

5.	On the Jupyter home page, click **New**, and then click **PySpark**.
A new browser tab will open with a new, empty notebook.
6.	Click **Untitled**, and then type **joins** to rename the notebook.
7.	In the first cell of the notebook, to create a dataframe, type the following:
    ```python
    from pyspark.sql import Row
    data1 = [('Alpha', 1), ('Bravo', 2), ('Charlie', 3)]
    df1 = sqlContext.createDataFrame(sc.parallelize(data1), ['name', 'id'])
    df1.collect()
    ```
8.	Click **Cell**, and then click **Run Cells**.
9.	Press Shift+Enter to create a new cell.
10.	In the new cell, to create a second dataframe, type the following:
    ```python
    data2 = [('Alpha', 1), ('Charlie', 3), ('Delta',4)]
    df2 = sqlContext.createDataFrame(sc.parallelize(data2), ['name', 'id'])
    df2.collect()
    ```
11.	Click **Cell**, and then click **Run Cells**.

### Creating an inner join

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,'id').collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

### Creating a left outer join

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,'id','left_outer').collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

### Creating a right outer join

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,'id','right_outer').collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

### Creating a full outer join

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,'id','outer').collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

### Creating a left semi join

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,'id','left_semi').collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

### Using explicit join columns

1.	Press Shift+Enter to create a new cell. In the new cell, type the following:
    ```python
    df1.join(df2,df1.id == df2.id).collect()
    ```
2.	Click **Cell**, and then click **Run Cells**.

## Demo 2: Using Livy to submit a Spark application

### Scenario

In this demonstration, you will see how to submit, monitor, and manage Spark batches from Livy.

### Preparation

A copy of the .jar file that is required for this demonstration is included in the files for this module at **E:\Demofiles\Mod08\SimpleSparkApp.jar**. Alternatively, you can compile the .jar file yourself by using the following link, in which case you should allow 30–45 minutes to perform this task:

-	You should have created the application developed in the article “Create a Scala Maven application to run on Apache Spark cluster on HDInsight” at https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-create-standalone-application.

-	Whether you use the precompiled .jar file or compile it yourself, upload the SimpleSparkApp.jar file to the storage account associated with your HDInsight cluster at /example/jars/SimpleSparkApp.jar.
    - In your HDInsight cluster, click Storage account, under Blob service, click Containers, upload the jar file in /example/jars.

### Verify that Livy is running on your HDInsight cluster

1.	Open a Windows PowerShell window, type the following command, and then press Enter.
    ```powershell
    $cred=Get-Credential
    ```
2.	For **User name**, type your HDInsight cluster user name, and then for **Password**, type the user’s password. 
3.	Type the following command, replacing **CLUSTERNAME** with the name of your HDInsight cluster, and then press Enter.
    ```powershell
    Invoke-RestMethod -Credential $cred -Uri https://CLUSTERNAME.azurehdinsight.net/livy/batches
    ```
    
    Livy is accessible if you do not receive an error message. 

### Submit a batch job

1.	In the PowerShell window, type the following command to create a variable that contains the POST variables with details of the batch, and then press Enter.
    ```powershell
    $json = @{ file="wasbs:///example/jars/SimpleSparkApp.jar"; className="com.microsoft.spark.example.WasbIOTest" } | ConvertTo-Json
    ```

    >**Note:** If you are using a different Spark application for this demonstration, update the values of **file** and **className** to match the location and class name of your application.
2.	Next, type the following command, replacing **CLUSTERNAME** with the name of your HDInsight cluster, and then press Enter.
    ```powershell
    Invoke-RestMethod -Credential $cred -Uri https://CLUSTERNAME.azurehdinsight.net/livy/batches -Method POST -body $json -ContentType 'application/json'
    ```

    You should see a result like the following.
    ```powershell
    id      : 0
    state   : starting
    appId   :
    appInfo : @{driverLogUrl=; sparkUiUrl=}
    log     : {}
    ```
3.	Note the value of the id property for use in the next step.

### Query batch jobs

1.	In the PowerShell window, type the following command to retrieve a list of all batch jobs, replacing **CLUSTERNAME** with the name of your HDInsight cluster, and then press Enter.
    ```powershell
    Invoke-RestMethod -Credential $cred -Uri https://CLUSTERNAME.azurehdinsight.net/livy/batches
    ```

    The result shows that one batch has been created.
2.	Type the following command to view details of a batch.
    ```powershell
    Invoke-RestMethod -Credential $cred -Uri https://CLUSTERNAME.azurehdinsight.net/livy/batches/BATCHID
    ```
    
    Replace **CLUSTERNAME** with the name of your HDInsight cluster, replace **BATCHID** with the ID of the batch created in the previous step (typically, this will be **0**), and then press Enter.

### Delete a batch job

1.	In the PowerShell window, type the following command to delete the batch job.
    ```powershell
    Invoke-RestMethod -Credential $cred -Uri https://CLUSTERNAME.azurehdinsight.net/livy/batches/BATCHID -Method DELETE
    ```

    Replace **CLUSTERNAME** with the name of your HDInsight cluster, replace **BATCHID** with the ID of the batch created in the previous step (typically, this will be **0**), and then press Enter.
2.	Close the PowerShell window.

When you delete a job, all of the output and status information that is generated by the Spark application that the job triggers is permanently removed. The application file is not affected.

>**Note:** If you prefer to use curl, and curl is available, use it instead of the PowerShell **Invoke-RestMethod** command.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.