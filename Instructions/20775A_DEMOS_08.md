# Module 8: Analyzing Data with Spark SQL

- [Module 8: Analyzing Data with Spark SQL](#module-8-analyzing-data-with-spark-sql)
    - [Demo 1: Joining dataframes](#demo-1-joining-dataframes)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Data setup](#data-setup)
        - [Creating an inner join](#creating-an-inner-join)
        - [Creating a left outer join](#creating-a-left-outer-join)
        - [Creating a right outer join](#creating-a-right-outer-join)
        - [Creating a full outer join](#creating-a-full-outer-join)
        - [Creating a left semi join](#creating-a-left-semi-join)
        - [Using explicit join columns](#using-explicit-join-columns)
    - [Demo 2: Using Livy to submit a Spark application](#demo-2-using-livy-to-submit-a-spark-application)
        - [Scenario](#scenario-1)
        - [Preparation](#preparation-1)
        - [Verify that Livy is running on your HDInsight cluster](#verify-that-livy-is-running-on-your-hdinsight-cluster)
        - [Submit a batch job](#submit-a-batch-job)
        - [Query batch jobs](#query-batch-jobs)
        - [Delete a batch job](#delete-a-batch-job)

## Demo 1: Joining dataframes

### Scenario

In this demonstration, you will see the effects of various types of dataframe joins.

### Preparation

This demonstration requires an HDInsight on Azure Spark cluster, which is created in the following steps.

1.  On the **20775A-LON-DEV** virtual machine, on the Start menu, start to type **Internet Explorer**, and then click **Internet Explorer**.
2.  In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Microsoft Learning Azure Pass subscription.
3.  In the Azure Portal, click **All resources**, and then verify that there are no existing HDInsight clusters in your subscription. 
4.  In the Azure Portal, in the left pane, click **+ Create a resource**.
5.  Click **Analytics**, and then click **HDInsight**.
6.  On the **HDInsight** blade, click **Custom (size, settings, apps)**.
7.  On the **Basics** blade, type the following details, and then click **Cluster type**:
    -  **Cluster name**: \<*your name\>\<date*\>spark
    -  **Subscription**: your subscription
8.  On the **Cluster configuration** blade, enter the following details, and then click **Select**:
    -  **Cluster type**: Spark
    -  **Operating system**: Linux
    -  **Version**: Spark 1.6.3 (HDI 3.5)
9.  On the **Basics** blade, enter the following details, and then click **Next**:
    -  **Cluster login username**: admin
    -  **Cluster login password**: Pa55w.rdPa55w.rd
    -  **Secure Shell (SSH) username**: sshuser
    -  **Use same password as cluster login**: selected
    -  **Resource group (Create new)**: SparkSQLrg
    -  **Location**: Select your region
10. On the **Security + networking** blade, click **Next**.
11. On the **Storage** blade, under **Select a Storage account**, click **Create new**.
12. In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**; point out that this name must be globally unique.
13. In the **Default container** box, replace the suggested name with the name of your cluster (for example, **\<*your name\>\<date*\>spark**).
14. Leave all other settings at their defaults, and then click **Next**.
15. On the **Applications (optional)** blade, click **Next**.
16. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **4**.
17. Click **Worker node size**.
18. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
19. Click **Head node size**.
20. On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
21. On the **Cluster size** blade, click **Next**.
22. On the **Script Actions (optional)** blade, click **Next**.
23. On the **Cluster summary** blade, point out the estimated cost per hour of this cluster. Remind students that, to avoid using up their Azure Pass allowance, it is important that they remove clusters when they are not using them.
24. On the **Cluster summary** blade, click **Create**.
25. Wait for the cluster to be provisioned before proceeding with the demonstration. This may take 15–20 minutes
26. Click **All resources**, click **\<*your name\>\<date*\>spark**, and then show the status as **Running**.



### Data setup

>**Note**: A text file containing the code used in this demo, can be found at **E:\\Demofiles\\Mod08\\dataframe_joins.txt**.

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.  In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**.
3.  In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>spark**, and then click **Cluster dashboards**.
4.  On the **Cluster dashboards** blade, click **Jupyter notebook**.
5.  If an **Windows Security** dialog box appears, use the username **admin** and the password **Pa55w.rdPa55w.rd**. A new browser tab will open on the Jupyter home page.
6.  On the Jupyter home page, click **New**, and then click **PySpark**. A new browser tab will open with a new, empty notebook.
7.  If the **404 : Not Found** page appears, complete the following steps:
    1.  Click the Jupyter page heading.
    2.  On the Jupyter home page, click **Untitled.ipynb**.
8.  Click **Untitled**, type **joins**, and then click **OK** to rename the notebook.
9.  In the first cell of the notebook, type the following to create a dataframe:
    ````python
    from pyspark.sql import Row
    data1 = [('Alpha', 1), ('Bravo', 2), ('Charlie', 3)]
    df1 = sqlContext.createDataFrame(sc.parallelize(data1), ['name', 'id'])
    df1.collect()
    ````
10. On the **Cell** menu, click **Run Cells**.
11. Press Shift+Enter to create a new cell.
12. In the new cell, type the following to create a second dataframe:
    ````python
    data2 = [('Alpha', 1), ('Charlie', 3), ('Delta',4)]
    df2 = sqlContext.createDataFrame(sc.parallelize(data2), ['name', 'id'])
    df2.collect()
    ````
13. On the **Cell** menu, click **Run Cells**.

### Creating an inner join

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,'id').collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

### Creating a left outer join

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,'id','left_outer').collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

### Creating a right outer join

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,'id','right_outer').collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

### Creating a full outer join

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,'id','outer').collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

### Creating a left semi join

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,'id','left_semi').collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

### Using explicit join columns

1.  Press Shift+Enter to create a new cell.
2.  In the new cell, type the following:
    ````python
    df1.join(df2,df1.id == df2.id).collect()
    ````
3.  On the **Cell** menu, click **Run Cells**.

## Demo 2: Using Livy to submit a Spark application

### Scenario

In this demonstration, you will see how to submit, monitor, and manage Spark batches from Livy.

### Preparation

A copy of the .jar file that is required for this demonstration is included in the files for this module at **E:\Demofiles\Mod08\SimpleSparkApp.jar**. Alternatively, you can compile the .jar file yourself by using the following link, in which case you should allow 30–45 minutes to perform this task:

-  You should have created the application developed in the article “Create a Scala Maven application to run on Apache Spark cluster on HDInsight” at https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-spark-create-standalone-application.

-  Whether you use the precompiled .jar file or compile it yourself, upload the **SimpleSparkApp.jar** file to the storage account associated with your HDInsight cluster at **/example/jars/SimpleSparkApp.jar**.
    1.  In the Azure Portal, click **All resources**, and then click **\<*your name\>\<date*\>sa**.
    2.  Under **Blob service**, click **Blobs**, and then click **\<*your name\>\<date*\>spark**.
    3.  Navigate to **/example/jars**, and then upload the **SimpleSparkApp.jar** file.

### Verify that Livy is running on your HDInsight cluster

1.  Open a Windows PowerShell window, type the following command, and then press Enter.
    ````powershell
    $cred=Get-Credential
    ````
2.  In the **Windows PowerShell credential request** dialog box, for **User name**, type **admin**, for **Password**, type **Pa55w.rdPa55w.rd**, and then click **OK**. 
3.  Type the following command, replacing **\<CLUSTERNAME\>** with the name of your HDInsight cluster, and then press Enter.
    ````powershell
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/batches
    ````
    
    Livy is accessible if you do not receive an error message. 

### Submit a batch job

1.  In the PowerShell window, type the following command to create a variable that contains the POST variables with details of the batch, and then press Enter.
    ````powershell
    $json = @{ file="wasbs:///example/jars/SimpleSparkApp.jar"; className="com.microsoft.spark.example.WasbIOTest" } | ConvertTo-Json
    ````

    >**Note**: If you are using a different Spark application for this demonstration, update the values of **file** and **className** to match the location and class name of your application.
2.  In the PowerShell window, type the following command, replacing **\<CLUSTERNAME\>** with the name of your HDInsight cluster, and then press Enter.
    ````powershell
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/batches -Method POST -body $json -ContentType 'application/json'
    ````

    You should see a result like the following.
    ````powershell
    id      : 0
    state   : starting
    appId   :
    appInfo : @{driverLogUrl=; sparkUiUrl=}
    log     : {}
    ````
3.  Note the value of the id property for use in the next step.

### Query batch jobs

1.  In the PowerShell window, type the following command to retrieve a list of all batch jobs, replacing **\<CLUSTERNAME\>** with the name of your HDInsight cluster, and then press Enter.
    ````powershell
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/batches
    ````

    The result shows that one batch has been created.
2.  In the PowerShell window, type the following command to view details of a batch. Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<BATCHID\>** with the ID of the batch created in the previous step (typically, this will be **0**), and then press Enter.
    ````powershell
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/batches/<BATCHID>
    ````
    
    

### Delete a batch job

1.  In the PowerShell window, type the following command to delete the batch job. Replace **\<CLUSTERNAME\>** with the name of your HDInsight cluster, replace **\<BATCHID\>** with the ID of the batch created in the previous step (typically, this will be **0**), and then press Enter.
    ````powershell
    Invoke-RestMethod -Credential $cred -Uri https://<CLUSTERNAME>.azurehdinsight.net/livy/batches/<BATCHID> -Method DELETE
    ````

    When you delete a job, all of the output and status information that is generated by the Spark application that the job triggers is permanently removed. The application file is not affected.
    
2.  Close the PowerShell window.

>**Note**: If you prefer to use curl, and curl is available, use it instead of the PowerShell **Invoke-RestMethod** command.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
