# Module 9: Analyze Data with Hive and Phoenix

## Lab: Analyze data with Hive and Phoenix

### Exercise 1: Implement interactive queries for big data with Interactive Hive

#### Task 1: Prepare the lab environment

1.  Ensure that the **MT17B-WS2016-NAT**, and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**. 

2.  In Internet Explorer, log in to the Azure Portal at **https://portal.azure.com**.  

3.  In the Azure Portal, in the left pane, click **+ Create a resource**. 

4.  Click **Analytics**, and then click **HDInsight**. 

5.  On the **HDInsight** blade, click **Custom (size, settings, apps)**.

6.  On the **Basics** blade, click **Cluster type**.

7.  On the **Cluster configuration** blade, enter the following settings, and then click **Select**:

    -   **Cluster type**: Interactive Hive

    -   **Operating system**: Linux

    -   **Version**: Interactive Hive 2.1.0 (HDI 3.6)

8.  On the **Basics** blade, enter the following settings, and then click **Next**:

    -   **Cluster name**: \<*your name*\>hivellap

    -   **Subscription**: your subscription

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rd123

    -   **Secure Shell (SSH) username**: sshuser

    -   **Use same password as cluster login**: selected

    -   **Resource group (Create new)**: hivellaptrainrg

    -   **Location**: your location

9.  On the **Security + networking** blade, click **Next**.

10. On the **Storage** blade, enter the following settings, and then click **Next**:

    -   **Select a Storage account** (**Create new)**: hivellaptrainstg

    -   **Default container**: \<*your name*\>hivellap-ctr

11. On the **Applications** blade, you can install popular data science tools such as H20 or Dataiku or any other apps; however, for Interactive Hive Cluster, no applications are available. Click **Next**.

12. On the **Cluster size** blade, enter the following settings, and then click **Next**:

    -   **Number of Worker nodes**: 2

    -   **Worker node size (View all)**: D13 v2 (2 nodes, 16 cores)

    -   **Head node size (View all)**: D13 (2 nodes, 16 cores)

    -   **Zookeeper node sizes**: default

13. On the **Script actions** blade, click **Next**.

14. On the **Cluster summary** blade, review the summary section, and then click **Create**.

15. The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned.

16. In the Azure Portal, click **All resources**, click **\<*your name\>\<date*\>hdi**, and then ensure the status shows as **Running**.

17. Click **Start**, type **Putty**, and then press Enter.

18. In the **PuTTY Configuration** dialog box, in the **Hostname (or IP address)** box, type **sshuser@\<clustername\>-ssh.azurehdinsight.net**, and then click **Open**.

    Replace **\<clustername\>** with the name of your cluster.

19. In the **PuTTY Security Alert** dialog box, click **Yes**.

20. At the PuTTY prompt, type **Pa55w.rd123**, and then press Enter.

21. At the PuTTY prompt, type the following command, and then press Enter:
    ````
    beeline -u 'jdbc:hive2://localhost:10001/;transportMode=http' -n admin
    ````

22. Click **Start**, type **Internet Explorer**, and then press Enter.

23. In Internet Explorer, navigate to **http://www.microsoft.com/en-us/download/details.aspx?id=40886**, and then click **Download**.

24. On the **Choose the download you want** page, select the **HiveODBC32.msi** check box, and then click **Next**.

25. In the message box, click **Run**.

26. In the **Microsoft Hive ODBC Driver Setup** dialog box, on the **Welcome to the** **Microsoft Hive ODBC Driver Setup Wizard** page, click **Next**.

27. On the **End-User License Agreement** page, select the **I accept the terms in the License Agreement** check box, and then click **Next**.

28. On the **Destination Folder** page, click **Next**.

29. On the **Ready to install Microsoft Hive ODBC Driver** page, click **Install**.

30. In the **User Account Control** dialog box, click **Yes**.

31. On the **Completed the Microsoft Hive ODBC Driver Setup Wizard** page, click **Finish**.

32. Repeat steps 23 to 31 to install the **HiveODBC64.msi** driver.

33. Close Internet Explorer.

#### Task 2: Import HVAC data into Hive

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise01**, right-click **upload_hvac_into_hive_table.hql**, and then click **Open with**.

2.  In the **How do you want to open this file?** dialog box, click **More apps**, click **Notepad**, and then click **OK**.

3.  Review all the steps to be created.

4.  Switch to the PuTTY console.

5.  At the PuTTY prompt, type the following command, and then press Enter:
    ````
    USE default;
    DROP TABLE IF EXISTS hvacdata;
    ````

6.  At the PuTTY prompt, type the following command, and then press Enter:
    ````
    CREATE EXTERNAL TABLE hvacdata (
    `Date` string, 
    `Time` string, 
    `TargetTemp` string, 
    `ActualTemp` string, 
    `System` string, 
    `SystemAge` string, 
    `BuildingID` string
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ', '
    STORED AS TEXTFILE LOCATION 'wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/'
    tblproperties("skip.header.line.count"="1"); 
    SELECT COUNT(*) FROM hvacdata;
    SELECT * FROM hvacdata LIMIT 10;
    ````

#### Task 3: Connect Excel to Interactive Hive Cluster

1.  Click **Start**, type **Open ODBC Data Sources (32-bit)**, and then press Enter.

2.  In the **ODBC Data Sources Administrator (32-bit)** dialog box, click **Add**.

3.  In the **Create New Data Source** dialog box, click **Microsoft Hive ODBC Driver**, and then click **Finish**.

4.  In the **Microsoft Hive ODBC Driver DSN Setup** dialog box, enter the following values:

    -   **Data Source Name**: Sample Microsoft Hive DSN

    -   **Description**: Sample Microsoft Hive DSN

    -   **Host(s)**: \<your cluster name\>.azurehdinsight.net

    -   **Port**: 443

    -   **Database**: default

    -   **Mechanism**: Windows Azure HDInsight Service

    -   **Realm**: disabled

    -   **Host FQDN**: disabled

    -   **Service Name**: disabled

    -   **User Name**: admin

    -   **Password**: Pa55w.rd123

    -   **Save Password (Encrypted)**: selected

    -   **Delegation UID**: blank

    -   **Thrift Transport**: HTTP

5.  Click **HTTP Options**.

6.  In the **HTTP Properties** dialog box, in the **HTTP Path** box, type **/hive2**, and then click **OK**.

7.  Click **SSL Options**.

8.  In the **SSL Options** dialog box, ensure that the **Enable SSL** check box is selected, and then click **OK**.

9.  In the **Microsoft Hive ODBC Driver DSN Setup** dialog box, click **Test**.

10. In the **Test Results** dialog box, you should see the **Successfully connected to data source** message, and then click **OK**.

11. In the **Microsoft Hive ODBC Driver DSN Setup** dialog box, click **OK**.

12. In the **ODBC Data Sources Administrator (32-bit)** dialog box, click **OK**.

13. Click **Start**, type **Excel**, and then press Enter.

14. In Excel 2016, click **Blank workbook**.

15. On the **Data** ribbon, click **Get External Data**, click **From Other Sources**, and then click **From Data Connection Wizard**.

16. In the **Data Connection Wizard** dialog box, on the **Welcome to the Data Connection Wizard** page, click **ODBC DSN**, and then click **Next**.

17. On the **Connect to ODBC Data Source** page, click **Sample Microsoft Hive DSN**, and then click **Next**.

18. On the **Select Database and Table** page, in the **Select the database that contains the data you want** list, click **HIVE**.

19. In the **Connect to a specific table** section, click **hvacdata**, and then click **Next**.

20. On the **Save Data Connection File and Finish** page, give a name and description to the connection, and then click **Finish**.

21. In the **Import Data** dialog box, click **OK**.

22. Your data will be available in Excel to carry out further analysis.

23. Compare your Excel file with the Solution file in the **E:\\Labfiles\\Lab09\\Solution\\Exercise01** folder.

24. Close Excel 2016 without saving any changes.

**Results**: After this exercise, you will have a good understanding of how to create an HBase cluster, connect to HBase Cluster via SSH, and use Phoenix to ingest data and perform interactive queries.

### Exercise 2: Perform exploratory data analysis by using Hive

#### Task 1: Ingest raw data into Hive

1.  On the **Start** menu, type **Internet Explorer**, and then click **Internet Explorer**.

2.  In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.

3.  Click **All resources**, and then click **\<*your name*\>hivellap**.

4.  In the **Cluster dashboards** section, click **Zeppelin notebook**.

5.  In the **Windows Security** dialog box, in the **User name** box, type **admin**, in the **Password** box, type **Pa55w.rd123**, and then click **OK**.

6.  On the header pane of the notebook, click **Notebook**, and then click **Create new note**.

7.  In the **Create new note** dialog box, in the text box, type **HVAC Analysis Report**, and then click **Create Note**.

8.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise02**, and then double-click **upload_hvac_buildings_into_hive.hql**.

9.  Review all the steps to be created before writing the HiveQL into Zeppelin Notebook.

10. Switch to Internet Explorer.

11. For **Step1**, in the new cell, type the following to switch to the default schema to create tables and ingest, and then press Shift+Enter:
    ````
    %jdbc(hive)
    USE default;
    ````

12. For **Step2**, in the new cell, type the following to switch to drop the **hvacdata** table, if it already exists in the default schema, and then press Shift+Enter:
    ````
    %jdbc(hive)
    DROP TABLE IF EXISTS hvacdata;
    ````

13. For **Step3**, in the new cell, type the following to switch to impose a structure on top of the **HVAC.csv** file using the **CREATE EXTERNAL TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    CREATE EXTERNAL TABLE hvacdata (
    `Date` string, 
    `Time` string, 
    `TargetTemp` string, 
    `ActualTemp` string, 
    `System` string, 
    `SystemAge` string, 
    `BuildingID` string
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ', '
    STORED AS TEXTFILE LOCATION 'wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/'
    tblproperties("skip.header.line.count"="1");
    ````

14. For **Step4**, in the new cell, type the following to drop the **buildings** table if it already exists in the **default** schema, and then press Shift+Enter:
    ````
    %jdbc(hive)
    DROP TABLE IF EXISTS buildings;
    ````

15. For **Step5**, in the new cell, type the following to impose a structure on top of the **buildings.csv** file using the **CREATE EXTERNAL TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    CREATE EXTERNAL TABLE buildings (
    BuildingID integer,
    BuildingMgr string,
    BuildingAge integer,
    HVACproduct string,
    Country string
    )
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ', '
    STORED AS TEXTFILE LOCATION 'wasbs:///HdiSamples/HdiSamples/SensorSampleData/building/'
    tblproperties("skip.header.line.count"="1");
    ````

#### Task 2: Create ORC tables

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise02**, and then double-click **create_orc_formatted_tables.hql**.

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook.

2.  Switch to Internet Explorer.

3.  For **Step1**, in the new cell, type the following to switch to the default schema to create tables and ingest, and then press Shift+Enter:
    ````
    %jdbc(hive)
    USE default;
    ````

4.  For **Step2**, in the new cell, type the following to drop the **hvacdataorc** table, if it already exists in the default schema, and then press Shift+Enter:
    ````
    %jdbc(hive)
    DROP TABLE IF EXISTS hvacdataorc;
    ````

5.  For **Step3**, in the new cell, type the following to create an internal table named **hvacdataorc** using **CREATE TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    CREATE TABLE hvacdataorc (
    `Date` string, 
    `Time` string, 
    TargetTemp integer, 
    ActualTemp integer, 
    System string, 
    SystemAge integer, 
    BuildingID integer
    )
    STORED AS ORC;
    ````

6.  For **Step4**, in the new cell, type the following to populate **hvacdataorc table** from **hvacdata** using the **INSERT OVERWRITE TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    INSERT OVERWRITE TABLE hvacdataorc
    SELECT  `Date`, 
            `Time`, 
            TargetTemp, 
            ActualTemp, 
            System, 
            SystemAge, 
            BuildingID 
    FROM hvacdata;
    ````

7.  For **Step5**, in the new cell, type the following to drop the **buildingsorc** table if it already exists in the **default** schema, and then press Shift+Enter:
    ````
    %jdbc(hive)
    DROP TABLE IF EXISTS buildingsorc;
    ````

8.  For **Step6**, in the new cell, type the following to create an internal table named **buildingsorc** using the **CREATE TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    CREATE TABLE buildingsorc (
    BuildingID integer,
    BuildingMgr string,
    BuildingAge integer,
    HVACproduct string,
    Country string
    )
    STORED AS ORC;
    ````

9.  For **Step7**, in the new cell, type the following to populate **buildingsorc table** from **buildings** using the **INSERT OVERWRITE TABLE** statement, and then press Shift+Enter:
    ````
    %jdbc(hive)
    INSERT OVERWRITE TABLE buildingsorc
    SELECT BuildingID,
        BuildingMgr,
        BuildingAge,
        HVACproduct,
        Country
    FROM buildings;
    ````

#### Task 3: Enrich data

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise02**, and then double-click **refine_hvac_buildings.hql**.

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook.

2.  Switch to Internet Explorer.

3.  For **Step1**, in the new cell, type the following to switch to the default schema to create tables and drop the **hvac\_temperatures** table, if it already exists, and then press Shift+Enter:
    ````
    %jdbc(hive)
    USE default;
    DROP TABLE IF EXISTS hvac_temperatures;
    ````

4.  For **Step2**, in the new cell, type the following to create a **hvac_temperatures** table by retaining all the columns from **hvacdataorc** table and adding three new columns (Temp_Variance, tempboundary and worsetemp) as per the following business rules, and then press Shift+Enter:

    -   **Temp_Variance**: difference between the TargetTemp column and the ActualTemp

    -   **tempboundary**: if the difference between the TargetTemp column and the ActualTemp is:

        -   greater than 4 then set the value to 'HOT'

        -   less than -4 then set the value to 'COLD'

        -   Otherwise Normal

    -   **worsetemp**: if the difference between the TargetTemp column and the ActualTemp is:

        -   greater than 4 then set the value to '1'

        -   less than -4 then set the value to '1'

        -   Otherwise '0'
    ````
    %jdbc(hive)
    CREATE TABLE hvac_temperatures
    STORED AS ORC
    AS
    select a.*, 
        (a.TargetTemp - a.ActualTemp) as Temp_Variance,
        case 
            when (a.TargetTemp - a.ActualTemp) > '4' then 'COLD'
            when (a.TargetTemp - a.ActualTemp) < '-4' then 'HOT'
            else 'NORMAL'
        end as tempboundary,
        case 
            when (a.TargetTemp - a.ActualTemp) > '4' then '1'
            when (a.TargetTemp - a.ActualTemp) < '-4' then '1'
            else '0'
        end as worsetemp
    from hvacdataorc a;
    ````

5.  For **Step3**, in the new cell, type the following to drop the **hvac_buildings** table if it already exists in the **default** schema and create the **hvac_buildings table** by joining the **buildings** table with the **hvac_temperatures table** created in the **Step2**, and then press Shift+Enter:
    ````
    %jdbc(hive)
    DROP TABLE IF EXISTS hvac_buildings;
    CREATE TABLE hvac_buildings
    STORED AS ORC
    AS
    select h.*, 
        b.Country, 
        b.HVACproduct, 
        b.BuildingAge, 
        b.BuildingMgr
    from buildings b 
    join 
    hvac_temperatures h 
    on b.buildingid = h.buildingid;
    ````

#### Task 4: Analyze and visualize data

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise02**, and then double-click **exploratory_data_analysis_with_hive.hql**.

    Review all the steps to be created before writing the HiveQL into Zeppelin Notebook.

2.  Switch to Internet Explorer.

3.  For **Step1**, in the new cell, type the following to perform basic data profiling on the **hvac_buildings** table, and then press Shift+Enter:

    -   Count the number of rows

    -   Preview a few rows

    -   Fetch the number of countries using HVAC products

    -   Count the number of countries using HVAC products
    ````
    %jdbc(hive)
    select count(*) from hvac_buildings;
    select * from hvac_buildings LIMIT 10;
    select distinct country from hvac_buildings;
    select count(distinct country) from hvac_buildings;
    ````

4.  For **Step2**, in the new cell, type the following to determine the number of buildings and the number of HVAC products in each country. Use the following columns (country, BuildingId and HVACProduct) to write the query, and then press Shift+Enter:
    ````
    %jdbc(hive)
    select country, 
        count(BuildingID) as no_of_buildings,
        count(HVACProduct) as no_of_hvacproducts
    from hvac_buildings 
    group by country
    order by no_of_buildings desc;
    ````

5.  For **Step3**, in the new cell, type the following to determine the temperature boundaries for various countries over a period (time series type view). Use the following columns (date, country and tempboundary) to write the query, and then press Shift+Enter:
    ````
    %jdbc(hive)
    select `date`,
        country, 
        tempboundary 
    from hvac_buildings
    order by `date`,
            country;
    ````

6.  For **Step4**, in the new cell, type the following to determine the worse temperatures for various countries over a period (time series type view). Use the following columns (date, country and worsetemp) to write the query, and then press Shift+Enter:
    ````
    %jdbc(hive)
    select `date`,
        country, 
        worsetemp 
    from hvac_buildings
    where worsetemp='1'
    order by `date`,
            country,  
            worsetemp;
    ````

7.  For **Step5**, in the new cell, type the following to identify the HVAC products that need to be upgraded. Use the following columns (HVACProduct, worsetemp) to write the query, and then press Shift+Enter:
    ````
    %jdbc(hive)
    select HVACproduct, 
        worsetemp 
    from hvac_buildings
    where worsetemp = '1'
    order by worsetemp desc;
    ````

8.  Close the Zeppelin tab.

> **Note**: To see the end to end result, refer to the **E:\\Labfiles\\Lab09\\Solution\\Exercise02\\HVACAnalysis.json**. You will need to import this notebook into Zeppelin.

**Results**: You have now carried out interactive queries using Interactive Hive and compared your achieved results with the expected outcomes.

### Exercise 3: Perform interactive processing by using Apache Phoenix

#### Task 1: Prepare the lab environment

1.  In the Azure Portal, in the left pane, click **+ Create a resource**. 

2.  Click **Analytics**, and then click **HDInsight**. 

3.  On the **HDInsight** blade, click **Custom (size, settings, apps)**.

4.  On the **Basics** blade, click **Cluster type**.

5.  On the **Cluster configuration** blade, enter the following settings, and then click **Select**:

    -   **Cluster type**: HBase

    -   **Operating system**: Linux

    -   **Version**: HBase 1.1.2 (HDI 3.6)

6.  On the **Basics** blade, enter the following settings, and then click **Next**:

    -   **Cluster name**: \<*your name*\>hbasetrain

    -   **Subscription**: your subscription

    -   **Cluster login username**: admin

    -   **Cluster login password**: Pa55w.rd123

    -   **Secure Shell (SSH) username**: sshuser

    -   **Use same password as cluster login**: selected

    -   **Resource group (Create new)**: hbasetrainrg

    -   **Location**: your location

7.  On the **Security + networking** blade, click **Next**.

8.  On the **Storage** blade, enter the following settings, and then click **Next**:

    -   **Select a Storage account (Create new)**: hbasetrainstg

    -   **Default container**: \<*your name*\>hbasetrain-ctr

9.  On the **Applications** blade, you can install popular data science tools such as H20 or Dataiku or any other apps; however, for Interactive Hive Cluster, no applications are available. Click **Next**.

10. On the **Cluster size blade**, enter the following settings, and then click **Next**:

    -   **Number of Region nodes**: 2

    -   **Region node size (View all)**: A3 (2 nodes, 8 cores)

    -   **Head node size (View all)**: A3 (2 nodes, 8 cores)

    -   **Zookeeper node sizes**: default

11. On the **Script actions** blade, click **Next**.

12. On the **Cluster summary** blade, review the summary section, and then click **Create**.

13. The deployment might take 20-30 minutes to complete. Wait for the cluster to be provisioned.

14. In the Azure Portal, click **All resources**, click **\<*your name*\>hbasetrain**, and then ensure the status shows as **Running**.

15. Click **Start**, type **Putty**, and then press Enter.

16. In the **PuTTY Configuration** dialog box, in the **Hostname (or IP address)** box, type **sshuser@\<clustername\>-ssh.azurehdinsight.net**, and then click **Open**.

    Replace \<clustername\> with the name of your cluster.

17. In the **PuTTY Security Alert** dialog box, click **Yes**.

18. At the PuTTY prompt, type **Pa55w.rd123**, and then press Enter.

19. At the PuTTY prompt, type the following command to install **jq** (json parser), and then press Enter:
    ````
    sudo apt install jq
    ````

20. At the PuTTY prompt, type **y**, and then press Enter.

21. At the PuTTY prompt, type the following command to install the **dos2unix** package, and then press Enter:
    ````
    sudo apt install dos2unix
    ````

22. In Internet Explorer, on the **\<*your name*\>hbasetrain** blade, in the **Cluster dashboards** section, click **Ambari home**.

23. In the **Windows Security** dialog box, in the **User name** box, type **admin**, and in the **Password** box, type **Pa55w.rd123**, and then click **OK**.

24. In Ambari, on the left side, click **ZooKeeper**, and then click any one of the three zookeeper node names that appear.

25. In the detail page of zookeeper node, in the **Summary** section, make a note of the **Hostname** for use in subsequent steps.

26. Click **Start**, type **Microsoft Azure Storage Explorer**, and then press Enter.

27. If the **Connect to Azure Storage** dialog box appears, complete the following steps:
    1.  Click **Add an Azure Account**, and then click **Sign in**.
    2.  In the **Sign in** dialog box, enter your Azure credentials, and then click **Sign in**.

28. In Microsoft Azure Storage Explorer, under **Storage Accounts**, expand **hbasetrainstg**, expand **Blob Containers**, and then click **\<*your name*\>hbasetrain-ctr**.

29. In the top menu, click **Upload**, and then click **Upload Folder**.

30. In the **Upload folder** dialog box, click the ellipsis (**...**).

31. In the **Select folder to upload** dialog box, navigate to **E:\\Labfiles**, click the **Lab09** folder, and then click **Select Folder**.

32. In the **Upload folder** dialog box, click **Upload**. Some files will not upload-this is OK.

33. Switch to PuTTY.

34. At the PuTTY prompt, type the following command to copy the **Lab09** folder contents from HDFS to the local file system on the head node of the cluster, and then press Enter:
    ````
    cd $HOME
    mkdir Lab09;hdfs dfs -copyToLocal /Lab09/* $HOME/Lab09
    ````

35. At the PuTTY prompt, type the following command to copy the **HVAC.csv** from the HDFS location to the local filesystem to **HOME/Lab09/Starter/Exercise03**, and then press Enter:
    ````
    hdfs dfs -copyToLocal /HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv $HOME/Lab09/Starter/Exercise03
    ````

36. At the PuTTY prompt, type the following command to copy the **building.csv** from the HDFS location to the local filesystem to **HOME/Lab09/Starter/Exercise03**, and then press Enter:
    ````
    hdfs dfs -copyToLocal /HdiSamples/HdiSamples/SensorSampleData/building/building.csv $HOME/Lab09/Starter/Exercise03
    ````

37. At the PuTTY prompt, type the following command to copy the **HVAC.csv** from the HDFS location to the local filesystem to **HOME/Lab09/Solution/Exercise03**, and then press Enter:
    ````
    hdfs dfs -copyToLocal /HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv $HOME/Lab09/Solution/Exercise03
    ````

38. At the PuTTY prompt, type the following command to copy the **building.csv** from the HDFS location to the local filesystem to **HOME/Lab09/Solution/Exercise03**, and then press Enter:
    ````
    hdfs dfs -copyToLocal /HdiSamples/HdiSamples/SensorSampleData/building/building.csv $HOME/Lab09/Solution/Exercise03
    ````

39. At the PuTTY prompt, type the following command to convert files into unix format, and then press Enter:
    ````
    dos2unix $HOME/Lab09/Starter/Exercise03/*.*
    dos2unix $HOME/Lab09/Solution/Exercise03/*.*
    ````

#### Task 2: Ingest data into Phoenix

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise03**, right-click **create_hvac_buildings_tables.sql** and then click **Edit**.

2.  In Notepad, review all the steps to be created.

3.  Switch to PuTTY.

4.  At the PuTTY prompt, type the following command to get the Horton Works Data Platform version (HDP_VERSION), and then press Enter:
    ````
    /usr/bin/hdp-select status|grep phoenix|head -1|cut -d " " -f3
    ````

    You need the **HDP_VERSION** in the following step and throughout this demo and lab.

5.  At the PuTTY prompt, type the following command to launch the SQLLine console, and then press Enter:
    ````
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/sqlline.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure
    ````

    Replace **\<HDP_VERSION\>** and **\<ZOOKEEPER_NAME\>** from the previous lab task.

6.  For **Step1**, at the PuTTY prompt, type the following to drop the **HVAC** table, if it already exists, and then press Enter:
    ````
    DROP TABLE IF EXISTS HVAC;
    ````

7.  For **Step2**, at the PuTTY prompt, type the following to create the **HVAC** table, and then press Enter:
    ````
    CREATE TABLE HVAC 
    (
    DATE VARCHAR(20), 
    TIME VARCHAR(10), 
    TARGETTEMP INTEGER, 
    ACTUALTEMP INTEGER, 
    SYSTEM VARCHAR(20), 
    SYSTEMAGE INTEGER, 
    BUILDINGID INTEGER not null
    CONSTRAINT HEAT_PK PRIMARY KEY(DATE,TIME,SYSTEM,BUILDINGID)
    );
    ````

8.  For **Step3**, at the PuTTY prompt, type the following to drop the **buildings** table, if it already exists, and then press Enter:
    ````
    DROP TABLE IF EXISTS buildings;
    ````

9.  For **Step4**, at the PuTTY prompt, type the following to drop the **buildings** table, if it already exists, and then press Enter:
    ````
    CREATE TABLE BUILDINGS (
    BUILDINGID INTEGER,
    BUILDINGMGR VARCHAR(10),
    BUILDINGAGE INTEGER,
    HVACPRODUCT VARCHAR(20),
    COUNTRY VARCHAR(30)
    CONSTRAINT BUILD_PK PRIMARY KEY(BUILDINGID)
    );
    ````

10. At the PuTTY prompt, type the following to quit the SQLLine console, and then press Enter:
    ````
    !quit
    ````

11. At the PuTTY prompt, type the following to load the **HVAC** and **BUILDINGS** tables in single-thread mode, and then press Enter:
    ````
    cd $HOME/Lab09/Starter/Exercise03
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py -t HVAC -h DATE,TIME,TARGETTEMP,ACTUALTEMP,SYSTEM,SYSTEMAGE,BUILDINGID <ZOOKEEPER_NAME>:2181:/hbase-unsecure HVAC.csv
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/psql.py -t BUILDINGS -h BUILDINGID,BUILDINGMGR,BUILDINGAGE,HVACPRODUCT,COUNTRY <ZOOKEEPER_NAME>:2181:/hbase-unsecure building.csv
    ````

    Replace both instances of **\<HDP\_VERSION\>** and **\<ZOOKEEPER\_NAME\>** from the previous lab task.

12. At the PuTTY prompt, type the following command to launch SQLLine console, and then press Enter:
    ````
    python /usr/hdp/<HDP_VERSION>/phoenix/bin/sqlline.py <ZOOKEEPER_NAME>:2181:/hbase-unsecure
    ````

    Replace **\<HDP\_VERSION\>** and **\<ZOOKEEPER\_NAME\>** from the previous lab task.

13. Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise03**, right-click **verify_bulk_ingests.sql**, and then click **Edit**.

14. In Notepad, review all the steps to be created.

15. Switch to PuTTY.

16. For **Step1**, at the PuTTY prompt, type the following to count the number of records in the table, and then press Enter:
    ````
    SELECT COUNT(*) FROM HVAC;
    SELECT COUNT(*) FROM BUILDINGS;
    ````

17. For **Step2**, at the PuTTY prompt, type the following to fetch 10 records from each table, and then press Enter:
    ````
    SELECT * FROM HVAC LIMIT 10;
    SELECT * FROM BUILDINGS LIMIT 10;
    ````

> **Note**: To refer to the results, open the **E:\\Labfiles\\Lab09\\Solution\\Exercise03\\Lab09-Exercise03-Task02-Results.txt** file.

#### Task 3: Perform interactive queries

1.  Using File Explorer, navigate to **E:\\Labfiles\\Lab09\\Starter\\Exercise03**, right-click **query_hvac_buildings.hql**, and then click **Edit**.

2.  In Notepad, review all the steps to be created.

3.  Switch to PuTTY, which is still running a SQLLine console.

4.  For **Step1a**, at the PuTTY prompt, type the following to fetch the number of countries using HVAC products, and then press Enter:
    ````
    SELECT DISTINCT COUNTRY FROM BUILDINGS;
    ````

5.  For **Step1b**, at the PuTTY prompt, type the following to count the number of countries using HVAC products, and then press Enter:
    ````
    SELECT COUNT(DISTINCT COUNTRY) FROM BUILDINGS;
    ````

6.  For **Step2**, at the PuTTY prompt, type the following to determine the temperatures and other columns for building number 17 on '06/09/13', and then press Enter:
    ````
    SELECT *
    FROM HVAC
    WHERE BUILDINGID=17 AND DATE='6/9/13';
    ````

7.  For **Step3**, at the PuTTY prompt, type the following to determine the maximum and minimum target, and the Actual temperatures, and then press Enter:
    ````
    SELECT min(TARGETTEMP) AS MIN_TARGETTEMP,
    max(TARGETTEMP) AS MAX_TARGETTEMP,
    min(ACTUALTEMP) AS MIN_ACTUALTEMP,
    max(ACTUALTEMP) AS MAX_ACTUALTEMP
    FROM HVAC;
    ````

8.  For **Step4**, at the PuTTY prompt, type the following to identify the countries where the Actual temperature reached a maximum of 80 degrees on 6/11/13. Use the HVAC and Buildings tables write this query, and then press Enter:
    ````
    SELECT B.COUNTRY, A.DATE, MAX(A.ACTUALTEMP)
    FROM HVAC A
    INNER JOIN
    BUILDINGS B
    ON A.BUILDINGID = B.BUILDINGID
    WHERE A.DATE = '6/11/13'
    GROUP BY B.COUNTRY,
            A.DATE
    HAVING MAX(A.ACTUALTEMP)=80;
    ````

9.  For **Step5**, at the PuTTY prompt, type the following to identify the countries where Actual temperature reached minimum of 55 on 6/11/13. Use the HVAC and buildings tables to write this query, and then press Enter:
    ````
    SELECT B.COUNTRY, A.DATE, MIN(A.ACTUALTEMP)
    FROM HVAC A
    INNER JOIN
    BUILDINGS B
    ON A.BUILDINGID = B.BUILDINGID
    WHERE A.DATE = '6/11/13'
    GROUP BY B.COUNTRY,
            A.DATE
    HAVING MIN(A.ACTUALTEMP)=55;
    ````

> **Note**: To refer to the results, open the **E:\\Labfiles\\Lab09\\Solution\\Exercise03\\Lab09-Exercise03-Task03-Results.txt** file.

10. Close all open windows, except Internet Explorer.

#### Task 4: Remove all Azure resources

1.  In the Microsoft Azure Portal, click **Resource groups**.

2.  On the **Resource groups** blade, right-click **hbasetrainrg**, and then click **Delete resource group**.

3.  On the **Are you sure you want to delete** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **hbasetrainrg**, and then click **Delete**.

4.  On the **Resource groups** blade, right-click **hivellaptrainrg**, and then click **Delete resource group**.

5.  On the **Are you sure you want to delete** blade, in the **TYPE THE RESOURCE GROUP NAME** box, type **hivellaptrainrg**, and then click **Delete**.

6.  Wait for your resource group to be deleted, and then click **All resources**. Verify that the cluster, and the storage account that was created with your cluster, have all been removed.

**Results**: After this exercise, you will have a good understanding of how to create an HBase cluster, connect to HBase Cluster via SSH, and use Phoenix to ingest data and perform interactive queries.

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
