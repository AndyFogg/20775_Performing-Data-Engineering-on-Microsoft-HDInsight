# Module 4: Loading Data into HDInsight

- [Module 4: Loading Data into HDInsight](#module-4-loading-data-into-hdinsight)
    - [Demo 1: Azure Storage Explorer](#demo-1-azure-storage-explorer)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Familiarize yourself with Azure Storage Explorer](#familiarize-yourself-with-azure-storage-explorer)
    - [Demo 2: Create Data Lake storage and add it to an HDInsight cluster](#demo-2-create-data-lake-storage-and-add-it-to-an-hdinsight-cluster)
        - [Scenario](#scenario-1)
        - [Preparation](#preparation-1)
        - [Create a Data Lake storage account](#create-a-data-lake-storage-account)
        - [Create an HDInsight cluster with access to Data Lake storage](#create-an-hdinsight-cluster-with-access-to-data-lake-storage)
    - [Demo 3: Managing storage using the Azure CLI](#demo-3-managing-storage-using-the-azure-cli)
        - [Scenario](#scenario-2)
        - [Preparation](#preparation-2)
        - [Managing storage using the Azure CLI](#managing-storage-using-the-azure-cli)

## Demo 1: Azure Storage Explorer

### Scenario

In this demonstration, you will see how to:
-  Use Azure Storage Explorer to create a new storage account.
-  Use Azure Storage Explorer to upload data to a Blob storage account.
-  Use Azure Storage Explorer to download data to your local machine.

This tutorial requires an active Azure subscription.

### Preparation
This section outlines the steps necessary to set up your environment for this module. You need an Azure trial subscription and Azure Storage Explorer to complete the demonstration. 

To obtain an Azure subscription, see: 
https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight

### Familiarize yourself with Azure Storage Explorer

In this section, you use the Azure Portal to create a Blob storage account and Azure Storage Explorer to create a new container in that account. You also view, upload, and download data with the created container.

1.  Ensure that the **MT17B-WS2016-NAT** and **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.  On the **20775A-LON-DEV** virtual machine, on the Start menu, type **Internet Explorer**, and then click **Internet Explorer**.
3.  In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
4.  In the Azure Portal, in the left pane, click **+ Create a resource**.
5.  On the **New** blade, click **Storage**, and then click **Storage account - blob, file, table, queue**.
6.  Enter the following information, and then click **Review + create**: 
    -  **Subscription**: your subscription
    -  **Resource group (Create new)**: \<*your name*\>rg
    -  **Storage account name**: \<*your name*\>sa
    -  **Location**: Your location
    -  **Performance**: Standard
    -  **Account kind**: Storage (general purpose v1)
    -  **Replication**: Read-access geo-redundant storage (RA-GRS)
7.  On the **Create storage account** blade, click **Create**.
8.  Wait until the storage account has been created.
9.  Click Start, type **Microsoft Azure Storage Explorer**, and then press Enter.
10. In the **Connect to Azure Storage** dialog box, click **Sign in**.
11. Sign in with your email address that is associated with your Azure account. Follow the onscreen prompt to enter both your email and password.
12. In the **EXPLORER** pane, your subscription is listed, along with any storage accounts already associated with your account.
13. Under your subscription, under **Storage Accounts**, expand **\<*your name*\>sa**, right-click **Blob Containers**, and then click **Create Blob Container**.
14. Type **data**, and then press Enter. This data container is where you will upload your csv file.
15. On the menu, click **+ New Folder**.
16. In the **Create New Virtual Directory** dialog box, type **Upload Data**, and then click **OK**. 
17. On the menu, click **Upload**, and then click **Upload Files**.
18. In the **Upload files** dialog box, next to the **Files** box, click the ellipsis (...).
19. In the **Select files to upload** dialog box, browse to **E:\Demofiles\Mod04**, click **uploadFile.txt**, and then click **Open**.
20. In the **Upload files** dialog box, click **Upload**.
    >**Note**: You can also drag the file directly into the storage account.
21. Right-click the **uploadFile.txt** file, and then click **Copy**.
22. Click the **Up** arrow, to return to the **data** container.
23. On the menu, click **+ New Folder**.
16. In the **Create New Virtual Directory** dialog box, type **Transfer Data**, and then click **OK**. 
17. On the menu, click **Paste**. You have used Microsoft Azure Storage Explorer to transfer files between two locations in the Azure cloud.
18. On the menu, click **Download**.
19. In the **Specify where to save the downloaded blob** dialog box, browse to **E:\Demofiles\Mod04**.
20. In the **File name** box, type **downloadedFile.txt**, and then click **Save**. 
21. In the **Activities** pane, you will see the download progress.
22. Once completed, click the **Show in folder** link.
23. In File Explorer, double-click the **downloadedFile.txt** file to open it.
24. Close Notepad, and then close Microsoft Azure Storage Explorer.

### Demo 2: Create Data Lake storage and add it to an HDInsight cluster

### Scenario

In this demonstration, you will see how to:
-  Create a Data Lake storage account.
-  Create an HDInsight cluster with access to Data Lake storage.

### Preparation

Complete the previous demonstration in this module.

### Create a Data Lake storage account

1. Ensure that the **MT17B-WS2016-NAT**, **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.  In Internet Explorer, navigate to **portal.azure.com**.
3.  Click **+ Create a resource**, click **Storage**, and then click **Data Lake Store Gen1**.
4.  On the **New Data Lake Store Gen1** blade, enter the settings as follows, and then click **Create**.
    -  **Storage account name**: \<*your name*\>dls
    -  **Subscription**: your subscription
    -  **Resource group**: \<*your name*\>rg
    -  **Location**: your location
5. Wait for the storage account to be created.
6. Click **All resourses**, click **\<*your name*\>dls**, and then click **Data explorer.**
7. Click **New folder**, type **clusters**, and then click **OK**.
8. Click the **clusters** folder.
9. Click **New folder**, type **\<*your name*\>hdi**, and then click **OK**.

### Create an HDInsight cluster with access to Data Lake storage

1.  Click **+ Create a resource**, click **Analytics**, and then click **HDInsight**.
2.  On the **HDInsight** blade, click **Custom (size, settings, apps)**.
3.  On the **Basics** blade, enter the settings as follows, and then click **Next**:
    -  **Cluster name**: \<*your name*\>hdi
    -  **Subscription**: your subscription
    -  **Cluster type**:
        -  **Cluster type**: Hadoop
        -  **Operating System**: Linux
        -  **Version**: Hadoop 2.7.3 (HDI 3.6)
    -  **Cluster login username**: hadmin
    -  **Cluster login password**: Pa55w.rdPa55w.rd
    -  **Secure Shell (SSH) username**: sadmin
    -  **Use same password as cluster login**: selected
    -  **Resource group**: \<*your name*\>rg
    -  **Location**: Select the same region as your Data Lake Store
4.  On the **Security + networking** blade, click **Next**.
5.  On the **Storage** blade, in the **Primary storage type** list, click **Azure Data Lake Store Gen1**.
6.  Under **Data Lake Storage Gen1 account**, click **Configure required settings**, and then click **\<*your name*\>dls**.
7.  Under **Data Lake Storage Gen1 access**, click **Configure required settings**.
8.  On the **Data Lake Storage Gen1** blade, under **Select Azure AD service principal**, click **Create new**.
9.  Under **Service principal**, click **Not configured**.
10. On the **Create service principal** blade, enter the settings as follows, and then click **Create**: 
    -  **Service principal name**: MyServicePrincipal
    -  **Certificate password**: Pa55wrd=123
    -  **Confirm password**: Pa55wrd123
11. On the **Data Lake Storage Gen1** blade, click **Access**.
12. On the **Select file permissions** blade, select the **\<*your name*\>dls** check box, accept the default settings.
13. Click **\<*your name*\>dls**, click clusters, select the **\<*your name*\>hdi** check box, accept the default settings, and then click **Select**.
14. On the **Assign selected permissions** blade, click **Run**.
15. On the **Assign selected permissions** blade, click **Done**.
16. On the **Data Lake Storage Gen1** blade, click **Select**.
17. On the **Storage** blade, click **Next**.
18. On the **Applications (optional)** blade, click **Next**.
19. On the **Cluster size** blade, in the **Number of Worker nodes** box, type **1**, and then click **Next**.
20. On the **Script Actions (optional)** blade, click **Next**.
21. On the **Cluster summary** blade, click **Create** to provision your cluster.
22. Wait for the cluster to be provisioned before proceeding with the demonstration. This may take 15â€“20 minutes.
23. Click **All resources**, click **\<*your name*\>hdi**, and then show the **Status** as **Running**.
24. On the **\<*your name*\>hdi** blade, under **Settings**, click **Data Lake Storage Gen1**.
25. You will see **Service Principal: Enabled**. This means that your cluster is now connected to the Azure Data Lake Store.

### Demo 3: Managing storage using the Azure CLI

### Scenario

This demonstration shows how to use the Azure CLI to manage storage accounts. The instructor will create a resource group, add an Azure Blob account and ADLS account, upload and download data to both, and delete the resources.

### Preparation

Complete the previous demonstration in this module.

### Managing storage using the Azure CLI

In the previous topic, you created a Blob storage account, an ADLS account, and uploaded data using the portal. In this demonstration, you will perform the same operations but use the Azure CLI instead.
1.  Ensure that the **MT17B-WS2016-NAT**, **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.  Open a command prompt.
3.  At the command prompt, to authenticate your Azure subscription, run the following command: 
    ````dos
    azure login
    ````
4.  If the **Select y to enable data collection** prompt appears, press **n**, and then press Enter. 
5.  Follow the login prompt to authenticate using a web browser by visiting **https://aka.ms/devicelogin**, enter the code, and then log in using your Azure credentials. 
6.  At the command prompt, to see the subscriptions associated with your email account, run the following command:
    ````dos
    azure account list
    ````
7.  If you have multiple subscriptions, at the command prompt, use the following command to select the subscription youâ€™d like to use:
    ````dos
    azure account set "<Subscription Name>"
    ````
8.  At the command prompt, to ensure the **Microsoft.DataLakeStore** resource provider is registered, run the following command:
    ````dos
    azure provider register --namespace Microsoft.DataLakeStore
    ````
9.  At the command prompt, to create a resource group in your subscription, run the following command. Replacing **\<RGName\>** with a suitable name.
    ````dos
    Azure group create -l eastus2 -n <RGName>
    ````
10. At the command prompt, to create an Azure Storage account, run the following command. Replacing **\<RGName\>**, and **\<storageaccountname\>** with a suitable name.
    ````dos
    azure storage account create -l eastus2 -g <RGName> --sku-name GRS --kind BlobStorage --access-tier Hot <storageaccountname>
    ````
11. At the command prompt, to get the connection string for this storage account, run the following command. Replacing **\<RGName\>**, and **\<storageaccountname\>**.
    ````dos
    azure storage account connectionstring show <storageaccountname> -g <RGName>
    ````
12. At the command prompt, to create a container named **clidata** in your newly created Blob storage account, run the following command. Replacing **\<ConnectionString\>** with the previous step output value.
    ````dos
    azure storage container create clidata --connection-string <ConnectionString>
    ````
13. Navigate to **E:\Demofiles\Mod04\clidata\hvac** in your terminal window. 
14. At the command prompt, to upload the file to your storage accountâ€™s clidata container, run the following command. Replacing **\<ConnectionString\>**.
    ````dos
    azure storage blob upload -f HVAC.csv â€“-container clidata -c <ConnectionString>
    ````
15. At the command prompt, to list the blobs in your clidata container, run the following command. Replacing **\<ConnectionString\>**.
    ````dos
    azure storage blob list â€“-container clidata â€“c <ConnectionString>
    ````
16. Navigate to **E:\Demofiles\Mod04\clidata\Building** in your terminal window. 
17. At the command prompt, to upload the file to your storage accountâ€™s clidata container, run the following command. Replacing **\<ConnectionString\>**.
    ````dos
    azure storage blob upload -f building.csv â€“-container clidata -c <ConnectionString>
    ````
18. At the command prompt, to list the blobs in your clidata container, run the following command. Replacing **\<ConnectionString\>**.
    ````dos
    azure storage blob list â€“-container clidata â€“c <ConnectionString>
    ````
19. You uploaded two blobs, **hvac/HVAC.csv** and **building/building.csv**. These files have information about the temperature of multiple office buildings. Now you want to upload the HVAC data to an Azure Data Lake Store.
20. At the command prompt, to create a new ADLS account, run the following command. Replacing **\<RGName\>**, and **\<ADLSName\>** with a suitable name. 
    ````dos
    azure datalake store account create -n <ADLSName> -l eastus2 -g <RGName>
    ````
21. Navigate to **E:\\Demofiles\\Mod04\\clidata\\hvac** in your terminal window. 
22. At the command prompt, to upload the **HVAC.csv** file to ADLS, run the following command. Replacing **\<ADLSName\>**.
    ````dos
    azure datalake store filesystem import -n <ADLSName> -p HVAC.csv -d clidata/hvac/hvac.csv
    ````
23. At the command prompt, to list the files in the hvac folder (to verify the upload), run the following command. Replacing **\<ADLSName\>**.
    ````dos
    Azure datalake store filesystem list -n <ADLSName> -p clidata/hvac
    ````
24. At the command prompt, to delete the resource group, run the following command. Replacing **\<RGName\>**.
    ````dos
    azure group delete -n <RGName>
    ````
25. To confirm the deletion, at the prompt, press **y**.
26. Close the command prompt.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
