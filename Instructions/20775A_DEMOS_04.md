# Module 4: Loading Data into HDInsight

- [Module 4: Loading Data into HDInsight](#module-4-loading-data-into-hdinsight)
    - [Demo 1: Azure Storage Explorer](#demo-1-azure-storage-explorer)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Familiarize yourself with Azure Storage Explorer](#familiarize-yourself-with-azure-storage-explorer)
        - [Demo 2: Create Data Lake storage and add it to an HDInsight cluster](#demo-2-create-data-lake-storage-and-add-it-to-an-hdinsight-cluster)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Create a Data Lake storage account](#create-a-data-lake-storage-account)
        - [Create an HDInsight cluster with access to Data Lake storage](#create-an-hdinsight-cluster-with-access-to-data-lake-storage)
        - [Demo 3: Managing storage using the Azure CLI](#demo-3-managing-storage-using-the-azure-cli)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Managing storage using the Azure CLI](#managing-storage-using-the-azure-cli)

## Demo 1: Azure Storage Explorer

### Scenario

In this demonstration, you will see how to:
-	Use Azure Storage Explorer to create a new storage account.
-	Use Azure Storage Explorer to upload data to a Blob storage account.
-	Use Azure Storage Explorer to download data to your local machine.

This tutorial requires an active Azure subscription.

### Preparation
This section outlines the steps necessary to set up your environment for this module. You need an Azure trial subscription and Azure Storage Explorer to complete the demonstration. 

To obtain an Azure subscription, see: 
https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight

### Familiarize yourself with Azure Storage Explorer

In this section, you use the Azure Portal to create a Blob storage account and Azure Storage Explorer to create a new container in that account. You also view, upload, and download data with the created container.

1.	Ensure that the **MT17B-WS2016-NAT**, **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	Using Internet Explorer, navigate to **portal.azure.com**.
3.	Select **new** from the left pane.
4.	In the **new** pane, click **Storage**, click **Storage account - blob, file, table, queue**.
5.	Fill in the pane with the following information, and then click **Create**: 
    -	Name: **\<your storage name\>**
    -	Deployment model: **Resource manager**
    -	Account kind: **General purpose**
    -	Performance: **Standard**
    -	Replication: **Read-access geo-redundant storage**
    -	Storage service encryption: **Disabled**
    -	Subscription: **\<your subscription name\>**
    -	Resource group: **Create new, then enter \<your resource group name\>**
    -	Location: **Your location**
6.	After the storage account has been created, open **Microsoft Azure Storage Explorer**.
7.	Select **Add an Azure account**, in the **Azure environment** list, select **Azure**, and then click **Sign in**.
8.	A pop-up asking you to sign in to your Azure account appears. Use the email address that is associated with your Azure account. Follow the onscreen prompt to enter both your email and password.
9.	In the explorer pane, your subscription is listed, along with any storage accounts already associated with your account. Here you will see the account you just created. Click the arrow next to it.
10.	You will now see blob containers listed. Right-click **Blob Container**, click **Create Blob Container**, and then name it **data**.
11.	This data container is where you will upload your csv file. Before uploading your file, create a folder named **Upload Data**. The **New Folder** button is in the middle of the selection pane. Create a new folder named **Upload Data**. 
12.	In the **E:\Demofiles\Mod04** folder, there is a file named **uploadFile.txt**. Upload this file to your Blob storage account. You can either drag the file into the storage account or click the **Upload** button in the selection pane to upload the file.
13.	Right-click the file and select **copy**.
14.	Return to the data container by pressing the **Up** arrow next to the address bar.
15.	Create a new folder called **Transfer Data**.
16.	Enter the new folder, and click the **Paste** button in the selection pane. You have used Microsoft Azure Storage Explorer to transfer files between two locations in the Azure cloud.
17.	Click the **Download** button in the selection pane, and then click **Save** to download your file named **uploadFile.txt**. 
18.	In the **Activities** pane, you will see the download progress. Once completed, click **Show in folder**.
19.	Double-click the file in the folder to open it.
20.	Close the file.
21.	Close Microsoft Azure Storage Explorer.

### Demo 2: Create Data Lake storage and add it to an HDInsight cluster

### Scenario

In this demonstration, you will see how to:
-	Create a Data Lake storage account.
-	Create an HDInsight cluster with access to Data Lake storage.

### Preparation

Complete the previous demonstration in this module.

### Create a Data Lake storage account

1. Ensure that the **MT17B-WS2016-NAT**, **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	In Internet Explorer, navigate to **portal.azure.com**.
3.	Select **New** from the left pane.
4.	In the **new** pane, click **Storage**, click **Storage account - blob, file, table, queue**.
5.	Fill out the details to create your Data Lake Store, and then click Create.
    -	Name: **\<your Data Lake Store name\>**
    -	Subscription: **\<your subscription name\>**
    -	Resource group: Click **Use existing** and then click **\<your resource group name\>**. 
    -	Location: **\<your location\>**
6. Wait for the storage account to be created.

### Create an HDInsight cluster with access to Data Lake storage

1. Click **New**, click **Data + Analytics**, and then click **HDInsight**.
2.	The cluster settings should be as follows:
    -	Cluster name: **\<cluster name\>**
    -	Cluster type settings should be as follows:
        -	Cluster type: **Hadoop**
        -	Operating System: **Linux**
        -	Version: **Hadoop 2.7.3 (HDI 3.5)**
        -	Cluster tier: **Standard**
    -	Resource group: Click **Use existing** and then click** \<your resource group name\>**
    -	Fill out the other required fields to your liking, and then click **Next**.
3.	Select **Azure Storage** for your **Primary storage type**, click **Select a Storage account**, select **\<your Data Lake Store name\>** (the Data Lake Store storage account you created previously).
4.	Click **Data Lake Store access** to provision the service principal that manages security. Click **Create new**. Configure the **Service principal** using the default certification expiration date and by providing a name and certificate password. Click **Create**. 
5.	Click **Select**, click **Next**.
6.	This will bring you to the **Cluster summary** page. At the top of the **HDInsight** blade, click **Custom (size, settings, apps)**.
6.	Select **Cluster size** from the **HDInsight** blade.
7.	Change the **Number of Worker nodes** from **4** to **1**, click **Next**.
8.	Click **Next**, and then click **Create** to provision your cluster.
9.	Select the cluster when it is provisioned.
10.	Select **Data Lake Store access** from the **Properties** section. 
11.	You will see **Service Principal: Enabled**. This means that your cluster is now connected to the Azure Data Lake Store.

### Demo 3: Managing storage using the Azure CLI

### Scenario

This demonstration shows how to use the Azure CLI to manage storage accounts. The instructor will create a resource group, add an Azure Blob account and ADLS account, upload and download data to both, and delete the resources.

### Preparation

Complete the previous demonstration in this module.

### Managing storage using the Azure CLI

In the previous topic, you created a Blob storage account, an ADLS account, and uploaded data using the portal. In this demonstration, you will perform the same operations but use the Azure CLI instead.
1.	Ensure that the **MT17B-WS2016-NAT**, **20775A-LON-DEV** virtual machines are running, and then log on to **20775A-LON-DEV** as **Admin** with the password **Pa55w.rd**.
2.	Open a command prompt.
3.	Authenticate your Azure subscription by running the following command: 
    ```dos
    azure login
    ```
4.	If the **Select y to enable data collection** prompt appears, press **n**. 
5.	Follow the login prompt to authenticate using web browser by visiting **https://aka.ms/devicelogin**, and then log in using your Azure credentials. 
6.	To see the subscriptions associated with your email account, run the following command:
    ```dos
    azure account list
    ```
7.	If you have multiple subscriptions, use the following command to select the subscription you’d like to use:
    ```dos
    azure account set "<Subscription Name>"
    ```
8.	Ensure the **Microsoft.DataLakeStore** resource provider is registered by running the following command:
    ```dos
    azure provider register --namespace Microsoft.DataLakeStore
    ```
9.	Update the datacenter location and name to create a resource group in your subscription:
    ```dos
    Azure group create -l eastus2 -n <RGName>
    ```
10.	Provide a name for your storage account. Run the following command to create an Azure Storage account:
    ```dos
    azure storage account create -l eastus2 -g <RGName> --sku-name GRS --kind BlobStorage --access-tier Hot <storageaccountname>
    ```
11.	Get the connection string for this storage account using the following command:
    ```dos
    azure storage account connectionstring show <storageaccountname> -g <RGName>
    ```
12.	Run the following command to create a container named clidata in your newly created Blob storage account:
    ```dos
    azure storage container create clidata --connection-string <ConnectionString>
    ```
13.	Navigate to **E:\Demofiles\Mod04\clidata\hvac** in your terminal window. 
14.	Run the following command to upload the file to your storage account’s clidata container:
    ```dos
    azure storage blob upload -f HVAC.csv –-container clidata -c <ConnectionString>
    ```
15.	Run the following command to list the blobs in your clidata container:
    ```dos
    azure storage blob list –-container clidata –c <ConnectionString>
    ```
16.	Navigate to **E:\Demofiles\Mod04\clidata\Building** in your terminal window. 
17.	Run the following command to upload the file to your storage account’s clidata container:
    ```dos
    azure storage blob upload -f building.csv –-container clidata -c <ConnectionString>
    ```
18.	Run the following command to list the blobs in your clidata container:
    ```dos
    azure storage blob list –-container clidata –c <ConnectionString>
    ```
19.	You uploaded two blobs, **hvac/HVAC.csv** and **building/building.csv**. These files have information about the temperature of multiple office buildings. Now you want to upload the HVAC data to an Azure Data Lake Store.
20.	Create a new ADLS account using the following command: 
    ```dos
    azure datalake store account create -n <ADLSName> -l eastus2 -g <RGName>
    ```
21.	Navigate to **E:\Demofiles\Mod04\clidata\hvac** in your terminal window. 
22.	Upload **HVAC.csv** to ADLS:
    ```dos
    azure datalake store filesystem import -n <ADLSName> -p HVAC.csv -d clidata/hvac/hvac.csv
    ```
23.	List the files in the hvac folder to verify the upload:
    ```dos
    Azure datalake store filesystem list -n <ADLSName> -p clidata/hvac
    ```
24.	This is the end of the demonstration and you will now delete the resource group:
    ```dos
    azure group delete -n <RGName>
    ```
25.	To confirm the deletion, at the prompt, press y.
26.	Close the command prompt.




---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.